/* auto-generated by NAPI-RS */
/* eslint-disable */
/**
 * Async iterator for streaming completion responses.
 *
 * Use with manual iteration by calling `next()`:
 *
 * @example
 * ```typescript
 * const stream = await client.stream(request);
 * let chunk;
 * while ((chunk = await stream.next()) !== null) {
 *   if (chunk.text) {
 *     process.stdout.write(chunk.text);
 *   }
 *   if (chunk.isDone) break;
 * }
 * ```
 *
 * Or use the callback-based `completeStream` for simpler consumption.
 */
export declare class JsAsyncStreamIterator {
  /**
   * Get the next chunk from the stream.
   *
   * Returns the next StreamChunk, or null when the stream is complete.
   * Check `chunk.isDone` to determine when streaming is complete.
   */
  next(): Promise<JsStreamChunk | null>
  /** Check if the stream is done. */
  get isFinished(): Promise<boolean>
}

/** Error information for a failed batch request. */
export declare class JsBatchError {
  /** Error type. */
  get errorType(): string
  /** Error message. */
  get message(): string
}

/**
 * A batch processing job.
 *
 * Contains information about the status and progress of a batch.
 */
export declare class JsBatchJob {
  /** The batch ID. */
  get id(): string
  /** The current status of the batch. */
  get status(): JsBatchStatus
  /** Request counts. */
  get requestCounts(): JsBatchRequestCounts
  /** When the batch was created (ISO 8601 timestamp). */
  get createdAt(): string | null
  /** When the batch started processing (ISO 8601 timestamp). */
  get startedAt(): string | null
  /** When the batch finished processing (ISO 8601 timestamp). */
  get endedAt(): string | null
  /** When the batch will expire (ISO 8601 timestamp). */
  get expiresAt(): string | null
  /** Error message if the batch failed. */
  get error(): string | null
  /** Check if the batch is complete (completed, failed, expired, or cancelled). */
  isComplete(): boolean
  /** Check if the batch is still in progress. */
  isInProgress(): boolean
}

/**
 * A request within a batch.
 *
 * @example
 * ```typescript
 * const batchRequests = [
 *   BatchRequest.create("request-1", completionRequest1),
 *   BatchRequest.create("request-2", completionRequest2),
 * ]
 * const batchJob = await client.createBatch(batchRequests)
 * ```
 */
export declare class JsBatchRequest {
  /** Create a new batch request with a custom ID and completion request. */
  static create(customId: string, request: JsCompletionRequest): JsBatchRequest
  /** The custom ID for this request. */
  get customId(): string
}

/** Request counts for a batch job. */
export declare class JsBatchRequestCounts {
  /** Total number of requests in the batch. */
  get total(): number
  /** Number of successfully completed requests. */
  get succeeded(): number
  /** Number of failed requests. */
  get failed(): number
  /** Number of pending requests. */
  get pending(): number
}

/** Result of a single request within a batch. */
export declare class JsBatchResult {
  /** The custom ID of the original request. */
  get customId(): string
  /** The completion response (if successful). */
  get response(): JsCompletionResponse | null
  /** The error (if failed). */
  get error(): JsBatchError | null
  /** Check if this result is successful. */
  isSuccess(): boolean
  /** Check if this result is an error. */
  isError(): boolean
}

/** Cache breakpoint configuration for prompt caching. */
export declare class JsCacheBreakpoint {
  /** Create an ephemeral cache breakpoint (5-minute TTL). */
  static ephemeral(): JsCacheBreakpoint
  /** Create an extended cache breakpoint (1-hour TTL, Anthropic beta). */
  static extended(): JsCacheBreakpoint
  /** The cache control type. */
  get cacheControl(): JsCacheControl
}

/** Request for text classification. */
export declare class JsClassificationRequest {
  /** Model identifier in "provider/model" format (e.g., "cohere/classify") */
  model: string
  text: string
  labels: Array<string>
  /** Create a new classification request. */
  constructor(model: string, text: string, labels: Array<string>)
}

/**
 * Fluent builder for LLMKitClient.
 *
 * Provides a fluent builder pattern for configuring the client with specific providers.
 * Each provider can be added using `with*FromEnv()` or `with*(apiKey)` methods.
 *
 * @example
 * ```typescript
 * import { ClientBuilder } from 'llmkit'
 *
 * // Build client with specific providers
 * const client = await new ClientBuilder()
 *     .withAnthropicFromEnv()
 *     .withOpenAIFromEnv()
 *     .withGroq("your-groq-api-key")
 *     .withDefaultRetry()
 *     .build()
 * ```
 */
export declare class JsClientBuilder {
  /** Create a new client builder. */
  constructor()
  /** Add Anthropic provider from ANTHROPIC_API_KEY environment variable. */
  withAnthropicFromEnv(): this
  /** Add Anthropic provider with explicit API key. */
  withAnthropic(apiKey: string): this
  /** Add OpenAI provider from OPENAI_API_KEY environment variable. */
  withOpenAIFromEnv(): this
  /** Add OpenAI provider with explicit API key. */
  withOpenAI(apiKey: string): this
  /** Add Azure OpenAI provider from environment variables. */
  withAzureFromEnv(): this
  /** Add AWS Bedrock provider from environment. */
  withBedrockFromEnv(): this
  /** Add Google Vertex AI provider from environment. */
  withVertexFromEnv(): this
  /** Add Google AI (Gemini) provider from GOOGLE_API_KEY environment variable. */
  withGoogleFromEnv(): this
  /** Add Groq provider from GROQ_API_KEY environment variable. */
  withGroqFromEnv(): this
  /** Add Groq provider with explicit API key. */
  withGroq(apiKey: string): this
  /** Add Mistral provider from MISTRAL_API_KEY environment variable. */
  withMistralFromEnv(): this
  /** Add Mistral provider with explicit API key. */
  withMistral(apiKey: string): this
  /** Add DeepSeek provider from DEEPSEEK_API_KEY environment variable. */
  withDeepSeekFromEnv(): this
  /** Add DeepSeek provider with explicit API key. */
  withDeepSeek(apiKey: string): this
  /** Add Cohere provider from COHERE_API_KEY environment variable. */
  withCohereFromEnv(): this
  /** Add Together AI provider from TOGETHER_API_KEY environment variable. */
  withTogetherFromEnv(): this
  /** Add Perplexity provider from PERPLEXITY_API_KEY environment variable. */
  withPerplexityFromEnv(): this
  /** Add OpenRouter provider from OPENROUTER_API_KEY environment variable. */
  withOpenRouterFromEnv(): this
  /** Add xAI (Grok) provider from XAI_API_KEY environment variable. */
  withXAIFromEnv(): this
  /** Use default retry configuration (10 retries with exponential backoff). */
  withDefaultRetry(): this
  /** Use custom retry configuration. */
  withRetry(config: JsRetryConfig): this
  /** Build the LLMKitClient. */
  build(): Promise<JsLLMKitClient>
}

/**
 * Request to complete a conversation.
 *
 * Use the static factory method `create()` or the builder pattern.
 *
 * @example
 * ```typescript
 * // Factory with model and messages
 * const request = CompletionRequest.create("claude-sonnet-4-20250514", [Message.user("Hello")])
 *   .withSystem("You are helpful")
 *   .withMaxTokens(1024)
 *
 * // Or use builder pattern
 * const request = CompletionRequest.create("gpt-4o", [Message.user("Hi")])
 *   .withTemperature(0.7)
 *   .withStreaming()
 * ```
 */
export declare class JsCompletionRequest {
  /** Create a new completion request with model and messages. */
  static create(model: string, messages: Array<JsMessage>): JsCompletionRequest
  /** Builder method: Set the system prompt. */
  withSystem(system: string): JsCompletionRequest
  /** Builder method: Set max tokens. */
  withMaxTokens(maxTokens: number): JsCompletionRequest
  /** Builder method: Set temperature. */
  withTemperature(temperature: number): JsCompletionRequest
  /** Builder method: Set top_p (nucleus sampling). */
  withTopP(topP: number): JsCompletionRequest
  /** Builder method: Set tools. */
  withTools(tools: Array<JsToolDefinition>): JsCompletionRequest
  /** Builder method: Set stop sequences. */
  withStopSequences(stopSequences: Array<string>): JsCompletionRequest
  /** Builder method: Enable streaming. */
  withStreaming(): JsCompletionRequest
  /** Builder method: Enable extended thinking with token budget. */
  withThinking(budgetTokens: number): JsCompletionRequest
  /** Builder method: Set thinking configuration. */
  withThinkingConfig(config: JsThinkingConfig): JsCompletionRequest
  /**
   * Builder method: Disable thinking/reasoning.
   *
   * Useful for getting faster, cheaper responses from reasoning models
   * like Qwen3, DeepSeek-R1, or when using OpenRouter's reasoning control.
   */
  withoutThinking(): JsCompletionRequest
  /**
   * Builder method: Set thinking effort level.
   *
   * Controls how much reasoning effort the model uses.
   * Supported by OpenRouter and similar providers.
   */
  withThinkingEffort(effort: JsThinkingEffort): JsCompletionRequest
  /** Builder method: Set JSON schema for structured output. */
  withJsonSchema(name: string, schema: any): JsCompletionRequest
  /** Builder method: Set response format. */
  withResponseFormat(format: JsStructuredOutput): JsCompletionRequest
  /** Builder method: Enable JSON object output. */
  withJsonOutput(): JsCompletionRequest
  /** Builder method: Set predicted output for speculative decoding. */
  withPrediction(predictedContent: string): JsCompletionRequest
  /** Builder method: Enable system prompt caching (ephemeral). */
  withSystemCaching(): JsCompletionRequest
  /** Builder method: Enable system prompt caching (extended, 1-hour TTL). */
  withSystemCachingExtended(): JsCompletionRequest
  /** Builder method: Enable extended output (128k tokens, Anthropic beta). */
  withExtendedOutput(): JsCompletionRequest
  /** Builder method: Enable interleaved thinking (Anthropic beta). */
  withInterleavedThinking(): JsCompletionRequest
  /** Builder method: Set extra provider-specific options. */
  withExtra(extra: any): JsCompletionRequest
  /** The model identifier. */
  get model(): string
  /** The conversation messages. */
  get messages(): Array<JsMessage>
  /** The system prompt (if set). */
  get system(): string | null
  /** Maximum tokens to generate. */
  get maxTokens(): number | null
  /** Sampling temperature. */
  get temperature(): number | null
  /** Whether streaming is enabled. */
  get stream(): boolean
  /** Check if the request has caching enabled. */
  hasCaching(): boolean
  /** Check if the request has thinking enabled. */
  hasThinking(): boolean
  /** Check if the request has structured output. */
  hasStructuredOutput(): boolean
}

/** Response from a completion request. */
export declare class JsCompletionResponse {
  /** Unique response ID. */
  get id(): string
  /** Model that generated the response. */
  get model(): string
  /** Content blocks in the response. */
  get content(): Array<JsContentBlock>
  /** Reason the model stopped. */
  get stopReason(): JsStopReason
  /** Token usage information. */
  get usage(): JsUsage
  /** Get all text content from the response concatenated. */
  textContent(): string
  /** Extract all tool use blocks from the response. */
  toolUses(): Array<JsContentBlock>
  /** Check if the response contains tool use. */
  hasToolUse(): boolean
  /** Get thinking content from the response if present. */
  thinkingContent(): string | null
}

/**
 * A block of content within a message.
 *
 * Use the static factory methods to create instances:
 * - `ContentBlock.text("Hello")`
 * - `ContentBlock.image("image/png", base64Data)`
 * - `ContentBlock.toolUse(id, name, inputObj)`
 */
export declare class JsContentBlock {
  /** Create a text content block. */
  static text(text: string): JsContentBlock
  /** Create an image content block from base64 data. */
  static image(mediaType: string, data: string): JsContentBlock
  /** Create an image content block from URL. */
  static imageUrl(url: string): JsContentBlock
  /** Create a tool use content block. */
  static toolUse(id: string, name: string, input: any): JsContentBlock
  /** Create a tool result content block. */
  static toolResult(toolUseId: string, content: string, isError?: boolean | undefined | null): JsContentBlock
  /** Create a thinking content block. */
  static thinking(thinking: string): JsContentBlock
  /** Create a PDF document content block. */
  static pdf(data: string): JsContentBlock
  /** Create a text content block with ephemeral caching. */
  static textCached(text: string): JsContentBlock
  /** True if this is a text block. */
  get isText(): boolean
  /** True if this is a tool use block. */
  get isToolUse(): boolean
  /** True if this is a tool result block. */
  get isToolResult(): boolean
  /** True if this is a document block. */
  get isDocument(): boolean
  /** True if this is a thinking block. */
  get isThinking(): boolean
  /** True if this is an image block. */
  get isImage(): boolean
  /** Get text content if this is a text block. */
  get textValue(): string | null
  /** Get thinking content if this is a thinking block. */
  get thinkingContent(): string | null
  /**
   * Get tool use details if this is a tool use block.
   * Returns an object with id, name, and input properties.
   */
  asToolUse(): JsToolUseInfo | null
  /**
   * Get tool result details if this is a tool result block.
   * Returns an object with toolUseId, content, and isError properties.
   */
  asToolResult(): JsToolResultInfo | null
}

/** Delta content for streaming responses. */
export declare class JsContentDelta {
  /** Get text if this is a text delta. */
  get text(): string | null
  /** Get thinking content if this is a thinking delta. */
  get thinking(): string | null
  /** True if this is a text delta. */
  get isText(): boolean
  /** True if this is a tool use delta. */
  get isToolUse(): boolean
  /** True if this is a thinking delta. */
  get isThinking(): boolean
  /**
   * Get tool use delta details.
   * Returns an object with optional id, name, and inputJsonDelta properties.
   */
  asToolUseDelta(): JsToolUseDelta | null
}

/** A single embedding vector. */
export declare class JsEmbedding {
  /** The index of this embedding in the batch. */
  get index(): number
  /** The embedding vector values. */
  get values(): Array<number>
  /** Get the number of dimensions. */
  get dimensionCount(): number
  /**
   * Compute cosine similarity with another embedding.
   *
   * @param other - Another embedding to compare with
   * @returns Cosine similarity score (-1 to 1)
   */
  cosineSimilarity(other: JsEmbedding): number
  /**
   * Compute dot product with another embedding.
   *
   * @param other - Another embedding to compute dot product with
   * @returns Dot product value
   */
  dotProduct(other: JsEmbedding): number
  /**
   * Compute Euclidean distance to another embedding.
   *
   * @param other - Another embedding to compute distance to
   * @returns Euclidean distance
   */
  euclideanDistance(other: JsEmbedding): number
}

/**
 * Request for generating embeddings.
 *
 * @example
 * ```typescript
 * // Single text
 * const request = new EmbeddingRequest("text-embedding-3-small", "Hello, world!");
 *
 * // Batch
 * const request = EmbeddingRequest.batch("text-embedding-3-small", ["Hello", "World"]);
 * ```
 */
export declare class JsEmbeddingRequest {
  /**
   * Create a new embedding request for a single text.
   *
   * @param model - The embedding model to use (e.g., "text-embedding-3-small")
   * @param text - The text to embed
   */
  constructor(model: string, text: string)
  /**
   * Create a new embedding request for multiple texts (batch).
   *
   * @param model - The embedding model to use
   * @param texts - List of texts to embed
   */
  static batch(model: string, texts: Array<string>): JsEmbeddingRequest
  /**
   * Set the output dimensions (for models that support dimension reduction).
   *
   * @param dimensions - The number of dimensions for the output embedding
   * @returns Self for method chaining
   */
  withDimensions(dimensions: number): JsEmbeddingRequest
  /**
   * Set the encoding format.
   *
   * @param format - The encoding format (Float or Base64)
   * @returns Self for method chaining
   */
  withEncodingFormat(format: JsEncodingFormat): JsEmbeddingRequest
  /**
   * Set the input type hint for optimized embeddings.
   *
   * @param inputType - The input type (Query or Document)
   * @returns Self for method chaining
   */
  withInputType(inputType: JsEmbeddingInputType): JsEmbeddingRequest
  /** Get the model name. */
  get model(): string
  /** Get the number of texts to embed. */
  get textCount(): number
  /** Get all input texts as an array. */
  texts(): Array<string>
  /** Get the dimensions setting. */
  get dimensions(): number | null
}

/** Response from an embedding request. */
export declare class JsEmbeddingResponse {
  /** The model used for embedding. */
  get model(): string
  /** The generated embeddings. */
  get embeddings(): Array<JsEmbedding>
  /** Token usage information. */
  get usage(): JsEmbeddingUsage
  /** Get the first embedding (convenience for single-text requests). */
  first(): JsEmbedding | null
  /** Get embedding values as a flat array (for single-text requests). */
  values(): Array<number> | null
  /** Get the embedding dimensions. */
  get dimensionCount(): number
}

/** Token usage for embedding requests. */
export declare class JsEmbeddingUsage {
  /** Number of tokens in the input. */
  get promptTokens(): number
  /** Total tokens processed. */
  get totalTokens(): number
}

/** A generated image. */
export declare class JsGeneratedImage {
  url?: string
  b64Json?: string
  revisedPrompt?: string
  /** Create a new generated image from URL. */
  constructor()
  /** Create a new generated image from URL. */
  static fromUrl(url: string): JsGeneratedImage
  /** Create a new generated image from base64 data. */
  static fromB64(data: string): JsGeneratedImage
  /** Set the revised prompt. */
  withRevisedPrompt(revisedPrompt: string): JsGeneratedImage
  /** Get the size of the image data if available (b64_json). */
  get size(): number
}

/** Request for generating images. */
export declare class JsImageGenerationRequest {
  model: string
  prompt: string
  n?: number
  size?: JsImageSize
  quality?: JsImageQuality
  style?: JsImageStyle
  responseFormat?: JsImageFormat
  negativePrompt?: string
  seed?: number
  /** Create a new image generation request. */
  constructor(model: string, prompt: string)
  /** Set the number of images to generate. */
  withN(n: number): JsImageGenerationRequest
  /** Set the image size. */
  withSize(size: JsImageSize): JsImageGenerationRequest
  /** Set the image quality. */
  withQuality(quality: JsImageQuality): JsImageGenerationRequest
  /** Set the image style. */
  withStyle(style: JsImageStyle): JsImageGenerationRequest
  /** Set the response format. */
  withFormat(responseFormat: JsImageFormat): JsImageGenerationRequest
  /** Set a negative prompt. */
  withNegativePrompt(negativePrompt: string): JsImageGenerationRequest
  /** Set the seed for reproducibility. */
  withSeed(seed: number): JsImageGenerationRequest
}

/**
 * LLMKit client for JavaScript/TypeScript.
 *
 * @example
 * ```typescript
 * import { LLMKitClient, Message, CompletionRequest } from 'llmkit'
 *
 * // Create client from environment variables
 * const client = LLMKitClient.fromEnv()
 *
 * // Create client with explicit provider config
 * const client = new LLMKitClient({
 *   providers: {
 *     anthropic: { apiKey: "sk-..." },
 *     openai: { apiKey: "sk-..." },
 *     azure: { apiKey: "...", endpoint: "https://...", deployment: "gpt-4" },
 *     bedrock: { region: "us-east-1" },
 *   }
 * })
 *
 * // Make a completion request
 * const response = await client.complete(
 *   CompletionRequest.create("claude-sonnet-4-20250514", [Message.user("Hello!")])
 * )
 * console.log(response.textContent())
 *
 * // Streaming with callback
 * client.completeStream(request.withStreaming(), (chunk, error) => {
 *   if (error) throw new Error(error)
 *   if (!chunk) return // done
 *   if (chunk.text) process.stdout.write(chunk.text)
 * })
 * ```
 */
export declare class JsLLMKitClient {
  /**
   * Create a new LLMKit client with provider configurations.
   *
   * @param options - Configuration options including providers dict
   * @param retryConfig - Optional retry configuration. If not provided, uses default
   *   (10 retries with exponential backoff). Pass `RetryConfig.none()` to disable retry.
   *
   * @example
   * ```typescript
   * const client = new LLMKitClient({
   *   providers: {
   *     anthropic: { apiKey: "sk-..." },
   *     azure: { apiKey: "...", endpoint: "https://...", deployment: "gpt-4" },
   *   }
   * })
   *
   * // With custom retry
   * const client = new LLMKitClient({}, RetryConfig.conservative())
   *
   * // Disable retry
   * const client = new LLMKitClient({}, RetryConfig.none())
   * ```
   */
  constructor(options?: LlmKitClientOptions | undefined | null, retryConfig?: JsRetryConfig | undefined | null)
  /**
   * Create client from environment variables.
   *
   * Automatically detects and configures all available providers from environment variables.
   *
   * @param retryConfig - Optional retry configuration. If not provided, uses default
   *   (10 retries with exponential backoff). Pass `RetryConfig.none()` to disable retry.
   *
   * Supported environment variables:
   * - ANTHROPIC_API_KEY: Anthropic (Claude)
   * - OPENAI_API_KEY: OpenAI (GPT)
   * - AZURE_OPENAI_API_KEY, AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_DEPLOYMENT: Azure OpenAI
   * - OPENROUTER_API_KEY: OpenRouter
   * - AWS_REGION or AWS_DEFAULT_REGION: AWS Bedrock (uses default credential chain)
   * - GOOGLE_API_KEY: Google AI (Gemini)
   * - GOOGLE_CLOUD_PROJECT, VERTEX_LOCATION, VERTEX_ACCESS_TOKEN: Google Vertex AI
   * - GROQ_API_KEY: Groq
   * - MISTRAL_API_KEY: Mistral
   * - COHERE_API_KEY or CO_API_KEY: Cohere
   * - AI21_API_KEY: AI21 Labs
   * - DEEPSEEK_API_KEY: DeepSeek
   * - XAI_API_KEY: xAI (Grok)
   * - TOGETHER_API_KEY: Together AI
   * - FIREWORKS_API_KEY: Fireworks AI
   * - PERPLEXITY_API_KEY: Perplexity
   * - CEREBRAS_API_KEY: Cerebras
   * - SAMBANOVA_API_KEY: SambaNova
   * - NVIDIA_NIM_API_KEY: NVIDIA NIM
   * - DATAROBOT_API_KEY: DataRobot
   * - HUGGINGFACE_API_KEY or HF_TOKEN: HuggingFace
   * - REPLICATE_API_TOKEN: Replicate
   * - CLOUDFLARE_API_TOKEN, CLOUDFLARE_ACCOUNT_ID: Cloudflare Workers AI
   * - WATSONX_API_KEY, WATSONX_PROJECT_ID: IBM watsonx.ai
   * - DATABRICKS_TOKEN, DATABRICKS_HOST: Databricks
   * - BASETEN_API_KEY: Baseten
   * - RUNPOD_API_KEY, RUNPOD_ENDPOINT_ID: RunPod
   * - ANYSCALE_API_KEY: Anyscale
   * - DEEPINFRA_API_KEY: DeepInfra
   * - NOVITA_API_KEY: Novita AI
   * - HYPERBOLIC_API_KEY: Hyperbolic
   * - LAMBDA_API_KEY: Lambda
   * - FRIENDLI_API_KEY: Friendli
   * - BAIDU_API_KEY: Baidu (ERNIE)
   * - ALIBABA_API_KEY: Alibaba (Qwen)
   * - VOLCENGINE_API_KEY: Volcengine
   * - MARITACA_API_KEY: Maritaca
   * - LIGHTON_API_KEY: LightOn
   * - VOYAGE_API_KEY: Voyage AI
   * - JINA_API_KEY: Jina AI
   * - STABILITY_API_KEY: Stability AI
   * - OLLAMA_BASE_URL: Ollama (local, defaults to http://localhost:11434)
   *
   * @example
   * ```typescript
   * // Default retry
   * const client = LLMKitClient.fromEnv()
   *
   * // Custom retry
   * const client = LLMKitClient.fromEnv(RetryConfig.conservative())
   *
   * // Disable retry
   * const client = LLMKitClient.fromEnv(RetryConfig.none())
   * ```
   */
  static fromEnv(retryConfig?: JsRetryConfig | undefined | null): JsLLMKitClient
  /**
   * Make a completion request.
   *
   * Returns a Promise that resolves to CompletionResponse.
   */
  complete(request: JsCompletionRequest): Promise<JsCompletionResponse>
  /**
   * Make a streaming completion request with callback.
   *
   * The callback receives the stream chunk directly, or null on error/done.
   * Check chunk.isDone to determine when streaming is complete.
   */
  completeStream(request: JsCompletionRequest, callback: (chunk: StreamChunk | null, error: string | null) => void): void
  /**
   * Make a streaming completion request with async iterator.
   *
   * Returns an async iterator that you can call `next()` on to get stream chunks:
   *
   * @example
   * ```typescript
   * const stream = await client.stream(request);
   * let chunk;
   * while ((chunk = await stream.next()) !== null) {
   *   if (chunk.text) process.stdout.write(chunk.text);
   *   if (chunk.isDone) break;
   * }
   * ```
   */
  stream(request: JsCompletionRequest): Promise<JsAsyncStreamIterator>
  /** Make a completion request with a specific provider. */
  completeWithProvider(providerName: string, request: JsCompletionRequest): Promise<JsCompletionResponse>
  /** List all registered providers. */
  providers(): Array<string>
  /** Get the default provider name. */
  get defaultProvider(): string | null
  /**
   * Count tokens for a request.
   *
   * This allows estimation of token counts before making a completion request,
   * useful for cost estimation and context window management.
   *
   * Note: Not all providers support token counting. Currently only Anthropic
   * provides native token counting support.
   */
  countTokens(request: JsTokenCountRequest): Promise<JsTokenCountResult>
  /**
   * Count tokens for a request with a specific provider.
   *
   * @param providerName - Name of the provider to use
   * @param request - TokenCountRequest with model, messages, optional system and tools
   * @returns TokenCountResult containing input_tokens count
   */
  countTokensWithProvider(providerName: string, request: JsTokenCountRequest): Promise<JsTokenCountResult>
  /**
   * Create a batch processing job.
   *
   * Submits multiple completion requests to be processed asynchronously.
   * Returns a BatchJob that can be used to track progress and retrieve results.
   *
   * @example
   * ```typescript
   * const requests = [
   *   BatchRequest.create("req-1", CompletionRequest.create("claude-sonnet-4-20250514", [Message.user("Hello")])),
   *   BatchRequest.create("req-2", CompletionRequest.create("claude-sonnet-4-20250514", [Message.user("World")])),
   * ]
   * const batchJob = await client.createBatch(requests)
   * console.log(`Batch created: ${batchJob.id}`)
   * ```
   */
  createBatch(requests: Array<JsBatchRequest>): Promise<JsBatchJob>
  /**
   * Get the status of a batch job.
   *
   * @param providerName - The provider that created the batch
   * @param batchId - The batch ID
   *
   * @example
   * ```typescript
   * const job = await client.getBatch("anthropic", batchJob.id)
   * console.log(`Status: ${job.status}`)
   * ```
   */
  getBatch(providerName: string, batchId: string): Promise<JsBatchJob>
  /**
   * Get the results of a completed batch.
   *
   * @param providerName - The provider that created the batch
   * @param batchId - The batch ID
   *
   * @example
   * ```typescript
   * const results = await client.getBatchResults("anthropic", batchJob.id)
   * for (const result of results) {
   *   if (result.isSuccess()) {
   *     console.log(`${result.customId}: ${result.response?.textContent()}`)
   *   } else {
   *     console.error(`${result.customId}: ${result.error?.message}`)
   *   }
   * }
   * ```
   */
  getBatchResults(providerName: string, batchId: string): Promise<Array<JsBatchResult>>
  /**
   * Cancel a batch job.
   *
   * @param providerName - The provider that created the batch
   * @param batchId - The batch ID
   *
   * @example
   * ```typescript
   * const job = await client.cancelBatch("anthropic", batchJob.id)
   * console.log(`Batch cancelled: ${job.status}`)
   * ```
   */
  cancelBatch(providerName: string, batchId: string): Promise<JsBatchJob>
  /**
   * List batch jobs for a provider.
   *
   * @param providerName - The provider to list batches for
   * @param limit - Maximum number of batches to return (optional)
   *
   * @example
   * ```typescript
   * const batches = await client.listBatches("anthropic", 10)
   * for (const batch of batches) {
   *   console.log(`${batch.id}: ${batch.status}`)
   * }
   * ```
   */
  listBatches(providerName: string, limit?: number | undefined | null): Promise<Array<JsBatchJob>>
  /**
   * Generate embeddings for text.
   *
   * Creates vector representations of text that can be used for semantic search,
   * clustering, classification, and other NLP tasks.
   *
   * Note: Not all providers support embeddings. Currently OpenAI and Cohere
   * support this feature.
   *
   * @example
   * ```typescript
   * const response = await client.embed(
   *   new EmbeddingRequest("text-embedding-3-small", "Hello, world!")
   * )
   * console.log(`Dimensions: ${response.dimensionCount}`)
   * console.log(`Values: ${response.values()?.slice(0, 5)}...`)
   * ```
   */
  embed(request: JsEmbeddingRequest): Promise<JsEmbeddingResponse>
  /**
   * Generate embeddings with a specific provider.
   *
   * @param providerName - Name of the embedding provider (e.g., "openai", "cohere")
   * @param request - EmbeddingRequest with model and text(s) to embed
   *
   * @example
   * ```typescript
   * const response = await client.embedWithProvider(
   *   "openai",
   *   new EmbeddingRequest("text-embedding-3-small", "Hello")
   * )
   * ```
   */
  embedWithProvider(providerName: string, request: JsEmbeddingRequest): Promise<JsEmbeddingResponse>
  /**
   * List all registered embedding providers.
   *
   * @returns Names of providers that support embeddings
   */
  embeddingProviders(): Array<string>
  /**
   * Check if a provider supports embeddings.
   *
   * @param providerName - Name of the provider to check
   * @returns True if the provider supports embeddings
   */
  supportsEmbeddings(providerName: string): boolean
  /**
   * List all registered speech synthesis providers.
   *
   * @returns Names of providers that support text-to-speech
   */
  speechProviders(): Array<string>
  /**
   * List all registered transcription providers.
   *
   * @returns Names of providers that support speech-to-text
   */
  transcriptionProviders(): Array<string>
  /**
   * List all registered image generation providers.
   *
   * @returns Names of providers that support image generation
   */
  imageProviders(): Array<string>
  /**
   * List all registered video generation providers.
   *
   * @returns Names of providers that support video generation
   */
  videoProviders(): Array<string>
  /**
   * List all registered ranking/reranking providers.
   *
   * @returns Names of providers that support document ranking
   */
  rankingProviders(): Array<string>
  /**
   * List all registered moderation providers.
   *
   * @returns Names of providers that support content moderation
   */
  moderationProviders(): Array<string>
  /**
   * List all registered classification providers.
   *
   * @returns Names of providers that support text classification
   */
  classificationProviders(): Array<string>
  /**
   * Transcribe audio to text.
   *
   * Converts speech audio to text using various providers (Deepgram, AssemblyAI).
   *
   * @param request - The transcription request with audio bytes and options
   * @returns The transcribed text with word-level details
   *
   * @example
   * ```typescript
   * import fs from 'fs'
   * import { LLMKitClient, TranscriptionRequest } from 'llmkit'
   *
   * const client = LLMKitClient.fromEnv()
   * const audioBytes = fs.readFileSync('speech.wav')
   *
   * const request = new TranscriptionRequest(audioBytes)
   * request.with_model('nova-3')
   *
   * const response = await client.transcribeAudio(request)
   * console.log(response.transcript)
   * ```
   */
  transcribeAudio(request: JsTranscriptionRequest): Promise<JsTranscribeResponse>
  /**
   * Transcribe audio to text with a specific provider.
   *
   * @param providerName - Name of the provider to use
   * @param request - TranscriptionRequest with audio data
   * @returns TranscribeResponse with transcript text
   */
  transcribeAudioWithProvider(providerName: string, request: JsTranscriptionRequest): Promise<JsTranscribeResponse>
  /**
   * Synthesize text to speech.
   *
   * Converts text to speech audio using various providers (ElevenLabs, AssemblyAI).
   *
   * @param request - The synthesis request with text and voice options
   * @returns The synthesized audio as bytes
   *
   * @example
   * ```typescript
   * import fs from 'fs'
   * import { LLMKitClient, SynthesisRequest } from 'llmkit'
   *
   * const client = LLMKitClient.fromEnv()
   *
   * const request = new SynthesisRequest('Hello, world!')
   * request.with_voice('pNInY14gQrG92XwBIHVr')
   *
   * const response = await client.synthesizeSpeech(request)
   * fs.writeFileSync('speech.mp3', Buffer.from(response.audioBytes))
   * ```
   */
  synthesizeSpeech(request: JsSynthesisRequest): Promise<JsSynthesizeResponse>
  /**
   * Synthesize text to speech with a specific provider.
   *
   * @param providerName - Name of the provider to use
   * @param request - SynthesisRequest with text and voice options
   * @returns SynthesizeResponse with audio data
   */
  synthesizeSpeechWithProvider(providerName: string, request: JsSynthesisRequest): Promise<JsSynthesizeResponse>
  /**
   * Generate video from a text prompt.
   *
   * Generates video content using various providers (Runware, DiffusionRouter).
   *
   * @param request - The video generation request with prompt and options
   * @returns The generated video or task information
   *
   * @example
   * ```typescript
   * import { LLMKitClient, VideoGenerationRequest } from 'llmkit'
   *
   * const client = LLMKitClient.fromEnv()
   *
   * const request = new VideoGenerationRequest('A cat chasing a red ball')
   * request.with_model('runway-gen-4.5')
   * request.with_duration(10)
   *
   * const response = await client.generateVideo(request)
   * console.log(`Video task ID: ${response.taskId}`)
   * ```
   */
  generateVideo(request: JsVideoGenerationRequest): Promise<JsVideoGenerationResponse>
  /**
   * Generate video from a prompt with a specific provider.
   *
   * @param providerName - Name of the provider to use
   * @param request - VideoGenerationRequest with prompt and parameters
   * @returns VideoGenerationResponse with video URL or job ID
   */
  generateVideoWithProvider(providerName: string, request: JsVideoGenerationRequest): Promise<JsVideoGenerationResponse>
  /**
   * Get the status of a video generation job.
   *
   * @param providerName - Name of the video provider
   * @param jobId - The job ID returned from generateVideo
   * @returns VideoGenerationResponse with current status
   */
  getVideoStatus(providerName: string, jobId: string): Promise<JsVideoGenerationResponse>
  /**
   * Generate images from a text prompt.
   *
   * # Arguments
   *
   * * `request` - `ImageGenerationRequest` with prompt, model, and optional parameters
   *
   * # Returns
   *
   * `ImageGenerationResponse` containing generated image data
   *
   * # Example
   *
   * ```typescript
   * import { LLMKitClient, ImageGenerationRequest, ImageSize, ImageQuality } from 'llmkit'
   *
   * const client = LLMKitClient.fromEnv()
   *
   * const request = new ImageGenerationRequest('fal-ai/flux/dev', 'A serene landscape')
   * request.with_n(1)
   * request.with_size(ImageSize.Square1024)
   * request.with_quality(ImageQuality.Hd)
   *
   * const response = await client.generateImage(request)
   * console.log(`Generated ${response.count} images`)
   * ```
   */
  generateImage(request: JsImageGenerationRequest): Promise<JsImageGenerationResponse>
  /**
   * Generate images from a text prompt with a specific provider.
   *
   * @param providerName - Name of the provider to use
   * @param request - ImageGenerationRequest with prompt and parameters
   * @returns ImageGenerationResponse with generated images
   */
  generateImageWithProvider(providerName: string, request: JsImageGenerationRequest): Promise<JsImageGenerationResponse>
  /** Rank documents by relevance to a query. */
  rankDocuments(request: JsRankingRequest): Promise<JsRankingResponse>
  /**
   * Rank documents by relevance to a query with a specific provider.
   *
   * @param providerName - Name of the provider to use
   * @param request - RankingRequest with query and documents
   * @returns RankingResponse with ranked documents
   */
  rankDocumentsWithProvider(providerName: string, request: JsRankingRequest): Promise<JsRankingResponse>
  /** Rerank search results for semantic relevance. */
  rerankResults(request: JsRerankingRequest): Promise<JsRerankingResponse>
  /** Check content for policy violations. */
  moderateText(request: JsModerationRequest): Promise<JsModerationResponse>
  /**
   * Check content for policy violations with a specific provider.
   *
   * @param providerName - Name of the provider to use
   * @param request - ModerationRequest with text to check
   * @returns ModerationResponse with flagged status and scores
   */
  moderateTextWithProvider(providerName: string, request: JsModerationRequest): Promise<JsModerationResponse>
  /** Classify text into provided labels. */
  classifyText(request: JsClassificationRequest): Promise<JsClassificationResponse>
  /**
   * Classify text into provided labels with a specific provider.
   *
   * @param providerName - Name of the provider to use
   * @param request - ClassificationRequest with text and labels
   * @returns ClassificationResponse with classifications
   */
  classifyTextWithProvider(providerName: string, request: JsClassificationRequest): Promise<JsClassificationResponse>
}

/**
 * A message in a conversation.
 *
 * Use the static factory methods to create instances:
 * - `Message.system("You are helpful")`
 * - `Message.user("Hello")`
 * - `Message.assistant("Hi there")`
 */
export declare class JsMessage {
  /** Create a system message with text content. */
  static system(text: string): JsMessage
  /** Create a user message with text content. */
  static user(text: string): JsMessage
  /** Create an assistant message with text content. */
  static assistant(text: string): JsMessage
  /** Create a user message with multiple content blocks. */
  static userWithContent(content: Array<JsContentBlock>): JsMessage
  /** Create an assistant message with multiple content blocks. */
  static assistantWithContent(content: Array<JsContentBlock>): JsMessage
  /** Create a user message with tool results. */
  static toolResults(results: Array<JsContentBlock>): JsMessage
  /** The role of the message sender. */
  get role(): JsRole
  /** The content blocks in this message. */
  get content(): Array<JsContentBlock>
  /** Get all text content from the message concatenated. */
  textContent(): string
  /** Check if the message contains any tool use blocks. */
  hasToolUse(): boolean
  /** Extract all tool use blocks from the message. */
  toolUses(): Array<JsContentBlock>
}

/** Complete model specification. */
export declare class JsModelInfo {
  /** Unified model ID (e.g., "anthropic/claude-3-5-sonnet"). */
  get id(): string
  /** Short alias (e.g., "claude-3-5-sonnet"). */
  get alias(): string | null
  /** Human-readable name. */
  get name(): string
  /** Provider. */
  get provider(): JsProvider
  /** Model status. */
  get status(): JsModelStatus
  /** Pricing information. */
  get pricing(): JsModelPricing
  /** Model capabilities. */
  get capabilities(): JsModelCapabilities
  /** Benchmark scores. */
  get benchmarks(): JsModelBenchmarks
  /** Model description. */
  get description(): string
  /** Whether the model can be used as a classifier. */
  get canClassify(): boolean
  /** Get the raw model ID without provider prefix. */
  rawId(): string
  /** Calculate quality per dollar (higher is better value). */
  qualityPerDollar(): number
  /** Estimate cost for a request. */
  estimateCost(inputTokens: number, outputTokens: number): number
  /** Calculate weighted quality score from benchmarks (0-100). */
  qualityScore(): number
}

/** Request for content moderation. */
export declare class JsModerationRequest {
  /** Model identifier in "provider/model" format (e.g., "openai/omni-moderation-latest") */
  model: string
  text: string
  /** Create a new moderation request. */
  constructor(model: string, text: string)
}

/** Ranking result with score. */
export declare class JsRankedDocument {
  index: number
  document: string
  score: number
  /** Create a new ranked document. */
  constructor(index: number, document: string, score: number)
}

/** Request for document ranking. */
export declare class JsRankingRequest {
  /** Model identifier in "provider/model" format (e.g., "cohere/rerank-english-v3.0") */
  model: string
  query: string
  documents: Array<string>
  topK?: number
  /** Create a new ranking request. */
  constructor(model: string, query: string, documents: Array<string>)
  /** Set the number of top results to return. */
  withTopK(topK: number): JsRankingRequest
}

/**
 * OpenAI Realtime API provider.
 *
 * Creates and manages realtime sessions for bidirectional voice and text communication.
 */
export declare class JsRealtimeProvider {
  /** Create a new Realtime provider with the given API key. */
  constructor(apiKey: string, model?: string | undefined | null)
  /** Create from environment variable `OPENAI_API_KEY`. */
  static fromEnv(): JsRealtimeProvider
  /**
   * Create a new realtime session.
   *
   * Establishes a WebSocket connection to the OpenAI Realtime API.
   */
  createSession(config: JsSessionConfig): Promise<JsRealtimeSession>
  /** Get the model name. */
  get model(): string
}

/**
 * Active realtime session for bidirectional communication.
 *
 * Provides methods to send text and audio to the OpenAI Realtime API.
 */
export declare class JsRealtimeSession {
  /** Send a text message. */
  sendText(text: string): Promise<void>
  /**
   * Send audio data.
   *
   * The audio should be in the format specified by input_audio_format
   * in the session config (default: PCM16, 24kHz, mono).
   */
  sendAudio(audioData: Buffer): Promise<void>
  /**
   * Commit the audio buffer and trigger response generation.
   *
   * Call this after sending audio to indicate the end of input
   * and trigger the model to generate a response.
   */
  commitAudio(): Promise<void>
  /** Get the current session configuration. */
  getConfig(): Promise<JsSessionConfig>
  /** Update the session configuration. */
  updateConfig(config: JsSessionConfig): Promise<void>
}

/** Reranked result. */
export declare class JsRerankedResult {
  index: number
  document: string
  relevanceScore: number
  /** Create a new reranked result. */
  constructor(index: number, document: string, relevanceScore: number)
}

/** Request for reranking search results. */
export declare class JsRerankingRequest {
  /** Model identifier in "provider/model" format (e.g., "voyage/rerank-2") */
  model: string
  query: string
  documents: Array<string>
  topN?: number
  /** Create a new reranking request. */
  constructor(model: string, query: string, documents: Array<string>)
  /** Set the number of top results to return. */
  withTopN(topN: number): JsRerankingRequest
}

/**
 * Configuration for retry behavior on transient failures.
 *
 * @example
 * ```typescript
 * import { LLMKitClient, RetryConfig } from 'llmkit'
 *
 * // Use production defaults (10 retries, exponential backoff)
 * const client = LLMKitClient.fromEnv()
 *
 * // Use conservative config (3 retries, faster)
 * const client = new LLMKitClient({ retryConfig: RetryConfig.conservative() })
 *
 * // Disable retry entirely
 * const client = new LLMKitClient({ retryConfig: RetryConfig.none() })
 *
 * // Custom config
 * const client = new LLMKitClient({
 *   retryConfig: new RetryConfig({
 *     maxRetries: 5,
 *     initialDelayMs: 500,
 *     maxDelayMs: 10000,
 *   })
 * })
 * ```
 */
export declare class JsRetryConfig {
  /**
   * Create a custom retry configuration.
   *
   * @param options - Configuration options
   *
   * @example
   * ```typescript
   * const config = new RetryConfig({
   *   maxRetries: 5,
   *   initialDelayMs: 500,
   *   maxDelayMs: 10000,
   *   backoffMultiplier: 2.0,
   *   jitter: true,
   * })
   * ```
   */
  constructor(options?: RetryConfigOptions | undefined | null)
  /**
   * Production-ready config with aggressive retry.
   *
   * 10 retries with exponential backoff:
   * 1s -> 2s -> 4s -> 8s -> 16s -> 32s -> 64s -> 128s -> 256s -> 300s (capped)
   *
   * Total max wait time: ~13 minutes across all retries.
   */
  static production(): JsRetryConfig
  /**
   * Conservative config for latency-sensitive operations.
   *
   * 3 retries: 1s -> 2s -> 4s (max 30s)
   */
  static conservative(): JsRetryConfig
  /**
   * Disabled retry - operations fail immediately on first error.
   *
   * Use for testing or when retry is handled at a higher level.
   */
  static none(): JsRetryConfig
  /** Maximum number of retry attempts. */
  get maxRetries(): number
  /** Initial delay before first retry in milliseconds. */
  get initialDelayMs(): number
  /** Maximum delay between retries in milliseconds. */
  get maxDelayMs(): number
  /** Multiplier for exponential backoff. */
  get backoffMultiplier(): number
  /** Whether random jitter is added to delays. */
  get jitter(): boolean
}

/** A chunk from a streaming response. */
export declare class JsStreamChunk {
  /** The type of stream event. */
  get eventType(): JsStreamEventType
  /** Index of the content block being updated. */
  get index(): number | null
  /** The delta content (if applicable). */
  get delta(): JsContentDelta | null
  /** Convenience: Get text from delta if present. */
  get text(): string | null
  /** Stop reason (only on message_stop). */
  get stopReason(): JsStopReason | null
  /** Usage information (may be partial or final). */
  get usage(): JsUsage | null
  /** True if this is a message stop event. */
  get isDone(): boolean
}

/** Configuration for structured output format. */
export declare class JsStructuredOutput {
  /** Create a JSON schema structured output. */
  static jsonSchema(name: string, schema: any): JsStructuredOutput
  /** Create a JSON object structured output (no specific schema). */
  static jsonObject(): JsStructuredOutput
  /** Create a text structured output. */
  static text(): JsStructuredOutput
  /** Check if this is a JSON schema output. */
  get isJsonSchema(): boolean
  /** Check if this is a JSON object output. */
  get isJsonObject(): boolean
  /** Check if this is a text output. */
  get isTextOutput(): boolean
}

/** Request for text-to-speech synthesis. */
export declare class JsSynthesisRequest {
  text: string
  voiceId?: string
  model?: string
  /** Create a new synthesis request. */
  constructor(text: string)
  withVoice(voiceId: string): JsSynthesisRequest
  withModel(model: string): JsSynthesisRequest
}

/** Configuration for extended thinking mode. */
export declare class JsThinkingConfig {
  /** Enable extended thinking with a token budget. */
  static enabled(budgetTokens: number): JsThinkingConfig
  /**
   * Disable extended thinking/reasoning.
   *
   * This will disable reasoning for providers that support it:
   * - OpenRouter: Sets reasoning.effort to "none"
   * - DeepSeek: Sets enable_thinking to false
   * - Anthropic: Omits the thinking block
   */
  static disabled(): JsThinkingConfig
  /**
   * Create a thinking config with a specific effort level.
   *
   * Useful for providers like OpenRouter that support effort-based reasoning control.
   */
  static withEffort(effort: JsThinkingEffort): JsThinkingConfig
  /** Create a thinking config with effort level and token budget. */
  static withEffortAndBudget(effort: JsThinkingEffort, budgetTokens: number): JsThinkingConfig
  /** The thinking type (enabled or disabled). */
  get thinkingType(): JsThinkingType
  /** The token budget for thinking (if enabled). */
  get budgetTokens(): number | null
  /** The effort level for reasoning (if set). */
  get effort(): JsThinkingEffort | null
  /** Whether to exclude thinking from the response. */
  get excludeFromResponse(): boolean
  /** Check if thinking is enabled. */
  get isEnabled(): boolean
}

/**
 * Request to count tokens for a model.
 *
 * This allows estimation of token counts before making a completion request,
 * useful for cost estimation and context window management.
 *
 * @example
 * ```typescript
 * // Create from model and messages
 * const request = TokenCountRequest.create(
 *   "claude-sonnet-4-20250514",
 *   [Message.user("Hello, how are you?")]
 * ).withSystem("You are a helpful assistant")
 *
 * // Count tokens
 * const result = await client.countTokens(request)
 * console.log(`Input tokens: ${result.inputTokens}`)
 *
 * // Or create from existing completion request
 * const request = TokenCountRequest.fromCompletionRequest(completionRequest)
 * ```
 */
export declare class JsTokenCountRequest {
  /** Create a new token count request with model and messages. */
  static create(model: string, messages: Array<JsMessage>): JsTokenCountRequest
  /** Create a token count request from an existing completion request. */
  static fromCompletionRequest(request: JsCompletionRequest): JsTokenCountRequest
  /** Builder method: Set the system prompt. */
  withSystem(system: string): JsTokenCountRequest
  /** Builder method: Set the tools. */
  withTools(tools: Array<JsToolDefinition>): JsTokenCountRequest
  /** The model identifier. */
  get model(): string
  /** The conversation messages. */
  get messages(): Array<JsMessage>
  /** The system prompt (if set). */
  get system(): string | null
}

/**
 * Result of a token counting request.
 *
 * @example
 * ```typescript
 * const result = await client.countTokens(request)
 * console.log(`Input tokens: ${result.inputTokens}`)
 * ```
 */
export declare class JsTokenCountResult {
  /** Total number of input tokens. */
  get inputTokens(): number
}

/**
 * Builder for creating tool definitions with a fluent API.
 *
 * @example
 * ```typescript
 * const tool = new ToolBuilder("get_weather")
 *   .description("Get current weather")
 *   .stringParam("city", "City name", true)
 *   .enumParam("unit", "Temperature unit", ["celsius", "fahrenheit"])
 *   .build()
 * ```
 */
export declare class JsToolBuilder {
  /** Create a new tool builder. */
  constructor(name: string)
  /** Set the tool description. */
  description(description: string): JsToolBuilder
  /** Add a string parameter. */
  stringParam(name: string, description: string, required?: boolean | undefined | null): JsToolBuilder
  /** Add an integer parameter. */
  integerParam(name: string, description: string, required?: boolean | undefined | null): JsToolBuilder
  /** Add a number (float) parameter. */
  numberParam(name: string, description: string, required?: boolean | undefined | null): JsToolBuilder
  /** Add a boolean parameter. */
  booleanParam(name: string, description: string, required?: boolean | undefined | null): JsToolBuilder
  /** Add an array parameter. */
  arrayParam(name: string, description: string, itemType: string, required?: boolean | undefined | null): JsToolBuilder
  /** Add an enum parameter (string with allowed values). */
  enumParam(name: string, description: string, values: Array<string>, required?: boolean | undefined | null): JsToolBuilder
  /** Add a custom parameter with a JSON schema. */
  customParam(name: string, schema: any, required?: boolean | undefined | null): JsToolBuilder
  /** Build the tool definition. */
  build(): JsToolDefinition
}

/** Definition of a tool that can be used by the model. */
export declare class JsToolDefinition {
  /** Create a new tool definition. */
  constructor(name: string, description: string, inputSchema: any)
  /** The tool name. */
  get name(): string
  /** The tool description. */
  get description(): string
  /** The input schema. */
  get inputSchema(): any
}

/** Options for Deepgram transcription. */
export declare class JsTranscribeOptions {
  model?: string
  smartFormat: boolean
  diarize: boolean
  language?: string
  punctuate: boolean
  /** Create a new TranscribeOptions with defaults. */
  constructor()
  /** Set the Deepgram model to use. */
  withModel(model: string): JsTranscribeOptions
  /** Enable smart formatting for better punctuation and capitalization. */
  withSmartFormat(enabled: boolean): JsTranscribeOptions
  /** Enable speaker diarization to identify different speakers. */
  withDiarize(enabled: boolean): JsTranscribeOptions
  /** Set the language of the audio. */
  withLanguage(language: string): JsTranscribeOptions
  /** Enable automatic punctuation addition. */
  withPunctuate(enabled: boolean): JsTranscribeOptions
}

/** Configuration for AssemblyAI transcription. */
export declare class JsTranscriptionConfig {
  language?: JsAudioLanguage
  enableDiarization: boolean
  enableEntityDetection: boolean
  enableSentimentAnalysis: boolean
  /** Create new transcription configuration with defaults. */
  constructor()
  /** Set the language for transcription. */
  withLanguage(language: JsAudioLanguage): JsTranscriptionConfig
  /** Enable speaker diarization. */
  withDiarization(enabled: boolean): JsTranscriptionConfig
  /** Enable entity detection. */
  withEntityDetection(enabled: boolean): JsTranscriptionConfig
  /** Enable sentiment analysis. */
  withSentimentAnalysis(enabled: boolean): JsTranscriptionConfig
}

/** Request for audio transcription. */
export declare class JsTranscriptionRequest {
  audioBytes: Array<number>
  model?: string
  language?: string
  /** Create a new transcription request. */
  constructor(audioBytes: Array<number>)
  withModel(model: string): JsTranscriptionRequest
  withLanguage(language: string): JsTranscriptionRequest
}

/** Token usage information. */
export declare class JsUsage {
  /** Number of tokens in the prompt. */
  get inputTokens(): number
  /** Number of tokens in the completion. */
  get outputTokens(): number
  /** Cache creation tokens (if applicable). */
  get cacheCreationInputTokens(): number
  /** Cache read tokens (if applicable). */
  get cacheReadInputTokens(): number
  /** Total tokens used (input + output). */
  totalTokens(): number
}

/**
 * Voice Activity Detection configuration.
 *
 * Controls how the Realtime API detects when the user has stopped speaking.
 */
export declare class JsVadConfig {
  /** Silence duration in milliseconds to trigger end-of-turn (default: 500) */
  silenceDurationMs: number
  /** Threshold for voice detection (0.0 to 1.0) */
  threshold?: number
  /** Create a new VAD configuration. */
  constructor(silenceDurationMs?: number | undefined | null, threshold?: number | undefined | null)
  /** Create default VAD configuration. */
  static default(): JsVadConfig
}

/** Options for video generation. */
export declare class JsVideoGenerationOptions {
  model?: string
  duration?: number
  width?: number
  height?: number
  quality?: string
  /** Create a new VideoGenerationOptions with defaults. */
  constructor()
  /** Set the video model to use. */
  withModel(model: string): JsVideoGenerationOptions
  /** Set the video duration in seconds. */
  withDuration(duration: number): JsVideoGenerationOptions
  /** Set the video width in pixels. */
  withWidth(width: number): JsVideoGenerationOptions
  /** Set the video height in pixels. */
  withHeight(height: number): JsVideoGenerationOptions
  /** Set the video quality. */
  withQuality(quality: string): JsVideoGenerationOptions
}

/** Request for video generation. */
export declare class JsVideoGenerationRequest {
  prompt: string
  model?: string
  duration?: number
  width?: number
  height?: number
  /** Create a new video generation request. */
  constructor(prompt: string)
  withModel(model: string): JsVideoGenerationRequest
  withDuration(duration: number): JsVideoGenerationRequest
  withWidth(width: number): JsVideoGenerationRequest
  withHeight(height: number): JsVideoGenerationRequest
}

/** Response from a video generation request. */
export declare class JsVideoGenerationResponse {
  videoBytes?: Array<number>
  videoUrl?: string
  format: string
  duration?: number
  width?: number
  height?: number
  taskId?: string
  status?: string
  /** Size of the video in bytes (if available). */
  get size(): number
}

/**
 * Get all models in the registry.
 *
 * @example
 * ```typescript
 * import { getAllModels } from 'llmkit';
 *
 * const models = getAllModels();
 * console.log(`Registry contains ${models.length} models`);
 * ```
 */
export declare function getAllModels(): Array<JsModelInfo>

/**
 * Get available models (provider API key is configured).
 *
 * @example
 * ```typescript
 * import { getAvailableModels } from 'llmkit';
 *
 * const available = getAvailableModels();
 * console.log(`${available.length} models available with current credentials`);
 * ```
 */
export declare function getAvailableModels(): Array<JsModelInfo>

/**
 * Get the cheapest model that meets requirements.
 *
 * @param minContext - Minimum context window size (null for any).
 * @param needsVision - Whether vision support is required.
 * @param needsTools - Whether tool calling support is required.
 *
 * @example
 * ```typescript
 * import { getCheapestModel } from 'llmkit';
 *
 * // Get cheapest model with at least 100k context
 * const cheapest = getCheapestModel(100000, false, true);
 * if (cheapest) {
 *   console.log(`Cheapest: ${cheapest.name} at $${cheapest.pricing.inputPer1m}/1M`);
 * }
 * ```
 */
export declare function getCheapestModel(minContext: number | undefined | null, needsVision: boolean, needsTools: boolean): JsModelInfo | null

/**
 * Get models that can be used as classifiers (fast, cheap, good instruction following).
 *
 * @example
 * ```typescript
 * import { getClassifierModels } from 'llmkit';
 *
 * const classifiers = getClassifierModels();
 * for (const model of classifiers) {
 *   console.log(`${model.name}: $${model.pricing.inputPer1m}/1M tokens`);
 * }
 * ```
 */
export declare function getClassifierModels(): Array<JsModelInfo>

/**
 * Get all current (non-deprecated) models.
 *
 * @example
 * ```typescript
 * import { getCurrentModels } from 'llmkit';
 *
 * const current = getCurrentModels();
 * console.log(`${current.length} current models available`);
 * ```
 */
export declare function getCurrentModels(): Array<JsModelInfo>

/**
 * Get model info by ID, alias, or raw ID.
 *
 * @example
 * ```typescript
 * import { getModelInfo } from 'llmkit';
 *
 * const info = getModelInfo('claude-sonnet-4-5');
 * if (info) {
 *   console.log(`${info.name}: $${info.pricing.inputPer1m}/1M input tokens`);
 *   console.log(`Context: ${info.capabilities.maxContext} tokens`);
 * }
 * ```
 */
export declare function getModelInfo(modelId: string): JsModelInfo | null

/**
 * Get all models for a specific provider.
 *
 * @example
 * ```typescript
 * import { getModelsByProvider, Provider } from 'llmkit';
 *
 * const anthropicModels = getModelsByProvider(Provider.Anthropic);
 * for (const model of anthropicModels) {
 *   console.log(`${model.name}: ${model.description}`);
 * }
 * ```
 */
export declare function getModelsByProvider(provider: JsProvider): Array<JsModelInfo>

/**
 * Get models with specific capabilities.
 *
 * @param vision - Filter by vision support (null to ignore).
 * @param tools - Filter by tool calling support (null to ignore).
 * @param thinking - Filter by extended thinking support (null to ignore).
 *
 * @example
 * ```typescript
 * import { getModelsWithCapability } from 'llmkit';
 *
 * // Get all vision models
 * const visionModels = getModelsWithCapability(true, null, null);
 *
 * // Get models with extended thinking
 * const thinkingModels = getModelsWithCapability(null, null, true);
 * ```
 */
export declare function getModelsWithCapability(vision?: boolean | undefined | null, tools?: boolean | undefined | null, thinking?: boolean | undefined | null): Array<JsModelInfo>

/**
 * Get registry statistics.
 *
 * @example
 * ```typescript
 * import { getRegistryStats } from 'llmkit';
 *
 * const stats = getRegistryStats();
 * console.log(`Registry: ${stats.totalModels} models from ${stats.providers} providers`);
 * ```
 */
export declare function getRegistryStats(): JsRegistryStats

/** Language for AssemblyAI transcription. */
export declare const enum JsAudioLanguage {
  English = 0,
  Spanish = 1,
  French = 2,
  German = 3,
  ChineseSimplified = 4,
  ChineseTraditional = 5,
  Japanese = 6
}

/** Batch job status. */
export declare const enum JsBatchStatus {
  /** Batch is being validated */
  Validating = 0,
  /** Batch is in progress */
  InProgress = 1,
  /** Batch is finalizing */
  Finalizing = 2,
  /** Batch completed successfully */
  Completed = 3,
  /** Batch failed */
  Failed = 4,
  /** Batch expired */
  Expired = 5,
  /** Batch was cancelled */
  Cancelled = 6
}

/** Cache control type for prompt caching. */
export declare const enum JsCacheControl {
  /** 5-minute TTL cache */
  Ephemeral = 0,
  /** 1-hour TTL cache (Anthropic beta) */
  Extended = 1
}

/** Response from classification request. */
export interface JsClassificationResponse {
  results: Array<JsClassificationResult>
}

/** Classification result with label and confidence. */
export interface JsClassificationResult {
  label: string
  confidence: number
}

/** Deepgram API version for selecting model features and endpoints. */
export declare const enum JsDeepgramVersion {
  /** API v1 (2023-12-01) - legacy support */
  V1 = 0,
  /** API v3 (2025-01-01) - latest with Nova-3 models */
  V3 = 1
}

/** Input type hint for embedding optimization. */
export declare const enum JsEmbeddingInputType {
  /** The input is a search query. */
  Query = 0,
  /** The input is a document to be indexed. */
  Document = 1
}

/** Output encoding format for embeddings. */
export declare const enum JsEncodingFormat {
  /** Float32 array (default). */
  Float = 0,
  /** Base64-encoded binary. */
  Base64 = 1
}

/** Response format for generated images. */
export declare const enum JsImageFormat {
  /** Return URL to the image. */
  Url = 0,
  /** Return base64-encoded image data. */
  B64Json = 1
}

/** Response from an image generation request. */
export interface JsImageGenerationResponse {
  created: number
  images: Array<JsGeneratedImage>
}

/** Image quality options. */
export declare const enum JsImageQuality {
  /** Standard quality (faster, cheaper). */
  Standard = 0,
  /** HD quality (more detail). */
  Hd = 1
}

/** Image size options. */
export declare const enum JsImageSize {
  /** 256x256 pixels (DALL-E 2 only). */
  Square256 = 0,
  /** 512x512 pixels (DALL-E 2 only). */
  Square512 = 1,
  /** 1024x1024 pixels (default). */
  Square1024 = 2,
  /** 1024x1792 pixels (portrait). */
  Portrait1024x1792 = 3,
  /** 1792x1024 pixels (landscape). */
  Landscape1792x1024 = 4
}

/** Image style options (DALL-E 3 only). */
export declare const enum JsImageStyle {
  /** Natural-looking images. */
  Natural = 0,
  /** More dramatic, vivid style. */
  Vivid = 1
}

/** Latency mode for ElevenLabs synthesis. */
export declare const enum JsLatencyMode {
  /** Lowest possible latency (fastest) */
  LowestLatency = 0,
  /** Low latency */
  LowLatency = 1,
  /** Balanced (default) */
  Balanced = 2,
  /** High quality */
  HighQuality = 3,
  /** Highest quality (slowest) */
  HighestQuality = 4
}

/** Benchmark scores (0-100 scale, higher is better). */
export interface JsModelBenchmarks {
  /** MMLU - General knowledge. */
  mmlu?: number
  /** HumanEval - Code generation. */
  humaneval?: number
  /** MATH - Mathematical reasoning. */
  math?: number
  /** GPQA Diamond - Graduate-level science. */
  gpqa?: number
  /** SWE-bench - Software engineering. */
  sweBench?: number
  /** IFEval - Instruction following. */
  ifeval?: number
  /** MMMU - Multimodal understanding. */
  mmmu?: number
  /** MGSM - Multilingual math. */
  mgsm?: number
  /** Time to first token (ms). */
  ttftMs?: number
  /** Tokens per second. */
  tokensPerSec?: number
}

/** Model capabilities. */
export interface JsModelCapabilities {
  /** Maximum input context size in tokens. */
  maxContext: number
  /** Maximum output tokens. */
  maxOutput: number
  /** Supports vision/image input. */
  vision: boolean
  /** Supports tool/function calling. */
  tools: boolean
  /** Supports streaming responses. */
  streaming: boolean
  /** Supports JSON mode. */
  jsonMode: boolean
  /** Supports structured output with JSON schema enforcement. */
  structuredOutput: boolean
  /** Supports extended thinking/reasoning. */
  thinking: boolean
  /** Supports prompt caching. */
  caching: boolean
}

/** Model pricing (per 1M tokens in USD). */
export interface JsModelPricing {
  /** Input token price per 1M tokens. */
  inputPer1M: number
  /** Output token price per 1M tokens. */
  outputPer1M: number
  /** Cached input token price per 1M tokens (if supported). */
  cachedInputPer1M?: number
}

/** Model availability status. */
export declare const enum JsModelStatus {
  /** Currently recommended model. */
  Current = 'Current',
  /** Still available but superseded by newer version. */
  Legacy = 'Legacy',
  /** Scheduled for removal, not recommended for new projects. */
  Deprecated = 'Deprecated'
}

/** Response from moderation request. */
export interface JsModerationResponse {
  flagged: boolean
  scores: JsModerationScores
}

/** Moderation category scores. */
export interface JsModerationScores {
  hate: number
  hateThreatening: number
  harassment: number
  harassmentThreatening: number
  selfHarm: number
  selfHarmIntent: number
  selfHarmInstructions: number
  sexual: number
  sexualMinors: number
  violence: number
  violenceGraphic: number
}

/** LLM Provider identifier. */
export declare const enum JsProvider {
  Anthropic = 'Anthropic',
  OpenAI = 'OpenAI',
  Google = 'Google',
  Mistral = 'Mistral',
  Groq = 'Groq',
  DeepSeek = 'DeepSeek',
  Cohere = 'Cohere',
  Bedrock = 'Bedrock',
  AzureOpenAI = 'AzureOpenAI',
  VertexAI = 'VertexAI',
  TogetherAI = 'TogetherAI',
  OpenRouter = 'OpenRouter',
  Cerebras = 'Cerebras',
  SambaNova = 'SambaNova',
  Fireworks = 'Fireworks',
  AI21 = 'AI21',
  HuggingFace = 'HuggingFace',
  Replicate = 'Replicate',
  Cloudflare = 'Cloudflare',
  Databricks = 'Databricks',
  Writer = 'Writer',
  Maritaca = 'Maritaca',
  NaverClova = 'NaverClova',
  Yandex = 'Yandex',
  GigaChat = 'GigaChat',
  Upstage = 'Upstage',
  SeaLion = 'SeaLion',
  Alibaba = 'Alibaba',
  AlephAlpha = 'AlephAlpha',
  Baidu = 'Baidu',
  Baseten = 'Baseten',
  DataRobot = 'DataRobot',
  LightOn = 'LightOn',
  NLPCloud = 'NLPCloud',
  Oracle = 'Oracle',
  Perplexity = 'Perplexity',
  RunPod = 'RunPod',
  SapAICore = 'SapAICore',
  Snowflake = 'Snowflake',
  Vllm = 'Vllm',
  WatsonX = 'WatsonX',
  Xai = 'Xai',
  DeepInfra = 'DeepInfra',
  NvidiaNIM = 'NvidiaNIM',
  Ollama = 'Ollama',
  Anyscale = 'Anyscale',
  GitHubModels = 'GitHubModels',
  FriendliAI = 'FriendliAI',
  Hyperbolic = 'Hyperbolic',
  LambdaLabs = 'LambdaLabs',
  Novita = 'Novita',
  Nebius = 'Nebius',
  Lepton = 'Lepton',
  Stability = 'Stability',
  Voyage = 'Voyage',
  Jina = 'Jina',
  Deepgram = 'Deepgram',
  ElevenLabs = 'ElevenLabs',
  GPT4All = 'GPT4All',
  Hunyuan = 'Hunyuan',
  MiniMax = 'MiniMax',
  Moonshot = 'Moonshot',
  Zhipu = 'Zhipu',
  Volcengine = 'Volcengine',
  Baichuan = 'Baichuan',
  Stepfun = 'Stepfun',
  Yi = 'Yi',
  Spark = 'Spark',
  LMStudio = 'LMStudio',
  Llamafile = 'Llamafile',
  Xinference = 'Xinference',
  LocalAI = 'LocalAI',
  Jan = 'Jan',
  Petals = 'Petals',
  Tgi = 'Tgi',
  Predibase = 'Predibase',
  OctoAI = 'OctoAI',
  Featherless = 'Featherless',
  OVHCloud = 'OVHCloud',
  Scaleway = 'Scaleway',
  Crusoe = 'Crusoe',
  Cerebrium = 'Cerebrium',
  Lightning = 'Lightning',
  AssemblyAI = 'AssemblyAI',
  RunwayML = 'RunwayML',
  Kakao = 'Kakao',
  LGExaone = 'LGExaone',
  PLaMo = 'PLaMo',
  Sarvam = 'Sarvam',
  Krutrim = 'Krutrim',
  NttTsuzumi = 'NttTsuzumi',
  SoftBankSarashina = 'SoftBankSarashina',
  Ionos = 'Ionos',
  Tilde = 'Tilde',
  SiloAI = 'SiloAI',
  SwissAI = 'SwissAI',
  Unify = 'Unify',
  Martian = 'Martian',
  Portkey = 'Portkey',
  Helicone = 'Helicone',
  SiliconFlow = 'SiliconFlow',
  Pika = 'Pika',
  Luma = 'Luma',
  Kling = 'Kling',
  HeyGen = 'HeyGen',
  DId = 'DId',
  TwelveLabs = 'TwelveLabs',
  RevAI = 'RevAI',
  Speechmatics = 'Speechmatics',
  PlayHT = 'PlayHT',
  Resemble = 'Resemble',
  Leonardo = 'Leonardo',
  Ideogram = 'Ideogram',
  BlackForestLabs = 'BlackForestLabs',
  Clarifai = 'Clarifai',
  Fal = 'Fal',
  Recraft = 'Recraft',
  Modal = 'Modal',
  Beam = 'Beam',
  Nscale = 'Nscale',
  Runware = 'Runware',
  AI71 = 'AI71',
  Local = 'Local',
  Custom = 'Custom'
}

/** Response from ranking request. */
export interface JsRankingResponse {
  results: Array<JsRankedDocument>
}

/** Registry statistics. */
export interface JsRegistryStats {
  /** Total number of models in the registry. */
  totalModels: number
  /** Number of current (non-deprecated) models. */
  currentModels: number
  /** Number of providers. */
  providers: number
  /** Number of models available (API key configured). */
  availableModels: number
}

/** Response from reranking request. */
export interface JsRerankingResponse {
  results: Array<JsRerankedResult>
}

/** Message role in a conversation. */
export declare const enum JsRole {
  /** System message providing context or instructions */
  System = 0,
  /** User message */
  User = 1,
  /** Assistant (LLM) message */
  Assistant = 2
}

/**
 * Configuration for a realtime session.
 *
 * Configures the behavior of the OpenAI Realtime API session including
 * modality, voice, audio format, and VAD settings.
 */
export interface JsSessionConfig {
  /** Voice model to use (e.g., "gpt-4o-realtime-preview") */
  model?: string
  /** Modality: "text-and-audio", "text-only", or "audio-only" */
  modalities: Array<string>
  /** Instructions for the model */
  instructions?: string
  /** Voice to use for audio output: "alloy", "echo", "shimmer" */
  voice: string
  /** Input audio encoding: "pcm16" or "g711_ulaw" */
  inputAudioFormat: string
  /** Output audio encoding: "pcm16" or "g711_ulaw" */
  outputAudioFormat: string
  /** Voice activity detection (VAD) configuration */
  voiceActivityDetection?: JsVadConfig
  /** Maximum output tokens */
  maxResponseOutputTokens?: number
  /** Tool choice: "auto", "required", or "none" */
  toolChoice?: string
  /** Temperature for sampling */
  temperature?: number
}

/** Reason the model stopped generating. */
export declare const enum JsStopReason {
  /** Natural end of response */
  EndTurn = 0,
  /** Hit max tokens limit */
  MaxTokens = 1,
  /** Model wants to use a tool */
  ToolUse = 2,
  /** Hit a stop sequence */
  StopSequence = 3,
  /** Response was filtered by content moderation */
  ContentFilter = 4
}

/** Streaming event type. */
export declare const enum JsStreamEventType {
  /** Message started */
  MessageStart = 0,
  /** Content block started */
  ContentBlockStart = 1,
  /** Content block delta (partial content) */
  ContentBlockDelta = 2,
  /** Content block stopped */
  ContentBlockStop = 3,
  /** Message delta */
  MessageDelta = 4,
  /** Message stopped */
  MessageStop = 5,
  /** Ping event */
  Ping = 6,
  /** Error event */
  Error = 7
}

/** Options for ElevenLabs text-to-speech synthesis. */
export interface JsSynthesizeOptions {
  modelId?: string
  voiceSettings?: JsVoiceSettings
  latencyMode: JsLatencyMode
  outputFormat?: string
}

/** Response from text-to-speech synthesis. */
export interface JsSynthesizeResponse {
  audioBytes: Array<number>
  format: string
  duration?: number
}

/**
 * Thinking/reasoning effort level.
 *
 * Controls how much computational effort the model spends on reasoning.
 * Supported by providers like OpenRouter that offer reasoning effort controls.
 */
export declare const enum JsThinkingEffort {
  /** Minimal reasoning effort */
  Low = 0,
  /** Balanced reasoning effort (default) */
  Medium = 1,
  /** High reasoning effort */
  High = 2,
  /** Maximum reasoning effort */
  Max = 3
}

/** Thinking mode type. */
export declare const enum JsThinkingType {
  /** Extended thinking is enabled */
  Enabled = 0,
  /** Extended thinking is disabled */
  Disabled = 1
}

/** Tool result information returned from asToolResult(). */
export interface JsToolResultInfo {
  toolUseId: string
  content: string
  isError: boolean
}

/** Tool use delta information. */
export interface JsToolUseDelta {
  id?: string
  name?: string
  inputJsonDelta?: string
}

/** Tool use information returned from asToolUse(). */
export interface JsToolUseInfo {
  id: string
  name: string
  input: any
}

/** Response from a transcription request. */
export interface JsTranscribeResponse {
  transcript: string
  confidence?: number
  words: Array<JsWord>
  duration?: number
  metadata?: string
}

/** Video generation models supported by Runware. */
export declare const enum JsVideoModel {
  /** RunwayML Gen-4.5 */
  RunwayGen45 = 0,
  /** Kling Video Generation */
  Kling20 = 1,
  /** Pika 1.0 */
  Pika10 = 2,
  /** Hailuo Mini Video */
  HailuoMini = 3,
  /** Leonardo Diffusion Ultra */
  LeonardoUltra = 4
}

/** Information about an available voice. */
export interface JsVoice {
  voiceId: string
  name: string
  category?: string
  description?: string
}

/** Voice settings for ElevenLabs synthesis. */
export interface JsVoiceSettings {
  stability: number
  similarityBoost: number
  style?: number
  useSpeakerBoost: boolean
}

/** A single word from transcription with timing and confidence. */
export interface JsWord {
  word: string
  start: number
  end: number
  confidence: number
  speaker?: number
}

/**
 * List all providers with at least one model.
 *
 * @example
 * ```typescript
 * import { listProviders } from 'llmkit';
 *
 * const providers = listProviders();
 * console.log(`Supported providers: ${providers.join(', ')}`);
 * ```
 */
export declare function listProviders(): Array<JsProvider>

/** Options for creating an LLMKitClient. */
export interface LlmKitClientOptions {
  /**
   * Provider configurations (key is provider name, value is config)
   * Supported providers: anthropic, openai, azure, bedrock, vertex, google,
   * groq, mistral, cohere, ai21, deepseek, together, fireworks, perplexity,
   * cerebras, sambanova, openrouter, ollama, huggingface, replicate,
   * cloudflare, watsonx, databricks, baseten, runpod, anyscale, deepinfra,
   * novita, hyperbolic, lm_studio, vllm, tgi, llamafile
   */
  providers?: Record<string, ProviderConfig>
  /** Default provider name */
  defaultProvider?: string
}

/** Configuration for a single provider. */
export interface ProviderConfig {
  /** API key for the provider */
  apiKey?: string
  /** Secret key for providers that require it (e.g., Baidu) */
  secretKey?: string
  /** Custom base URL (optional) */
  baseUrl?: string
  /** Azure OpenAI endpoint */
  endpoint?: string
  /** Azure OpenAI deployment name */
  deployment?: string
  /** AWS region for Bedrock, or location for Vertex */
  region?: string
  /** Google Cloud project ID for Vertex */
  project?: string
  /** Location for Vertex AI */
  location?: string
  /** Cloudflare account ID */
  accountId?: string
  /** Cloudflare API token */
  apiToken?: string
  /** Vertex AI access token (deprecated: use ADC via GOOGLE_APPLICATION_CREDENTIALS instead) */
  accessToken?: string
  /** Databricks host URL */
  host?: string
  /** Databricks token */
  token?: string
  /** RunPod endpoint ID */
  endpointId?: string
  /** Model ID (for openai_compatible) */
  modelId?: string
}

/** Options for creating a RetryConfig. */
export interface RetryConfigOptions {
  /** Maximum number of retry attempts (default: 10) */
  maxRetries?: number
  /** Initial delay before first retry in milliseconds (default: 1000) */
  initialDelayMs?: number
  /** Maximum delay between retries in milliseconds (default: 300000) */
  maxDelayMs?: number
  /** Multiplier for exponential backoff (default: 2.0) */
  backoffMultiplier?: number
  /** Whether to add random jitter to delays (default: true) */
  jitter?: boolean
}

/**
 * Check if a model supports structured output (JSON schema enforcement).
 *
 * @example
 * ```typescript
 * import { supportsStructuredOutput } from 'llmkit';
 *
 * if (supportsStructuredOutput('gpt-4o')) {
 *   // Use structured output with JSON schema
 * } else {
 *   // Fall back to text-based parsing
 * }
 * ```
 */
export declare function supportsStructuredOutput(modelId: string): boolean
