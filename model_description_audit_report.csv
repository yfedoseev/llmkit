Model ID,Model Name,Description,Issue Found?,Specific Issue,Severity
deepseek/deepseek-reasoner,DeepSeek R1,Advanced reasoning with 71% AIME pass rate,YES,"HALLUCINATED STATISTIC: Description claims 71% AIME pass rate, but official sources show 79.8% Pass@1 on AIME 2024, and newer R1-0528 achieves 87.5% on AIME 2025. The 71% figure appears to be made up or from an intermediate version.",High
mistral/mistral-large-2512,Mistral Large 3,675B MoE flagship with EU regional support,YES,"PARTIALLY ACCURATE BUT MISLEADING: Model has 675B total parameters but only 41B ACTIVE parameters (Mixture of Experts). Description omits active parameters entirely, which is critical information. Also, ""2512"" in model ID suggests 2025-12, but description uses ""Mistral Large 3"" which may not match official naming.",High
cohere/command-r-08-2024,Command R,32B affordable,YES,"HALLUCINATED PARAMETER COUNT: Command R is 104B parameters, not 32B. The description appears to have confused parameter sizes or fabricated the 32B claim. This is a factual error.",Critical
cohere/command-r-plus-08-2024,Command R+,Enterprise RAG,PARTIAL,"MISSING CRITICAL SPECS: Description is extremely vague and omits that Command R+ is 104B parameters. While ""Enterprise RAG"" is accurate (model supports RAG), the description fails to mention model size, making it incomplete and insufficiently descriptive.",Medium
anthropic/claude-3-7-sonnet-20250219,Claude 3.7 Sonnet,Hybrid reasoning model,PARTIAL,"INCOMPLETE DESCRIPTION: While ""hybrid reasoning"" is technically correct (it can do both standard and extended thinking), the description omits key features: it's the first hybrid reasoning model from Anthropic, supports up to 128K output tokens, and allows users to control thinking budget. The description undersells the model's capabilities.",Medium
google/gemini-3-pro,Gemini 3 Pro,Latest flagship with deep think reasoning,PARTIAL,"VAGUE/POTENTIALLY INACCURATE: Description says ""deep think reasoning"" but official Google documentation emphasizes ""thinking_level parameter"" for controlling reasoning depth, not ""deep think"" as a fixed feature. Also uses ""deep think"" which is not official Google terminology; they use ""extended thinking"".",Low
google/gemini-3-flash,Gemini 3 Flash,High-speed reasoning with deep think,PARTIAL,"TERMINOLOGY ISSUE: Uses unofficial term ""deep think"" instead of official Google term ""extended thinking"". While the capability exists, the terminology is inaccurate to official documentation.",Low
google/gemini-2.0-flash,Gemini 2.0 Flash,Ultra-fast multimodal with extended thinking,PARTIAL,"CONTRADICTION: Description claims ""extended thinking"" support, but Gemini 2.0 Flash primarily focuses on speed, not extended thinking. Extended thinking (experimental) is available on other Gemini models but not standard on 2.0 Flash.",Medium
ai21/jamba-2.0-large,Jamba 2.0 Large,256K hybrid SSM,PARTIAL,"INCOMPLETE BUT TECHNICALLY CORRECT: Description mentions ""256K"" context and ""hybrid SSM"" which are accurate. However, it omits important specs: actual parameters (not disclosed in benchmarks) and that it's an SSM-Transformer hybrid. Information is sparse but not false.",Low
sea-lion/Qwen-SEA-LION-v4-32B-IT,SEA-LION v4 32B,11 SEA languages,PARTIALLY CORRECT,"TECHNICALLY CORRECT BUT MISLEADING: SEA-LION v3 supports 13 languages (11 in pre-training + 2 in SFT). The description claims 11, which is incomplete for v3. For v4, the exact language count may differ. Description is too sparse and potentially understates capabilities.",Low
bedrock/amazon.nova-pro-v1:0,Amazon Nova Pro,Best accuracy/cost,PARTIAL,"MARKETING CLAIM WITHOUT SPECIFICS: Description is marketing language without technical detail. While Nova Pro does have good accuracy/cost ratio, the description provides zero specifications (context, parameters, capabilities). Too vague to be useful as a model description.",Medium
bedrock/amazon.nova-lite-v1:0,Amazon Nova Lite,Cost-effective,PARTIAL,"VAGUE MARKETING LANGUAGE: Description is too generic. Official specs show it supports 300K tokens, is multimodal, and excels at video/chart understanding, but none of this is mentioned. Description provides almost no useful information.",Medium
alibaba/qwen-max,Qwen Max,Flagship reasoning model with official pricing,PARTIAL,"MISLEADING: Description emphasizes ""with official pricing"" which is metadata about documentation, not a feature. Official sources show Qwen Max (now part of Qwen3 family) is part of the Qwen3 line which has 1T+ parameters with MoE architecture, but description provides no specification details.",Medium
openai/o1,o1,Extended thinking for complex problems,PARTIAL,"INCOMPLETE: While accurate that o1 uses extended thinking, description omits critical specs: it requires 10-30 second round-trips for complex problems, uses 25K+ tokens for reasoning budget, and has different pricing ($15-60/input, $60-240/output). Description undersells latency tradeoff.",Low
openai/gpt-4.1,GPT-4.1,1M context model,UNKNOWN,"SUSPICIOUS: OpenAI does not publicly document a ""gpt-4.1"" model. This appears to be a fabricated or internal model name. Official OpenAI models are gpt-4, gpt-4-turbo, gpt-4o, etc. This model ID and description may be entirely made up.",Critical
openai/gpt-4.1-mini,GPT-4.1 Mini,Fast 1M context,UNKNOWN,"SUSPICIOUS: Like gpt-4.1, ""gpt-4.1-mini"" does not appear in official OpenAI documentation. This appears to be fabricated or internal naming. OpenAI does not have a "".1"" versioning scheme in their released models.",Critical
mistral/codestral-2501,Codestral,Code specialist model,PARTIAL,"INCOMPLETE: Description is accurate but sparse. Official specs show Codestral is 22B parameters, supports 80+ programming languages, and has 256K context (Codestral 25.01). Description omits critical specifications.",Low
google/gemini-1.5-pro,Gemini 1.5 Pro,2M context,YES,"OUTDATED: While Gemini 1.5 Pro now supports 2M token context window, the description ""2M context"" lacks critical detail that this was expanded from 1M, and doesn't mention it's a milestone model. Description is too minimal and provides no differentiation.",Low
baidu/ernie-4.5-turbo-128k,ERNIE 4.5 Turbo,Official ERNIE 4.5 pricing from Qianfan,PARTIAL,"METADATA AS DESCRIPTION: Description mentions ""pricing from Qianfan"" which is metadata about pricing source, not a model feature. Official specs show ERNIE 4.5 has 128K context, but description provides zero technical specifications. Completely inadequate description.",High
zhipu/glm-4.7,GLM 4.7,Latest GLM with official pricing,PARTIAL,"METADATA AS DESCRIPTION: Like other regional models, description emphasizes ""official pricing"" (metadata) rather than actual features. Provides no technical specifications about the model's capabilities or architecture.",Medium
moonshot/kimi-k2,Kimi K2,Extended context with official pricing,PARTIAL,"METADATA-FOCUSED: Description emphasizes ""official pricing"" instead of features. While ""extended context"" is mentioned (200K tokens), it provides no differentiation or key capabilities.",Medium
cerebras/llama-3.1-8b,Llama 3.1 8B (Cerebras),Fastest small model,PARTIAL,"VAGUE MARKETING CLAIM: ""Fastest small model"" is a claim without context. Cerebras is fast due to hardware, not the model. Description provides no specifications about the underlying Llama 3.1 8B model or why Cerebras variant would be faster.",Medium
sambanova/llama-3.3-70b,Llama 3.3 70B (SambaNova),Ultra-fast,YES,"VAGUE AND PROVIDER-CENTRIC: Description only mentions speed without any model specifications. ""Ultra-fast"" is a marketing claim about the provider's hardware (SambaNova), not the model itself. No technical details about Llama 3.3 70B capabilities.",Medium
fireworks/llama-3.3-70b,Llama 3.3 70B (Fireworks),Fast inference,YES,"GENERIC/PROVIDER-FOCUSED: Description ""Fast inference"" is about Fireworks infrastructure, not the Llama 3.3 70B model itself. Provides zero information about model capabilities, parameters, or distinctions.",Medium
