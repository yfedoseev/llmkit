// Auto-generated MODEL_DATA entries
// Generated by compile_all_models.py
// Total models: 1969

const MODEL_DATA_NEW: &[&str] = &[
    // === # ACTION RECOGNITION  (1 models) ===
        "# ACTION RECOGNITION / VIDEO|None|None|None|-/-|-/-|-|-/-/-||chat",

    // === # BETA (1 models) ===
        "# BETA/PREVIEW MODELS|None|None|None|-/-|-/-|-|-/-/-||chat",

    // === # EDGE (1 models) ===
        "# EDGE/MOBILE/LIGHTWEIGHT MODELS (sub-2B parameters)|None|None|None|-/-|-/-|-|-/-/-||chat",

    // === # GGML (1 models) ===
        "# GGML/GGUF VARIANTS FOR LOCAL DEPLOYMENT|None|None|None|-/-|-/-|-|-/-/-||chat",

    // === # MUSIC GENERATION  (1 models) ===
        "# MUSIC GENERATION / ANALYSIS|None|None|None|-/-|-/-|-|-/-/-||chat",

    // === # NOISE REDUCTION  (1 models) ===
        "# NOISE REDUCTION / ENHANCEMENT|None|None|None|-/-|-/-|-|-/-/-||chat",

    // === # QUANTIZED (1 models) ===
        "# QUANTIZED/COMPRESSED VARIANTS|None|None|None|-/-|-/-|-|-/-/-||chat",

    // === # REINFORCEMENT LEARNING  (1 models) ===
        "# REINFORCEMENT LEARNING / POLICY MODELS|None|None|None|-/-|-/-|-|-/-/-||chat",

    // === # TABULAR DATA (1 models) ===
        "# TABULAR DATA/STRUCTURED|None|None|None|-/-|-/-|-|-/-/-||chat",

    // === # TIME SERIES  (1 models) ===
        "# TIME SERIES / FORECASTING|None|None|None|-/-|-/-|-|-/-/-||chat",

    // === ADAPTER (2 models) ===
        "adapter/llama-medical-lora|llama-medical-lora-a|Meta: Llama Medical LoRA|C|0.000600/0.000600|8K/2K|VSTJ|-/-/-|Llama 3 with medical LoRA|chat",
        "adapter/mistral-legal-lora|mistral-legal-lora-a|Mistral: Legal LoRA|C|0.000400/0.000400|8K/2K|VSTJ|-/-/-|Mistral with legal LoRA|chat",

    // === AFRICA (2 models) ===
        "africa/afriberta-base|afriberta|afriberta-model|AfriCLIP: AfriBERTa Base|C|0.000000/0.000000|512/256|S|-/-/-|AfriBERTa for African language NLP|chat",
        "africa/naija-bert|naija-bert|nigerian-bert|NaijaBERT: Nigerian|C|0.000000/0.000000|512/256|S|-/-/-|NaijaBERT for Nigerian Pidgin and English|chat",

    // === AI21 (2 models) ===
        "ai21/jamba-mini-1.7|jamba-mini-1.7|AI21: Jamba Mini 1.7|C|0.000000/0.000000|256K/4K|JT|-/-/-|Jamba Mini 1.7 is a compact and efficient member of the Jamba open model family,|chat",
        "ai21/jamba-large-1.7|jamba-large-1.7|AI21: Jamba Large 1.7|C|0.000002/0.000008|256K/4K|JT|-/-/-|Jamba Large 1.7 is the latest model in the Jamba open family, offering improveme|chat",

    // === AIML (2 models) ===
        "aiml/indic-llama|indic-llama|indic-13b|AIML: Indic Llama|C|0.000000/0.000000|4K/2K|T|-/-/-|Llama variant optimized for Indian languages|chat",
        "aiml/dhruva|dhruva|dhruva-base|AIML: Dhruva|C|0.000000/0.000000|4K/2K|T|-/-/-|Dhruva model for Indic languages support|chat",

    // === AION-LABS (3 models) ===
        "aion-labs/aion-1.0|aion-1.0|AionLabs: Aion-1.0|C|0.000004/0.000008|131K/32K|K|-/-/-|Aion-1.0 is a multi-model system designed for high performance across various ta|chat",
        "aion-labs/aion-1.0-mini|aion-1.0-mini|AionLabs: Aion-1.0-Mini|C|0.000001/0.000001|131K/32K|K|-/-/-|Aion-1.0-Mini 32B parameter model is a distilled version of the DeepSeek-R1 mode|chat",
        "aion-labs/aion-rp-llama-3.1-8b|aion-rp-llama-3.1-8b|AionLabs: Aion-RP 1.0 (8B)|C|0.000001/0.000002|32K/32K|-|-/-/-|Aion-RP-Llama-3.1-8B ranks the highest in the character evaluation portion of th|chat",

    // === ALFREDPROS (1 models) ===
        "alfredpros/codellama-7b-instruct-solidity|codellama-7b-instruc|AlfredPros: CodeLLaMa 7B Instruct Solidity|C|0.000001/0.000001|4K/4K|-|-/-/-|A finetuned 7 billion parameters Code LLaMA - Instruct model to generate Solidit|chat",

    // === ALIBABA (5 models) ===
        "alibaba/tongyi-deepresearch-30b-a3b:free|tongyi-deepresearch-|Tongyi DeepResearch 30B A3B (free)|C|-/-|131K/131K|JKST|-/-/-|Tongyi DeepResearch is an agentic large language model developed by Tongyi Lab, |chat",
        "alibaba/tongyi-deepresearch-30b-a3b|tongyi-deepresearch-|Tongyi DeepResearch 30B A3B|C|0.000000/0.000000|131K/131K|JKST|-/-/-|Tongyi DeepResearch is an agentic large language model developed by Tongyi Lab, |chat",
        "alibaba/qwen-max|qwen-max|qwen-pro|Alibaba: Qwen Max|C|0.000001/0.000006|32K/8K|VSTJ|-/-/-|Alibaba flagship Qwen Max model with extended reasoning|chat",
        "alibaba/qwen-plus|qwen-plus|qwen-standard|Alibaba: Qwen Plus|C|0.000000/0.000000|32K/4K|VSTJ|-/-/-|Balanced Qwen Plus model for production use|chat",
        "alibaba/qwen-turbo|qwen-turbo|qwen-fast|Alibaba: Qwen Turbo|C|0.000000/0.000000|8K/2K|VT|-/-/-|Fast and efficient Qwen Turbo variant|chat",

    // === ALIGNED (2 models) ===
        "aligned/llama-3-70b-preference-aligned|llama-3-aligned|llama-aligned|Meta: Llama 3 70B Preference Aligned|C|0.000002/0.000005|8K/2K|VSTJK|-/-/-|Llama 3 70B aligned to human preferences with RLHF|chat",
        "aligned/mistral-large-preference-aligned|mistral-aligned|mistral-rlhf|Mistral: Large Preference Aligned|C|1.50/4.50|128K/8K|VSTJ|-/-/-|Mistral Large fine-tuned with RLHF preference alignment|chat",

    // === ALLENAI (5 models) ===
        "allenai/olmo-3.1-32b-think:free|olmo-3.1-32b-think:f|AllenAI: Olmo 3.1 32B Think (free)|C|-/-|65K/65K|JKS|-/-/-|Olmo 3.1 32B Think is a large-scale, 32-billion-parameter model designed for dee|chat",
        "allenai/olmo-3-32b-think:free|olmo-3-32b-think:fre|AllenAI: Olmo 3 32B Think (free)|C|-/-|65K/65K|JKS|-/-/-|Olmo 3 32B Think is a large-scale, 32-billion-parameter model purpose-built for |chat",
        "allenai/olmo-3-7b-instruct|olmo-3-7b-instruct|AllenAI: Olmo 3 7B Instruct|C|0.000000/0.000000|65K/65K|JST|-/-/-|Olmo 3 7B Instruct is a supervised instruction-fine-tuned variant of the Olmo 3 |chat",
        "allenai/olmo-3-7b-think|olmo-3-7b-think|AllenAI: Olmo 3 7B Think|C|0.000000/0.000000|65K/65K|JKS|-/-/-|Olmo 3 7B Think is a research-oriented language model in the Olmo family designe|chat",
        "allenai/olmo-2-0325-32b-instruct|olmo-2-0325-32b-inst|AllenAI: Olmo 2 32B Instruct|C|0.000000/0.000000|128K/32K|-|-/-/-|OLMo-2 32B Instruct is a supervised instruction-finetuned variant of the OLMo-2 |chat",

    // === ALPINDALE (1 models) ===
        "alpindale/goliath-120b|goliath-120b|Goliath 120B|C|0.000006/0.000008|6K/1K|J|-/-/-|A large LLM created by combining two fine-tuned Llama 70B models into one 120B m|chat",

    // === AMAZON (6 models) ===
        "amazon/nova-2-lite-v1|nova-2-lite-v1|Amazon: Nova 2 Lite|C|0.000000/0.000003|1000K/65K|KTV|-/-/-|Nova 2 Lite is a fast, cost-effective reasoning model for everyday workloads tha|chat",
        "amazon/nova-premier-v1|nova-premier-v1|Amazon: Nova Premier 1.0|C|0.000003/0.000013|1000K/32K|TV|-/-/-|Amazon Nova Premier is the most capable of Amazon's multimodal models for comple|chat",
        "amazon/nova-lite-v1|nova-lite-v1|Amazon: Nova Lite 1.0|C|0.000000/0.000000|300K/5K|TV|-/-/-|Amazon Nova Lite 1.0 is a very low-cost multimodal model from Amazon that focuse|chat",
        "amazon/nova-micro-v1|nova-micro-v1|Amazon: Nova Micro 1.0|C|0.000000/0.000000|128K/5K|T|-/-/-|Amazon Nova Micro 1.0 is a text-only model that delivers the lowest latency resp|chat",
        "amazon/nova-pro-v1|nova-pro-v1|Amazon: Nova Pro 1.0|C|0.000001/0.000003|300K/5K|TV|-/-/-|Amazon Nova Pro 1.0 is a capable multimodal model from Amazon focused on providi|chat",
        "amazon/nova-premier-latest|amazon-nova-premier|Amazon Nova Premier|C|0.000001/0.000003|300K/40K|SVT|-/-/-|Premium multimodal reasoning model|chat",

    // === ANALYTICS (3 models) ===
        "analytics/datavizgpt|datavizgpt|analytics-viz|DataVizGPT: Visualization|C|0.000001/0.000003|8K/2K|VSTJ|-/-/-|Data visualization and storytelling|chat",
        "analytics/analyticsgpt|analyticsgpt|analytics-data|AnalyticsGPT: Analytics|C|0.000001/0.000004|8K/2K|VSTJ|-/-/-|Advanced analytics and insight generation|chat",
        "analytics/predictgpt|predictgpt|analytics-predict|PredictGPT: Forecasting|C|0.000001/0.000003|8K/2K|VSTJ|-/-/-|Time series prediction and trend analysis|chat",

    // === ANOMALY (2 models) ===
        "anomaly/isolation-forest-lstm|isolation-lstm|anomaly-detect|IsolationForest LSTM|C|0.000000/0.000000|512/256|S|-/-/-|LSTM-based anomaly detection|chat",
        "anomaly/autoencoder-vae|vae-anomaly|anomaly-vae|Variational Autoencoder|C|0.000000/0.000000|512/256|S|-/-/-|VAE for unsupervised anomaly detection|chat",

    // === ANTHRACITE-ORG (1 models) ===
        "anthracite-org/magnum-v4-72b|magnum-v4-72b|Magnum v4 72B|C|0.000003/0.000005|16K/2K|J|-/-/-|This is a series of models designed to replicate the prose quality of the Claude|chat",

    // === ANTHROPIC (35 models) ===
        "anthropic/claude-opus-4.5|claude-opus-4.5|Anthropic: Claude Opus 4.5|C|0.000005/0.000025|200K/32K|JKSTV|-/-/-|Claude Opus 4.5 is Anthropic's frontier reasoning model optimized for complex so|chat",
        "anthropic/claude-haiku-4.5|claude-haiku-4.5|Anthropic: Claude Haiku 4.5|C|0.000001/0.000005|200K/64K|KTV|-/-/-|Claude Haiku 4.5 is Anthropic's fastest and most efficient model, delivering nea|chat",
        "anthropic/claude-sonnet-4.5|claude-sonnet-4.5|Anthropic: Claude Sonnet 4.5|C|0.000003/0.000015|1000K/64K|JKSTV|-/-/-|Claude Sonnet 4.5 is Anthropic's most advanced Sonnet model to date, optimized f|chat",
        "anthropic/claude-opus-4.1|claude-opus-4.1|Anthropic: Claude Opus 4.1|C|0.000015/0.000075|200K/50K|JKSTV|-/-/-|Claude Opus 4.1 is an updated version of Anthropic's flagship model, offering im|chat",
        "anthropic/claude-opus-4|claude-opus-4|Anthropic: Claude Opus 4|C|0.000015/0.000075|200K/32K|KTV|-/-/-|Claude Opus 4 is benchmarked as the world's best coding model, at time of releas|chat",
        "anthropic/claude-sonnet-4|claude-sonnet-4|Anthropic: Claude Sonnet 4|C|0.000003/0.000015|1000K/64K|KTV|-/-/-|Claude Sonnet 4 significantly enhances the capabilities of its predecessor, Sonn|chat",
        "anthropic/claude-3.7-sonnet:thinking|claude-3.7-sonnet:th|Anthropic: Claude 3.7 Sonnet (thinking)|C|0.000003/0.000015|200K/64K|KTV|-/-/-|Claude 3.7 Sonnet is an advanced large language model with improved reasoning, c|chat",
        "anthropic/claude-3.7-sonnet|claude-3.7-sonnet|Anthropic: Claude 3.7 Sonnet|C|0.000003/0.000015|200K/64K|KTV|-/-/-|Claude 3.7 Sonnet is an advanced large language model with improved reasoning, c|chat",
        "anthropic/claude-3.5-haiku-20241022|claude-3.5-haiku-202|Anthropic: Claude 3.5 Haiku (2024-10-22)|C|0.000001/0.000004|200K/8K|TV|-/-/-|Claude 3.5 Haiku features enhancements across all skill sets including coding, t|chat",
        "anthropic/claude-3.5-haiku|claude-3.5-haiku|Anthropic: Claude 3.5 Haiku|C|0.000001/0.000004|200K/8K|TV|-/-/-|Claude 3.5 Haiku features offers enhanced capabilities in speed, coding accuracy|chat",
        "anthropic/claude-3.5-sonnet|claude-3.5-sonnet|Anthropic: Claude 3.5 Sonnet|C|0.000006/0.000030|200K/8K|TV|-/-/-|New Claude 3.5 Sonnet delivers better-than-Opus capabilities, faster-than-Sonnet|chat",
        "anthropic/claude-3-haiku|claude-3-haiku|Anthropic: Claude 3 Haiku|C|0.000000/0.000001|200K/4K|TV|-/-/-|Claude 3 Haiku is Anthropic's fastest and most compact model for
near-instant re|chat",
        "anthropic/claude-3-opus|claude-3-opus|Anthropic: Claude 3 Opus|C|0.000015/0.000075|200K/4K|TV|-/-/-|Claude 3 Opus is Anthropic's most powerful model for highly complex tasks. It bo|chat",
        "anthropic/claude-sonnet-4-5-20250929|claude-4.5-sonnet|Claude 4.5 Sonnet|C|3.00/15.00/0.3000|200K/16K|VTJSKC|-/-/-|Best for complex coding and analysis, supports 1M context with beta header|chat",
        "anthropic/claude-opus-4-1-20250805|claude-4.1-opus|Claude 4.1 Opus|C|15.00/75.00/1.50|200K/32K|VTJSKC|-/-/-|Most powerful Claude for agentic tasks, coding, and reasoning|chat",
        "anthropic/claude-opus-4-20250514|claude-4-opus|Claude 4 Opus|C|15.00/75.00/1.50|200K/32K|VTJSKC|-/-/-|Claude 4 flagship for complex tasks, research, and analysis|chat",
        "anthropic/claude-sonnet-4-20250514|claude-4-sonnet|Claude 4 Sonnet|C|3.00/15.00/0.3000|200K/64K|VTJSKC|-/-/-|Balanced intelligence and speed for everyday tasks|chat",
        "anthropic/claude-haiku-4-5-20251015|claude-4.5-haiku|Claude 4.5 Haiku|C|1.00/5.00/0.1000|200K/8K|VTJSC|-/-/-|Fast model optimized for low latency and cost|chat",
        "anthropic/claude-3-7-sonnet-20250219|claude-3.7-sonnet|Claude 3.7 Sonnet|C|3.00/15.00/0.3000|200K/128K|VTJSKC|-/-/-|Extended thinking with 128K output for complex reasoning|chat",
        "anthropic/claude-3-5-sonnet-20241022|claude-3.5-sonnet|Claude 3.5 Sonnet|C|3.00/15.00/0.3000|200K/8K|VTJSC|-/-/-|Best for coding, analysis, and complex reasoning tasks|chat",
        "anthropic/claude-3-5-haiku-20241022|claude-3.5-haiku|Claude 3.5 Haiku|C|0.8000/4.00/0.0800|200K/8K|VTJSC|-/-/-|Fast and affordable for high-volume tasks|chat",
        "anthropic/claude-3-opus-20240229|claude-3-opus|Claude 3 Opus|C|15.00/75.00/1.50|200K/4K|VTJC|-/-/-|Powerful model for complex tasks requiring deep understanding|chat",
        "anthropic/claude-3-sonnet-20240229|claude-3-sonnet|Claude 3 Sonnet|C|3.00/15.00/0.3000|200K/4K|VTJ|-/-/-|Balanced performance for wide range of tasks|chat",
        "anthropic/claude-3-haiku-20240307|claude-3-haiku|Claude 3 Haiku|C|0.2500/1.25/0.0300|200K/4K|VTJ|-/-/-|Fastest and most compact Claude 3 model|chat",
        "anthropic/claude-opus-4-5-20251101|claude-opus-4-5|Claude Opus 4.5|C|0.000015/0.000075|200K/4K|SVTJK|-/-/-|Frontier model with extended thinking capability|chat",
        "anthropic/claude-sonnet-4-5-20250924|claude-sonnet-4-5|Claude Sonnet 4.5|C|0.000003/0.000015|200K/4K|SVTJK|-/-/-|Advanced reasoning with improved speed|chat",
        "anthropic/claude-opus-4-finetuned-medical|claude-opus-medical|claude-medical|Anthropic: Claude Opus Medical FT|C|6.00/30.00|200K/32K|VSTJKC|-/-/-|Claude Opus fine-tuned for medical document analysis|chat",
        "anthropic/claude-opus-4-finetuned-legal|claude-opus-legal|claude-legal|Anthropic: Claude Opus Legal FT|C|6.00/30.00|200K/32K|VSTJKC|-/-/-|Claude Opus fine-tuned for legal contract review|chat",
        "anthropic/claude-opus-4-finetuned-financial|claude-opus-finance|claude-finance|Anthropic: Claude Opus Finance FT|C|6.00/30.00|200K/32K|VSTJKC|-/-/-|Claude Opus fine-tuned for financial analysis|chat",
        "anthropic/claude-opus-4-finetuned-code|claude-opus-code|claude-code|Anthropic: Claude Opus Code FT|C|6.00/30.00|200K/32K|VSTJKC|-/-/-|Claude Opus fine-tuned for code generation and analysis|chat",
        "anthropic/claude-sonnet-4-finetuned-chat|claude-sonnet-chat|claude-chat-ft|Anthropic: Claude Sonnet Chat FT|C|3.00/15.00|200K/4K|VSTJKC|-/-/-|Claude Sonnet fine-tuned for conversational AI|chat",
        "anthropic/claude-3-opus-vision|claude-3-opus-vision|Anthropic: Claude 3 Opus Vision|L|0.0150/0.0750|200K/4K|VSTJKC|-/-/-|Original Claude 3 Opus with vision|chat",
        "anthropic/claude-3-sonnet-vision|claude-3-sonnet-vision|Anthropic: Claude 3 Sonnet Vision|L|0.0030/0.0150|200K/4K|VSTJKC|-/-/-|Claude 3 Sonnet with vision capabilities|chat",
        "anthropic/claude3opus|anthropic-claude3-opus|Anthropic: Claude 3 Opus|L|0.0150/0.0750|200K/4K|VSTJKC|-/-/-|Original Claude 3 Opus release|chat",
        "anthropic/claude-3-haiku-vision|claude-3-haiku-vision|Anthropic: Claude 3 Haiku Vision|L|0.000250/0.0013|200K/1K|VSTJKC|-/-/-|Lightweight Claude 3 with vision support|chat",

    // === ANYSCALE (1 models) ===
        "anyscale/meta-llama/Llama-2-13b|llama-2-13b-any|Meta: Llama 2 13B (Anyscale)|C|0.000100/0.000200|4K/2K|VSTJ|-/-/-|Llama 2 13B via Anyscale|chat",

    // === ARCEE-AI (6 models) ===
        "arcee-ai/trinity-mini:free|trinity-mini:free|Arcee AI: Trinity Mini (free)|C|-/-|131K/32K|JKST|-/-/-|Trinity Mini is a 26B-parameter (3B active) sparse mixture-of-experts language m|chat",
        "arcee-ai/trinity-mini|trinity-mini|Arcee AI: Trinity Mini|C|0.000000/0.000000|131K/131K|JKST|-/-/-|Trinity Mini is a 26B-parameter (3B active) sparse mixture-of-experts language m|chat",
        "arcee-ai/spotlight|spotlight|Arcee AI: Spotlight|C|0.000000/0.000000|131K/65K|V|-/-/-|Spotlight is a 7-billion-parameter vision-language model derived from Qwen'2.5-V|chat",
        "arcee-ai/maestro-reasoning|maestro-reasoning|Arcee AI: Maestro Reasoning|C|0.000001/0.000003|131K/32K|-|-/-/-|Maestro Reasoning is Arcee's flagship analysis model: a 32'B-parameter derivativ|chat",
        "arcee-ai/virtuoso-large|virtuoso-large|Arcee AI: Virtuoso Large|C|0.000001/0.000001|131K/64K|T|-/-/-|Virtuoso-Large is Arcee's top-tier general-purpose LLM at 72'B parameters, tuned|chat",
        "arcee-ai/coder-large|coder-large|Arcee AI: Coder Large|C|0.000000/0.000001|32K/8K|-|-/-/-|Coder-Large is a 32'B-parameter offspring of Qwen'2.5-Instruct that has been fur|chat",

    // === ARLIAI (1 models) ===
        "arliai/qwq-32b-arliai-rpr-v1|qwq-32b-arliai-rpr-v|ArliAI: QwQ 32B RpR v1|C|0.000000/0.000000|32K/32K|JKS|-/-/-|QwQ-32B-ArliAI-RpR-v1 is a 32B parameter model fine-tuned from Qwen/QwQ-32B usin|chat",

    // === ASIA (12 models) ===
        "asia/baichuan-13b|baichuan-13b-a|Baichuan: 13B|C|0.000400/0.000400|4K/2K|VSTJ|-/-/-|Baichuan Chinese optimized 13B|chat",
        "asia/baichuan-7b|baichuan-7b-a|Baichuan: 7B|C|0.000200/0.000200|4K/2K|VSTJ|-/-/-|Baichuan 7B model|chat",
        "asia/xverse-65b|xverse-65b-a|XVERSE: 65B|C|0.000900/0.000900|4K/2K|VSTJ|-/-/-|XVERSE 65B multilingual|chat",
        "asia/yi-1.5-34b|yi-34b-a|01.AI: Yi 1.5 34B|C|0.000600/0.000600|200K/2K|VSTJ|-/-/-|Yi 1.5 34B extended context|chat",
        "asia/yi-1.5-9b|yi-9b-a|01.AI: Yi 1.5 9B|C|0.000150/0.000150|200K/2K|VSTJ|-/-/-|Yi 1.5 9B lightweight|chat",
        "asia/minicpm-v|minicpm-v-a|MiniCPM-V|C|0.000080/0.000080|1K/1K|VT|-/-/-|MiniCPM vision model|chat",
        "asia/minicpm-2b|minicpm-2b-a|MiniCPM: 2B|C|0.000030/0.000030|4K/1K|VST|-/-/-|MiniCPM ultra-small|chat",
        "asia/chatglm3|chatglm3-a|Zhipu: ChatGLM3|C|0.000500/0.000500|8K/2K|VSTJ|-/-/-|ChatGLM3 Chinese advanced|chat",
        "asia/chatglm4|chatglm4-a|Zhipu: ChatGLM4|C|0.0010/0.0010|128K/2K|VSTJK|-/-/-|ChatGLM4 extended context|chat",
        "asia/moonshot-8k|moonshot-8k-a|Moonshot: 8K|C|0.0010/0.0010|8K/2K|VSTJ|-/-/-|Moonshot 8K context model|chat",
        "asia/moonshot-32k|moonshot-32k-a|Moonshot: 32K|C|0.0030/0.0030|32K/2K|VSTJ|-/-/-|Moonshot 32K extended|chat",
        "asia/moonshot-128k|moonshot-128k-a|Moonshot: 128K|C|0.0120/0.0120|128K/2K|VSTJ|-/-/-|Moonshot 128K long context|chat",

    // === AUDIO (37 models) ===
        "audio/wav2vec2-large|wav2vec2-large|Facebook: Wav2Vec2 Large|C|0.000020/0.000050|16K/1|VT|-/-/-|State-of-the-art speech recognition|chat",
        "audio/clap-base|clap-base|Salesforce: CLAP Base|C|0.000010/0.000030|16K/1|VT|-/-/-|CLIP for audio understanding|chat",
        "audio/wav2vec2-base-finetuned-multilingual-asr|wav2vec-multi|wav2vec-asr-ft|Hugging Face: Wav2Vec2 Multilingual ASR FT|C|0.000000/0.000000|448/2K|V|-/-/-|Wav2Vec2 fine-tuned for multilingual speech recognition|chat",
        "audio/whisper-base-finetuned-medical-terminology|whisper-medical|whisper-med-ft|OpenAI: Whisper Medical FT|C|0.000000/0.000000|448/2K|V|-/-/-|Whisper base fine-tuned for medical terminology|chat",
        "audio/wav2vec2-xlarge|wav2vec2-xlarge-a|Facebook: Wav2Vec2 XL|C|0.000050/0.000150|16K/512|VT|-/-/-|Self-supervised speech XL|chat",
        "audio/conformer-large|conformer-large-a|Conformer Large ASR|C|0.000080/0.000200|16K/512|VT|-/-/-|Conformer for speech recognition|chat",
        "audio/whisper-large|whisper-large-a|OpenAI: Whisper Large|C|0.000100/0.000300|16K/512|VT|-/-/-|Robust multilingual ASR|chat",
        "audio/hubert-xlarge|hubert-xlarge-a|HuBERT XLarge ASR|C|0.000080/0.000200|16K/512|VT|-/-/-|Self-supervised HuBERT XL|chat",
        "audio/glow-tts|glow-tts-a|Glow-TTS|C|0.000030/0.000100|512/16K|VT|-/-/-|Efficient TTS flow-based model|chat",
        "audio/fastpitch|fastpitch-a|FastPitch TTS|C|0.000050/0.000150|512/16K|VT|-/-/-|Fast pitch-based TTS|chat",
        "audio/vits|vits-a|VITS Neural Vocoder|C|0.000080/0.000200|512/16K|VT|-/-/-|Variational inference TTS|chat",
        "audio/naturalspeech3|naturalspeech3-a|Microsoft: NaturalSpeech3|C|0.000100/0.000300|512/16K|VT|-/-/-|Neural audio codec TTS|chat",
        "audio/musicgen|musicgen-a|Meta: MusicGen|C|0.000150/0.000500|1K/16K|VT|-/-/-|Controllable music generation|chat",
        "audio/jukebox|jukebox-a|OpenAI: Jukebox|C|0.000200/0.000800|8K/16K|VT|-/-/-|Music generation with lyrics|chat",
        "audio/musicbert|musicbert-a|MusicBERT|C|0.000120/0.000300|512/512|VT|-/-/-|Music understanding BERT|chat",
        "audio/riffusion|riffusion-a|Riffusion Spectro|C|0.000100/0.000300|512/512|VT|-/-/-|Spectrogram diffusion for music|chat",
        "audio/demucs|demucs-a|Meta: Demucs|C|0.000180/0.000500|16K/16K|VT|-/-/-|Source separation SOTA|chat",
        "audio/spleeter|spleeter-a|Deezer: Spleeter|C|0.000100/0.000300|16K/16K|VT|-/-/-|Fast source separation|chat",
        "audio/hybrid-spectrogram|hybrid-spec-a|Hybrid Spectrogram|C|0.000150/0.000400|16K/16K|VT|-/-/-|Hybrid time-frequency separation|chat",
        "audio/audioset-classifier|audioset-cls-a|AudioSet Classifier|C|0.000080/0.000200|16K/512|VT|-/-/-|Environmental sound classification|chat",
        "audio/urbansound8k|urbansound8k-a|UrbanSound8K Model|C|0.000100/0.000250|16K/512|VT|-/-/-|Urban sound recognition|chat",
        "audio/fsd50k-tagger|fsd50k-tagger-a|FSD50K Tagger|C|0.000120/0.000300|16K/512|VT|-/-/-|Freesound Dataset tagging|chat",
        "audio/shazam-fingerprint|shazam-fp-a|Shazam Fingerprinting|C|0.000050/0.000150|16K/128|VT|-/-/-|Audio fingerprinting MFCC|chat",
        "audio/audfprint|audfprint-a|AudFPrint|C|0.000080/0.000200|16K/128|VT|-/-/-|Robust audio matching|chat",
        "audio/ecapa-tdnn|ecapa-tdnn-a|ECAPA-TDNN Speaker|C|0.000120/0.000300|16K/256|VT|-/-/-|Speaker verification SOTA|chat",
        "audio/xvector|xvector-a|X-Vector Speaker|C|0.000100/0.000250|16K/256|VT|-/-/-|DNN-based speaker embedding|chat",
        "audio/ser-wav2vec|ser-wav2vec-a|SER Wav2Vec2|C|0.000100/0.000300|16K/512|VT|-/-/-|Speech emotion recognition|chat",
        "audio/multi-modal-emotion|multimodal-emo-a|Multimodal Emotion|C|0.000150/0.000500|16K/512|VT|-/-/-|Audio-visual emotion detection|chat",
        "audio/whisper-tiny|whisper-tiny|whisper-small|OpenAI: Whisper Tiny|C|0/0|448/2K|V|-/-/-|Whisper Tiny for speech recognition on edge|chat",
        "audio/whisper-base|whisper-base|whisper-std|OpenAI: Whisper Base|C|0/0|448/2K|V|-/-/-|Whisper Base for basic transcription|chat",
        "audio/whisper-small|whisper-small|whisper-med|OpenAI: Whisper Small|C|0/0|448/2K|V|-/-/-|Whisper Small for improved accuracy|chat",
        "audio/whisper-medium|whisper-medium|whisper-acc|OpenAI: Whisper Medium|C|0/0|448/2K|V|-/-/-|Whisper Medium for high accuracy|chat",
        "audio/wav2vec2-base|wav2vec2|wav2vec|Facebook: Wav2Vec2 Base|C|0.000000/0.000000|448/2K|V|-/-/-|Wav2Vec2 Base for speech representation|chat",
        "audio/demucs-base|demucs|audio-separation|Meta: Demucs|C|0.000000/0.000000|448/2K|V|-/-/-|Demucs for music source separation|chat",
        "audio/voicefilter|voicefilter|audio-filter|Google: VoiceFilter|C|0.000000/0.000002|448/2K|V|-/-/-|VoiceFilter for speech enhancement|chat",
        "audio/jukebox-5b|jukebox-5b|music-gen|OpenAI: Jukebox 5B|C|0.000000/0.000002|2K/2K|V|-/-/-|Jukebox 5B for music generation|chat",
        "audio/music-vae|musicvae|music-vae|Google: MusicVAE|C|0.000000/0.000000|512/512|V|-/-/-|MusicVAE for music composition|chat",

    // === AUTOMOTIVE (2 models) ===
        "automotive/diagnosticsgpt|diagnosticsgpt|auto-diag|DiagnosticsGPT: Vehicles|C|0.000001/0.000003|8K/2K|VSTJ|-/-/-|Vehicle diagnostics and maintenance prediction|chat",
        "automotive/autonomousgpt|autonomousgpt|auto-av|AutonomousGPT: Driving|C|0.000002/0.000005|8K/2K|VSTJK|-/-/-|Autonomous vehicle decision making and planning|chat",

    // === AZURE (2 models) ===
        "azure/gpt-4-turbo-vision|azure-gpt-4-turbo-vision|Azure: GPT-4 Turbo Vision|C|0.000010/0.000030|128K/4K|VST|-/-/-|GPT-4 Turbo with vision via Azure OpenAI|chat",
        "azure/gpt-4o-vision|azure-gpt-4o-vision|Azure: GPT-4o Vision|C|0.0025/0.0100|128K/16K|VSTJS|-/-/-|GPT-4o deployed via Azure with vision|chat",

    // === BAICHUAN (7 models) ===
        "baichuan/Baichuan4-Turbo|baichuan4-turbo|Baichuan4 Turbo|C|0.1400/0.1400|131K/8K|VTJS|-/-/-|Flagship turbo model|chat",
        "baichuan/Baichuan4-Air|baichuan4-air|Baichuan4 Air|C|0.0140/0.0140|131K/8K|TJS|-/-/-|Lightweight efficient model|chat",
        "baichuan/Baichuan3-Turbo|baichuan3-turbo|Baichuan3 Turbo|C|0.0170/0.0170|131K/8K|TJS|-/-/-|Previous gen turbo model|chat",
        "baichuan/Baichuan3-Turbo-128k|baichuan3-128k|Baichuan3 Turbo 128K|C|0.0340/0.0340|131K/8K|TJS|-/-/-|128K context model|chat",
        "baichuan/Baichuan2-Turbo|baichuan2-turbo|Baichuan2 Turbo|C|0.0110/0.0110|32K/4K|TJ|-/-/-|Legacy turbo model|chat",
        "baichuan/Baichuan2-Turbo-192k|baichuan2-192k|Baichuan2 Turbo 192K|C|0.0220/0.0220|196K/4K|TJ|-/-/-|192K context legacy model|chat",
        "baichuan/Baichuan-Text-Embedding|baichuan-embed|Baichuan Text Embedding|C|0.000700/-|512/1K|E|-/-/-|Text embeddings|embed",

    // === BAIDU (10 models) ===
        "baidu/ernie-4.5-21b-a3b-thinking|ernie-4.5-21b-a3b-th|Baidu: ERNIE 4.5 21B A3B Thinking|C|0.000000/0.000000|131K/65K|K|-/-/-|ERNIE-4.5-21B-A3B-Thinking is Baidu's upgraded lightweight MoE model, refined to|chat",
        "baidu/ernie-4.5-21b-a3b|ernie-4.5-21b-a3b|Baidu: ERNIE 4.5 21B A3B|C|0.000000/0.000000|120K/8K|T|-/-/-|A sophisticated text-based Mixture-of-Experts (MoE) model featuring 21B total pa|chat",
        "baidu/ernie-4.5-vl-28b-a3b|ernie-4.5-vl-28b-a3b|Baidu: ERNIE 4.5 VL 28B A3B|C|0.000000/0.000001|30K/8K|KTV|-/-/-|A powerful multimodal Mixture-of-Experts chat model featuring 28B total paramete|chat",
        "baidu/ernie-4.5-vl-424b-a47b|ernie-4.5-vl-424b-a4|Baidu: ERNIE 4.5 VL 424B A47B|C|0.000000/0.000001|123K/16K|KV|-/-/-|ERNIE-4.5-VL-424B-A47B is a multimodal Mixture-of-Experts (MoE) model from Baidu|chat",
        "baidu/ernie-4.5-300b-a47b|ernie-4.5-300b-a47b|Baidu: ERNIE 4.5 300B A47B|C|0.000000/0.000001|123K/12K|JS|-/-/-|ERNIE-4.5-300B-A47B is a 300B parameter Mixture-of-Experts (MoE) language model |chat",
        "baidu/ernie-bot-4|ernie-4|ernie-bot-4|Baidu: ERNIE Bot 4|C|0.000000/0.000000|8K/2K|VSTJ|-/-/-|Baidu ERNIE Bot 4 with strong Chinese understanding|chat",
        "baidu/ernie-bot-3.5|ernie-3.5|ernie-turbo|Baidu: ERNIE Bot 3.5|C|0.000000/0.000000|8K/2K|VSTJ|-/-/-|Baidu ERNIE Bot 3.5 Turbo for faster inference|chat",
        "baidu/ernie-bot-8k|ernie-8k|ernie-extended|Baidu: ERNIE Bot 8K|C|0.000000/0.000000|8K/2K|VSTJ|-/-/-|Extended context ERNIE Bot for long documents|chat",
        "baidu/ernie-4v-8k|ernie-4v-8k|Baidu: ERNIE 4.0 Vision|C|0.000001/0.000008|8K/2K|VST|-/-/-|Baidu multimodal model for Chinese and English vision tasks|chat",
        "baidu/ernie-4v-32k|ernie-4v-32k|Baidu: ERNIE 4.0 Vision 32K|C|0.000002/0.000015|32K/4K|VST|-/-/-|Extended context Baidu vision model|chat",

    // === BEDROCK (106 models) ===
        "bedrock/ai21.jamba-1-5-large-v1:0|jamba-1-5-large|AI21 Jamba 1.5 Large|C|2.00/8.00|256K/4K|TJ|-/-/-|AI21 large hybrid SSM-Transformer model|chat",
        "bedrock/ai21.jamba-1-5-mini-v1:0|jamba-1-5-mini|AI21 Jamba 1.5 Mini|C|0.2000/0.4000|256K/4K|TJ|-/-/-|AI21 efficient hybrid model|chat",
        "bedrock/ai21.jamba-instruct-v1:0|jamba-instruct|AI21 Jamba Instruct|C|0.5000/0.7000|256K/4K|TJ|-/-/-|AI21 instruction-tuned Jamba|chat",
        "bedrock/ai21.j2-ultra-v1|jurassic-2-ultra|AI21 Jurassic-2 Ultra|L|18.80/18.80|8K/8K|-|-/-/-|Legacy AI21 flagship model|chat",
        "bedrock/ai21.j2-mid-v1|jurassic-2-mid|AI21 Jurassic-2 Mid|L|12.50/12.50|8K/8K|-|-/-/-|Legacy AI21 mid-tier model|chat",
        "bedrock/amazon.nova-pro-v1:0|nova-pro|Amazon Nova Pro|C|0.8000/3.20|300K/5K|SVTJ|-/-/-|Advanced multimodal understanding and generation|chat",
        "bedrock/amazon.nova-lite-v1:0|nova-lite|Amazon Nova Lite|C|0.0600/0.2400|300K/5K|SVTJ|-/-/-|Fast and cost-effective multimodal|chat",
        "bedrock/amazon.nova-micro-v1:0|nova-micro|Amazon Nova Micro|C|0.0350/0.1400|128K/5K|TJ|-/-/-|Text-only fastest and lowest cost|chat",
        "bedrock/amazon.nova-premier-v1:0|nova-premier|Amazon Nova Premier|C|2.50/10.00|1000K/5K|SVTJK|-/-/-|Most capable Nova for complex tasks|chat",
        "bedrock/amazon.nova-canvas-v1:0|nova-canvas|Amazon Nova Canvas|C|0.0400/-|-/-|I|-/-/-|State-of-art image generation|image",
        "bedrock/amazon.nova-reel-v1:0|nova-reel|Amazon Nova Reel|C|0.8000/-|-/-|D|-/-/-|Studio-quality video generation|video",
        "bedrock/amazon.nova-sonic-v1:0|nova-sonic|Amazon Nova Sonic|C|4.88/7.50|-/-|A|-/-/-|Streaming speech-to-speech|audio",
        "bedrock/amazon.nova-2-lite-v1:0|nova-2-lite|Amazon Nova 2 Lite|C|0.0800/0.3200|300K/5K|SVTJ|-/-/-|Next-gen efficient multimodal|chat",
        "bedrock/amazon.nova-2-pro-v1:0|nova-2-pro|Amazon Nova 2 Pro|C|1.00/4.00|300K/5K|SVTJK|-/-/-|Next-gen advanced multimodal|chat",
        "bedrock/amazon.titan-text-premier-v1:0|titan-text-premier|Amazon Titan Text Premier|C|0.5000/1.50|32K/3K|TJ|-/-/-|Titan flagship text model|chat",
        "bedrock/amazon.titan-text-express-v1|titan-text-express|Amazon Titan Text Express|C|0.2000/0.6000|8K/4K|-|-/-/-|Balanced Titan text model|chat",
        "bedrock/amazon.titan-text-lite-v1|titan-text-lite|Amazon Titan Text Lite|C|0.1500/0.2000|4K/4K|-|-/-/-|Lightweight Titan text|chat",
        "bedrock/amazon.titan-embed-text-v2:0|titan-embed-v2|Amazon Titan Text Embeddings V2|C|0.0200/-|8K/1K|E|-/-/-|Latest Titan embeddings|embed",
        "bedrock/amazon.titan-embed-text-v1|titan-embed-v1|Amazon Titan Text Embeddings V1|C|0.1000/-|8K/1K|E|-/-/-|Titan embeddings v1|embed",
        "bedrock/amazon.titan-embed-image-v1|titan-embed-image|Amazon Titan Multimodal Embeddings|C|0.8000/-|128/1K|VE|-/-/-|Image+text embeddings|embed",
        "bedrock/amazon.titan-image-generator-v2:0|titan-image-v2|Amazon Titan Image Generator V2|C|0.0080/-|-/-|I|-/-/-|Advanced image generation|image",
        "bedrock/amazon.titan-image-generator-v1|titan-image-v1|Amazon Titan Image Generator V1|C|0.0100/-|-/-|I|-/-/-|Image generation|image",
        "bedrock/amazon.rerank-v1:0|amazon-rerank|Amazon Rerank|C|1.00/-|32K/-|R|-/-/-|Semantic reranking model|rerank",
        "bedrock/anthropic.claude-sonnet-4-5-v1:0|claude-4-5-sonnet|Claude 4.5 Sonnet|C|3.00/15.00|200K/8K|SVTJKC|-/-/-|Best for complex coding and analysis|chat",
        "bedrock/anthropic.claude-opus-4-1-v1:0|claude-4-1-opus|Claude 4.1 Opus|C|15.00/75.00|200K/32K|SVTJKC|-/-/-|Most powerful Claude model|chat",
        "bedrock/anthropic.claude-opus-4-v1:0|claude-4-opus|Claude 4 Opus|C|15.00/75.00|200K/32K|SVTJKC|-/-/-|Claude 4 flagship|chat",
        "bedrock/anthropic.claude-sonnet-4-v1:0|claude-4-sonnet|Claude 4 Sonnet|C|3.00/15.00|200K/8K|SVTJKC|-/-/-|Claude 4 balanced model|chat",
        "bedrock/anthropic.claude-3-7-sonnet-v1:0|claude-3-7-sonnet|Claude 3.7 Sonnet|C|3.00/15.00|200K/8K|SVTJKC|-/-/-|Enhanced Claude 3.5 successor|chat",
        "bedrock/anthropic.claude-3-5-sonnet-v2:0|claude-3-5-sonnet-v2|Claude 3.5 Sonnet V2|C|3.00/15.00|200K/8K|SVTJKC|-/-/-|Latest 3.5 Sonnet with computer use|chat",
        "bedrock/anthropic.claude-3-5-sonnet-v1:0|claude-3-5-sonnet-v1|Claude 3.5 Sonnet V1|C|3.00/15.00|200K/8K|SVTJC|-/-/-|Original Claude 3.5 Sonnet|chat",
        "bedrock/anthropic.claude-3-5-haiku-v1:0|claude-3-5-haiku|Claude 3.5 Haiku|C|0.8000/4.00|200K/8K|SVTJC|-/-/-|Fast Claude 3.5 model|chat",
        "bedrock/anthropic.claude-3-opus-v1:0|claude-3-opus|Claude 3 Opus|C|15.00/75.00|200K/4K|SVTJC|-/-/-|Claude 3 most capable|chat",
        "bedrock/anthropic.claude-3-sonnet-v1:0|claude-3-sonnet|Claude 3 Sonnet|L|3.00/15.00|200K/4K|SVTJC|-/-/-|Claude 3 balanced|chat",
        "bedrock/anthropic.claude-3-haiku-v1:0|claude-3-haiku|Claude 3 Haiku|C|0.2500/1.25|200K/4K|SVTJC|-/-/-|Claude 3 fastest|chat",
        "bedrock/anthropic.claude-v2:1|claude-2-1|Claude 2.1|L|8.00/24.00|200K/4K|T|-/-/-|Legacy Claude 2.1|chat",
        "bedrock/anthropic.claude-v2|claude-2|Claude 2|L|8.00/24.00|100K/4K|-|-/-/-|Legacy Claude 2|chat",
        "bedrock/anthropic.claude-instant-v1|claude-instant|Claude Instant|L|0.8000/2.40|100K/4K|-|-/-/-|Legacy fast Claude|chat",
        "bedrock/cohere.command-r-plus-v1:0|command-r-plus|Cohere Command R+|C|2.50/10.00|128K/4K|TJS|-/-/-|Cohere flagship RAG model|chat",
        "bedrock/cohere.command-r-v1:0|command-r|Cohere Command R|C|0.5000/1.50|128K/4K|TJS|-/-/-|Cohere efficient RAG model|chat",
        "bedrock/cohere.command-light-text-v14|command-light|Cohere Command Light|L|0.3000/0.6000|4K/4K|-|-/-/-|Legacy Cohere light model|chat",
        "bedrock/cohere.command-text-v14|command-text|Cohere Command|L|1.50/2.00|4K/4K|-|-/-/-|Legacy Cohere command|chat",
        "bedrock/cohere.embed-english-v3|cohere-embed-en-v3|Cohere Embed English V3|C|0.1000/-|512/1K|E|-/-/-|English embeddings|embed",
        "bedrock/cohere.embed-multilingual-v3|cohere-embed-multi-v3|Cohere Embed Multilingual V3|C|0.1000/-|512/1K|E|-/-/-|Multilingual embeddings|embed",
        "bedrock/cohere.rerank-v3-5:0|cohere-rerank-v3-5|Cohere Rerank 3.5|C|2.00/-|4K/-|R|-/-/-|Latest Cohere reranker|rerank",
        "bedrock/cohere.rerank-multilingual-v3:0|cohere-rerank-multi|Cohere Rerank Multilingual|C|2.00/-|4K/-|R|-/-/-|Multilingual reranking|rerank",
        "bedrock/deepseek.deepseek-r1-v1:0|deepseek-r1|DeepSeek R1|C|1.40/5.60|128K/16K|TJK|-/-/-|DeepSeek reasoning model|chat",
        "bedrock/deepseek.deepseek-v3-1-v1:0|deepseek-v3-1|DeepSeek V3.1|C|0.2700/1.10|128K/16K|TJS|-/-/-|DeepSeek efficient MoE|chat",
        "bedrock/google.gemma-3-27b-it-v1:0|gemma-3-27b|Google Gemma 3 27B IT|C|0.3000/0.3500|128K/8K|VT|-/-/-|Gemma 3 large model|chat",
        "bedrock/google.gemma-3-12b-it-v1:0|gemma-3-12b|Google Gemma 3 12B IT|C|0.1000/0.1500|128K/8K|VT|-/-/-|Gemma 3 medium model|chat",
        "bedrock/google.gemma-3-4b-it-v1:0|gemma-3-4b|Google Gemma 3 4B IT|C|0.0600/0.0800|128K/8K|-|-/-/-|Gemma 3 small model|chat",
        "bedrock/google.gemma-2-27b-it-v1:0|gemma-2-27b|Google Gemma 2 27B IT|C|0.3000/0.3500|8K/8K|-|-/-/-|Gemma 2 large model|chat",
        "bedrock/google.gemma-7b-it-v1:0|gemma-7b|Google Gemma 7B IT|L|0.0700/0.1400|8K/8K|-|-/-/-|Legacy Gemma instruction|chat",
        "bedrock/luma.ray-v2:0|luma-ray-v2|Luma Ray V2|C|0.6500/-|-/-|D|-/-/-|Fast video generation|video",
        "bedrock/meta.llama4-maverick-17b-instruct-v1:0|llama-4-maverick|Llama 4 Maverick 17B|C|0.2200/0.8800|128K/8K|SVTJ|-/-/-|Llama 4 multimodal specialist|chat",
        "bedrock/meta.llama4-scout-17b-instruct-v1:0|llama-4-scout|Llama 4 Scout 17B|C|0.1700/0.6800|1000K/8K|SVTJ|-/-/-|Llama 4 long context|chat",
        "bedrock/meta.llama3-3-70b-instruct-v1:0|llama-3-3-70b|Llama 3.3 70B Instruct|C|0.7200/0.7200|128K/8K|TJS|-/-/-|Llama 3.3 flagship|chat",
        "bedrock/meta.llama3-2-90b-instruct-v1:0|llama-3-2-90b-vision|Llama 3.2 90B Vision|C|2.00/2.00|128K/4K|SVTJ|-/-/-|Llama 3.2 large vision|chat",
        "bedrock/meta.llama3-2-11b-instruct-v1:0|llama-3-2-11b-vision|Llama 3.2 11B Vision|C|0.1600/0.1600|128K/4K|SVTJ|-/-/-|Llama 3.2 small vision|chat",
        "bedrock/meta.llama3-2-3b-instruct-v1:0|llama-3-2-3b|Llama 3.2 3B Instruct|C|0.1500/0.1500|128K/4K|TJ|-/-/-|Llama 3.2 tiny|chat",
        "bedrock/meta.llama3-2-1b-instruct-v1:0|llama-3-2-1b|Llama 3.2 1B Instruct|C|0.1000/0.1000|128K/4K|-|-/-/-|Llama 3.2 smallest|chat",
        "bedrock/meta.llama3-1-405b-instruct-v1:0|llama-3-1-405b|Llama 3.1 405B Instruct|C|5.32/16.00|128K/8K|TJS|-/-/-|Llama 3.1 largest|chat",
        "bedrock/meta.llama3-1-70b-instruct-v1:0|llama-3-1-70b|Llama 3.1 70B Instruct|C|0.7200/0.7200|128K/8K|TJS|-/-/-|Llama 3.1 large|chat",
        "bedrock/meta.llama3-1-8b-instruct-v1:0|llama-3-1-8b|Llama 3.1 8B Instruct|C|0.2200/0.2200|128K/8K|TJ|-/-/-|Llama 3.1 small|chat",
        "bedrock/meta.llama3-70b-instruct-v1:0|llama-3-70b|Llama 3 70B Instruct|L|2.65/3.50|8K/2K|TJ|-/-/-|Legacy Llama 3 large|chat",
        "bedrock/meta.llama3-8b-instruct-v1:0|llama-3-8b|Llama 3 8B Instruct|L|0.3000/0.6000|8K/2K|-|-/-/-|Legacy Llama 3 small|chat",
        "bedrock/meta.llama2-70b-chat-v1|llama-2-70b|Llama 2 70B Chat|L|1.95/2.56|4K/2K|-|-/-/-|Legacy Llama 2|chat",
        "bedrock/meta.llama2-13b-chat-v1|llama-2-13b|Llama 2 13B Chat|L|0.7500/1.00|4K/2K|-|-/-/-|Legacy Llama 2 small|chat",
        "bedrock/minimax.minimax-m2-v1:0|minimax-m2|MiniMax M2|C|0.8000/3.20|128K/8K|TJ|-/-/-|MiniMax flagship model|chat",
        "bedrock/mistral.mistral-large-2411-v1:0|mistral-large-2411|Mistral Large 24.11|C|2.00/6.00|128K/8K|VTJS|-/-/-|Latest Mistral Large|chat",
        "bedrock/mistral.mistral-large-2407-v1:0|mistral-large-2407|Mistral Large 24.07|C|2.00/6.00|128K/8K|TJS|-/-/-|Mistral Large previous|chat",
        "bedrock/mistral.pixtral-large-2502-v1:0|pixtral-large|Pixtral Large 25.02|C|2.00/6.00|128K/8K|SVTJS|-/-/-|Mistral multimodal flagship|chat",
        "bedrock/mistral.pixtral-12b-2409-v1:0|pixtral-12b|Pixtral 12B|C|0.1500/0.1500|128K/8K|SVT|-/-/-|Mistral small multimodal|chat",
        "bedrock/mistral.ministral-8b-2410-v1:0|ministral-8b|Ministral 8B|C|0.1000/0.1000|128K/8K|TJ|-/-/-|Mistral efficient edge model|chat",
        "bedrock/mistral.ministral-3b-2410-v1:0|ministral-3b|Ministral 3B|C|0.0400/0.0400|128K/8K|-|-/-/-|Mistral tiny edge model|chat",
        "bedrock/mistral.mistral-small-2409-v1:0|mistral-small-2409|Mistral Small 24.09|C|0.1000/0.3000|32K/8K|TJ|-/-/-|Cost-effective Mistral|chat",
        "bedrock/mistral.codestral-2501-v1:0|codestral-2501|Codestral 25.01|C|0.3000/0.9000|256K/16K|TJ|-/-/-|Code-specialized Mistral|chat",
        "bedrock/mistral.voxtral-mini-3b-v1:0|voxtral-mini|Voxtral Mini 3B|C|0.0500/0.0500|32K/8K|A|-/-/-|Mistral speech model|audio",
        "bedrock/mistral.mistral-7b-instruct-v0:2|mistral-7b|Mistral 7B Instruct|L|0.1500/0.2000|32K/8K|-|-/-/-|Legacy Mistral 7B|chat",
        "bedrock/mistral.mixtral-8x7b-instruct-v0:1|mixtral-8x7b|Mixtral 8x7B Instruct|L|0.4500/0.7000|32K/8K|-|-/-/-|Legacy Mixtral MoE|chat",
        "bedrock/moonshot.kimi-k2-thinking-v1:0|kimi-k2-thinking|Moonshot Kimi K2 Thinking|C|0.6000/2.40|131K/16K|TJK|-/-/-|Kimi reasoning model|chat",
        "bedrock/nvidia.nemotron-nano-8b-v1:0|nemotron-nano|NVIDIA Nemotron Nano 8B|C|0.1000/0.1000|128K/4K|TJ|-/-/-|NVIDIA efficient model|chat",
        "bedrock/qwen.qwen3-235b-instruct-v1:0|qwen3-235b|Qwen3 235B Instruct|C|1.50/4.00|131K/8K|TJK|-/-/-|Qwen3 largest model|chat",
        "bedrock/qwen.qwen3-72b-instruct-v1:0|qwen3-72b|Qwen3 72B Instruct|C|0.4000/0.8000|131K/8K|TJK|-/-/-|Qwen3 large model|chat",
        "bedrock/qwen.qwen3-32b-instruct-v1:0|qwen3-32b|Qwen3 32B Instruct|C|0.2000/0.4000|131K/8K|TJK|-/-/-|Qwen3 medium model|chat",
        "bedrock/qwen.qwen3-14b-instruct-v1:0|qwen3-14b|Qwen3 14B Instruct|C|0.1000/0.2000|131K/8K|TJ|-/-/-|Qwen3 small model|chat",
        "bedrock/qwen.qwen3-8b-instruct-v1:0|qwen3-8b|Qwen3 8B Instruct|C|0.0600/0.1200|131K/8K|TJ|-/-/-|Qwen3 efficient model|chat",
        "bedrock/qwen.qwen3-4b-instruct-v1:0|qwen3-4b|Qwen3 4B Instruct|C|0.0400/0.0800|131K/8K|-|-/-/-|Qwen3 tiny model|chat",
        "bedrock/stability.sd3-5-large-v1:0|sd3-5-large|Stable Diffusion 3.5 Large|C|0.0650/-|-/-|I|-/-/-|SD3.5 large image generation|image",
        "bedrock/stability.sd3-5-large-turbo-v1:0|sd3-5-turbo|Stable Diffusion 3.5 Turbo|C|0.0400/-|-/-|I|-/-/-|SD3.5 fast generation|image",
        "bedrock/stability.sd3-large-v1:0|sd3-large|Stable Diffusion 3 Large|C|0.0800/-|-/-|I|-/-/-|SD3 large generation|image",
        "bedrock/stability.sd3-medium-v1:0|sd3-medium|Stable Diffusion 3 Medium|C|0.0350/-|-/-|I|-/-/-|SD3 medium generation|image",
        "bedrock/stability.stable-image-core-v1:0|stable-image-core|Stable Image Core|C|0.0400/-|-/-|I|-/-/-|Fast image generation|image",
        "bedrock/stability.stable-image-ultra-v1:0|stable-image-ultra|Stable Image Ultra|C|0.1400/-|-/-|I|-/-/-|Highest quality images|image",
        "bedrock/stability.sd3-5-medium-v1:0|sd3-5-medium|Stable Diffusion 3.5 Medium|C|0.0250/-|-/-|I|-/-/-|SD3.5 medium generation|image",
        "bedrock/stability.stable-diffusion-xl-v1|sdxl|Stable Diffusion XL|C|0.0400/-|-/-|I|-/-/-|SDXL image generation|image",
        "bedrock/stability.stable-image-edit-v1:0|stable-image-edit|Stable Image Edit|C|0.0400/-|-/-|I|-/-/-|Inpaint and outpaint|image",
        "bedrock/stability.stable-image-style-v1:0|stable-image-style|Stable Image Style|C|0.0400/-|-/-|I|-/-/-|Style transfer|image",
        "bedrock/stability.stable-image-control-v1:0|stable-image-control|Stable Image Control|C|0.0400/-|-/-|I|-/-/-|Sketch and structure to image|image",
        "bedrock/stability.stable-image-background-v1:0|stable-image-bg|Stable Image Background|C|0.0400/-|-/-|I|-/-/-|Background removal/replace|image",
        "bedrock/twelvelabs.marengo-embed-v1:0|marengo-embed|TwelveLabs Marengo Embed|C|0.0250/-|-/1K|VE|-/-/-|Video embeddings|embed",
        "bedrock/twelvelabs.pegasus-1-2-v1:0|pegasus-1-2|TwelveLabs Pegasus 1.2|C|0.5000/1.50|-/4K|D|-/-/-|Video understanding|video",
        "bedrock/writer.palmyra-x5-v1:0|palmyra-x5|Writer Palmyra X5|C|4.00/12.00|128K/8K|TJS|-/-/-|Writer latest flagship|chat",
        "bedrock/writer.palmyra-x4-v1:0|palmyra-x4|Writer Palmyra X4|C|2.00/6.00|128K/8K|TJS|-/-/-|Writer previous flagship|chat",
        "bedrock/claude-3.5-sonnet-vision|bedrock-claude-3.5-vision|AWS: Claude 3.5 Sonnet Vision|C|0.0030/0.0150|200K/4K|VSTJKC|-/-/-|Claude 3.5 Sonnet deployed via AWS Bedrock|chat",
        "bedrock/amazon-nova-pro-vision|bedrock-nova-pro-vision|AWS: Nova Pro Vision|C|0.000800/0.0032|300K/5K|VSTJ|-/-/-|Amazon Nova Pro multimodal model via Bedrock|chat",
        "bedrock/amazon-nova-lite-vision|bedrock-nova-lite-vision|AWS: Nova Lite Vision|C|0.000060/0.000240|300K/5K|VSTJ|-/-/-|Lightweight Nova vision model|chat",

    // === BETA (3 models) ===
        "beta/anthropic-research-opus|claude-research-b|Anthropic: Research Opus|C|3.00/12.00|200K/32K|VSTJKC|-/-/-|Claude Opus research edition|chat",
        "beta/gpt-4.5-preview|gpt-4.5-prev|OpenAI: GPT-4.5 Preview|C|0.0020/0.0060|128K/16K|VSTJK|-/-/-|GPT-4.5 early preview|chat",
        "beta/gemini-2-preview|gemini-2-prev|Google: Gemini 2 Preview|C|0.0100/0.0400|1000K/8K|VSTJK|-/-/-|Gemini 2 preview edition|chat",

    // === BYTEDANCE (1 models) ===
        "bytedance/ui-tars-1.5-7b|ui-tars-1.5-7b|ByteDance: UI-TARS 7B|C|0.000000/0.000000|128K/2K|V|-/-/-|UI-TARS-1.5 is a multimodal vision-language agent optimized for GUI-based enviro|chat",

    // === BYTEDANCE-SEED (2 models) ===
        "bytedance-seed/seed-1.6-flash|seed-1.6-flash|ByteDance Seed: Seed 1.6 Flash|C|0.000000/0.000000|262K/16K|JKSTV|-/-/-|Seed 1.6 Flash is an ultra-fast multimodal deep thinking model by ByteDance Seed|chat",
        "bytedance-seed/seed-1.6|seed-1.6|ByteDance Seed: Seed 1.6|C|0.000000/0.000002|262K/32K|JKSTV|-/-/-|Seed 1.6 is a general-purpose model released by the ByteDance Seed team. It inco|chat",

    // === CALIBRATION (1 models) ===
        "calibration/temperature-scaling|temp-scale-f|Temperature Scaling|C|0.000010/0.000030|512/512|VST|-/-/-|Confidence calibration|chat",

    // === CEREBRAS (7 models) ===
        "cerebras/llama3.1-8b|llama-3.1-8b|Llama 3.1 8B|C|0.1000/0.1000|8K/8K|TJS|-/-/-|Meta Llama 3.1 8B on Cerebras WSE, ultra-fast|chat",
        "cerebras/llama3.1-70b|llama-3.1-70b|Llama 3.1 70B|C|0.6000/0.6000|8K/8K|TJS|-/-/-|Meta Llama 3.1 70B on Cerebras WSE|chat",
        "cerebras/llama-3.3-70b|llama-3.3-70b|Llama 3.3 70B|C|0.8500/1.20|8K/8K|TJS|-/-/-|Latest Llama 3.3 on Cerebras WSE|chat",
        "cerebras/qwen-2.5-32b|qwen-2.5-32b|Qwen 2.5 32B|C|0.2000/0.2000|8K/8K|TJS|-/-/-|Alibaba Qwen 2.5 32B on Cerebras|chat",
        "cerebras/qwen-2.5-coder-32b|qwen-coder-32b|Qwen 2.5 Coder 32B|C|0.2000/0.2000|8K/8K|TJS|-/-/-|Qwen 2.5 Coder for code generation|chat",
        "cerebras/deepseek-r1-distill-llama-70b|deepseek-r1-70b|DeepSeek R1 Distill 70B|C|0.8500/1.20|8K/8K|TJK|-/-/-|DeepSeek R1 reasoning distilled to Llama|chat",
        "cerebras/llama-4-scout-17b-16e-instruct|llama-4-scout|Llama 4 Scout 17B|C|0.1500/0.6000|131K/8K|VTJS|-/-/-|Meta Llama 4 Scout on Cerebras WSE|chat",

    // === CHAT (2 models) ===
        "chat/neural-chat-7b-v3-conversation|neural-chat-conv|neural-conv|Intel: Neural Chat 7B Conversation|C|0.000000/0.000000|8K/2K|VSTJ|-/-/-|Neural Chat optimized for natural conversation flow|chat",
        "chat/zephyr-7b-beta-chat-optimized|zephyr-chat|zephyr-conv|HuggingFace: Zephyr 7B Chat Optimized|C|0.000000/0.000000|4K/2K|VSTJ|-/-/-|Zephyr 7B optimized for chat interactions|chat",

    // === CHINA (3 models) ===
        "china/qwen-72b|qwen-72b-c|Alibaba: Qwen 72B|C|0.000800/0.000800|32K/2K|VSTJ|-/-/-|Qwen 72B large language model|chat",
        "china/baichuan2-13b|baichuan2-13b-c|Baichuan: Baichuan2 13B|C|0.000400/0.000400|4K/2K|VSTJ|-/-/-|Baichuan 13B Chinese optimized|chat",
        "china/internlm2-20b|internlm2-20b-c|Shanghai AI Lab: InternLM2 20B|C|0.000500/0.000500|4K/2K|VSTJ|-/-/-|InternLM2 20B multilingual|chat",

    // === CLASSIFICATION (3 models) ===
        "classification/xlnet-base-cased-imdb|xlnet-classification|text-classify|XLNet: IMDB Classification|C|0.000000/0.000000|512/256|S|-/-/-|XLNet for document classification|chat",
        "classification/deberta-v3-large-mnli|deberta-mnli|text-inference|Microsoft: DeBERTa MNLI|C|0.000000/0.000000|512/256|S|-/-/-|DeBERTa for textual entailment|chat",
        "classification/electra-large-discriminator-finetuned-rte|electra-rte|text-entailment|Google: ELECTRA RTE|C|0.000000/0.000000|512/256|S|-/-/-|ELECTRA for RTE entailment classification|chat",

    // === CLASSIFY (3 models) ===
        "classify/distilroberta-base|distilroberta-f|DistilRoBERTa Base|C|0.000080/0.000250|512/256|VSTJ|-/-/-|RoBERTa classification|chat",
        "classify/deberta-v3-large|deberta-f|DeBERTa v3 Large|C|0.000150/0.000450|512/256|VSTJ|-/-/-|DeBERTa classification|chat",
        "classify/electra-large|electra-f|ELECTRA Large|C|0.000120/0.000350|512/256|VSTJ|-/-/-|ELECTRA classification|chat",

    // === CODE (12 models) ===
        "code/deepseek-coder-7b|deepseek-coder-7b-f|DeepSeek: Coder 7B|C|0.000150/0.000150|4K/2K|VSTJK|-/-/-|DeepSeek Code 7B|chat",
        "code/deepseek-coder-97b|deepseek-coder-97b-f|DeepSeek: Coder 97B|C|0.000900/0.000900|4K/2K|VSTJK|-/-/-|DeepSeek Code 97B|chat",
        "code/codestral|codestral-f|Mistral: Codestral|C|0.000500/0.0015|32K/2K|VSTJK|-/-/-|Mistral Codestral code model|chat",
        "code/granite-code-3b|granite-3b-f|IBM: Granite Code 3B|C|0.000080/0.000080|2K/1K|VSTJK|-/-/-|Granite Code 3B lightweight|chat",
        "code/granite-code-20b|granite-20b-f|IBM: Granite Code 20B|C|0.000300/0.000300|4K/1K|VSTJK|-/-/-|Granite Code 20B|chat",
        "code/replit-code-v1.5|replit-code-f|Replit: Code v1.5|C|0.000200/0.000200|4K/1K|VSTJK|-/-/-|Replit Code generation|chat",
        "code/starcoder|starcoder|star-coder|BigCode: StarCoder|C|0.000000/0.000000|8K/2K|JT|-/-/-|StarCoder 15B for code generation|chat",
        "code/starcoder2|starcoder2|star-coder-2|BigCode: StarCoder2|C|0.000000/0.000000|16K/4K|JT|-/-/-|StarCoder2 15B improved code model|chat",
        "code/phi-1-code|phi-1-code|phi-code|Microsoft: Phi 1 Code|C|0.000000/0.000000|2K/1K|JT|-/-/-|Phi 1 1.3B specialized for coding|chat",
        "code/wizardcoder-15b|wizardcoder|wizard-code|WizardCoder: 15B|C|0.000000/0.000000|4K/2K|JT|-/-/-|WizardCoder 15B for complex code tasks|chat",
        "code/codellama-34b|codellama-34b|codellama-large|Meta: Code Llama 34B|C|0.000000/0.000001|16K/4K|JT|-/-/-|Code Llama 34B for advanced programming|chat",
        "code/codellama-70b|codellama-70b|codellama-xl|Meta: Code Llama 70B|C|0.000001/0.000002|16K/4K|JT|-/-/-|Code Llama 70B with extended capabilities|chat",

    // === CODING (4 models) ===
        "coding/devgpt|devgpt|coding-dev|DevGPT: Development|C|0.000001/0.000004|8K/2K|VTJK|-/-/-|AI pair programmer for full-stack development|chat",
        "coding/debuggpt|debuggpt|coding-debug|DebugGPT: Testing|C|0.000001/0.000003|8K/2K|VTJK|-/-/-|Automated debugging and test generation|chat",
        "coding/docgpt|docgpt|coding-doc|DocGPT: Documentation|C|0.000001/0.000002|4K/2K|VSTJ|-/-/-|Code documentation and explanation generation|chat",
        "coding/securegpt|securegpt|coding-security|SecureGPT: Security|C|0.000001/0.000004|8K/2K|VTJK|-/-/-|Security vulnerability detection and remediation|chat",

    // === COGNITIVECOMPUTATIONS (1 models) ===
        "cognitivecomputations/dolphin-mistral-24b-venice-edition:free|dolphin-mistral-24b-|Venice: Uncensored (free)|C|-/-|32K/8K|JS|-/-/-|Venice Uncensored Dolphin Mistral 24B Venice Edition is a fine-tuned variant of |chat",

    // === COGVLM (1 models) ===
        "cogvlm/cogvlm2-9b-instruct|cogvlm2-9b|Hugging Face: CogVLM2 9B|C|0.000000/0.000002|8K/2K|VST|-/-/-|Chinese-optimized vision language model|chat",

    // === COHERE (17 models) ===
        "cohere/command-a|command-a|Cohere: Command A|C|0.000003/0.000010|256K/8K|JS|-/-/-|Command A is an open-weights 111B parameter model with a 256k context window foc|chat",
        "cohere/command-r7b-12-2024|command-r7b-12-2024|Cohere: Command R7B (12-2024)|C|0.000000/0.000000|128K/4K|JS|-/-/-|Command R7B (12-2024) is a small, fast update of the Command R+ model, delivered|chat",
        "cohere/command-r-08-2024|command-r-08-2024|Cohere: Command R (08-2024)|C|0.000000/0.000001|128K/4K|JST|-/-/-|command-r-08-2024 is an update of the [Command R](/models/cohere/command-r) with|chat",
        "cohere/command-r-plus-08-2024|command-r-plus-08-20|Cohere: Command R+ (08-2024)|C|0.000003/0.000010|128K/4K|JST|-/-/-|command-r-plus-08-2024 is an update of the [Command R+](/models/cohere/command-r|chat",
        "cohere/command-a-03-2025|command-a|Command A|C|2.50/10.00|256K/8K|VTJS|-/-/-|Agent-focused model with 256K context|chat",
        "cohere/command|command|Command|C|1.00/2.00|4K/4K|TJ|-/-/-|Legacy command model|chat",
        "cohere/command-light|command-light|Command Light|C|0.3000/0.6000|4K/4K|TJ|-/-/-|Legacy lightweight command model|chat",
        "cohere/aya-expanse-32b|aya-expanse-32b|Aya Expanse 32B|C|0.5000/1.50|128K/4K|TJ|-/-/-|Multilingual model supporting 23 languages|chat",
        "cohere/aya-expanse-8b|aya-expanse-8b|Aya Expanse 8B|C|0.0500/0.1000|8K/4K|TJ|-/-/-|Efficient multilingual model|chat",
        "cohere/embed-english-v3.0|embed-english-v3|Embed English v3|C|0.1000/-|512/1K|E|-/-/-|English text embeddings|embed",
        "cohere/embed-multilingual-v3.0|embed-multilingual-v3|Embed Multilingual v3|C|0.1000/-|512/1K|E|-/-/-|Multilingual text embeddings|embed",
        "cohere/embed-english-light-v3.0|embed-english-light|Embed English Light v3|C|0.1000/-|512/384|E|-/-/-|Lightweight English embeddings|embed",
        "cohere/embed-multilingual-light-v3.0|embed-multilingual-light|Embed Multilingual Light v3|C|0.1000/-|512/384|E|-/-/-|Lightweight multilingual embeddings|embed",
        "cohere/rerank-v3.5|rerank-v3.5|Rerank v3.5|C|2.00/-|4K/-|R|-/-/-|Document reranking model, $2/1K searches|rerank",
        "cohere/rerank-english-v3.0|rerank-english-v3|Rerank English v3|C|2.00/-|4K/-|R|-/-/-|English document reranking|rerank",
        "cohere/rerank-multilingual-v3.0|rerank-multilingual-v3|Rerank Multilingual v3|C|2.00/-|4K/-|R|-/-/-|Multilingual document reranking|rerank",
        "cohere/command-r7-plus-20260110|cohere-command-r7-plus|Cohere Command R7 Plus|C|0.000000/0.000006|128K/4K|TJ|-/-/-|Latest Cohere advanced model|chat",

    // === COREFERENCE (1 models) ===
        "coreference/coref-roberta|coref-roberta|coreference|AllenAI: Coreference RoBERTa|C|0.000000/0.000000|512/256|S|-/-/-|RoBERTa for coreference resolution|chat",

    // === COT (1 models) ===
        "cot/cot-generator|cot-f|CoT Generator|C|0.000400/0.0012|4K/2K|VSTJ|-/-/-|Chain-of-thought generator|chat",

    // === DEEPCOGITO (4 models) ===
        "deepcogito/cogito-v2.1-671b|cogito-v2.1-671b|Deep Cogito: Cogito v2.1 671B|C|0.000001/0.000001|128K/32K|JKS|-/-/-|Cogito v2.1 671B MoE represents one of the strongest open models globally, match|chat",
        "deepcogito/cogito-v2-preview-llama-405b|cogito-v2-preview-ll|Deep Cogito: Cogito V2 Preview Llama 405B|C|0.000003/0.000003|32K/8K|JKST|-/-/-|Cogito v2 405B is a dense hybrid reasoning model that combines direct answering |chat",
        "deepcogito/cogito-v2-preview-llama-70b|cogito-v2-preview-ll|Deep Cogito: Cogito V2 Preview Llama 70B|C|0.000001/0.000001|32K/8K|JKST|-/-/-|Cogito v2 70B is a dense hybrid reasoning model that combines direct answering c|chat",
        "deepcogito/cogito-v2-preview-llama-109b-moe|cogito-v2-preview-ll|Cogito V2 Preview Llama 109B|C|0.000000/0.000001|32K/8K|KTV|-/-/-|An instruction-tuned, hybrid-reasoning Mixture-of-Experts model built on Llama-4|chat",

    // === DEEPGRAM (20 models) ===
        "deepgram/nova-3|nova-3|Nova 3|C|0.0043/-|-/-|A|-/-/-|Most accurate STT, $0.0043/min|audio",
        "deepgram/nova-3-medical|nova-3-medical|Nova 3 Medical|C|0.0050/-|-/-|A|-/-/-|Medical transcription, $0.005/min|audio",
        "deepgram/nova-2|nova-2|Nova 2|C|0.0043/-|-/-|A|-/-/-|Previous gen STT, $0.0043/min|audio",
        "deepgram/nova-2-general|nova-2-general|Nova 2 General|C|0.0043/-|-/-|A|-/-/-|General purpose, $0.0043/min|audio",
        "deepgram/nova-2-meeting|nova-2-meeting|Nova 2 Meeting|C|0.0043/-|-/-|A|-/-/-|Meeting transcription, $0.0043/min|audio",
        "deepgram/nova-2-phonecall|nova-2-phone|Nova 2 Phone|C|0.0043/-|-/-|A|-/-/-|Phone call transcription, $0.0043/min|audio",
        "deepgram/nova-2-voicemail|nova-2-voicemail|Nova 2 Voicemail|C|0.0043/-|-/-|A|-/-/-|Voicemail transcription, $0.0043/min|audio",
        "deepgram/nova-2-finance|nova-2-finance|Nova 2 Finance|C|0.0043/-|-/-|A|-/-/-|Financial transcription, $0.0043/min|audio",
        "deepgram/nova-2-conversational-ai|nova-2-conv|Nova 2 Conversational|C|0.0043/-|-/-|A|-/-/-|Conversational AI, $0.0043/min|audio",
        "deepgram/nova-2-drivethru|nova-2-drive|Nova 2 Drive-thru|C|0.0043/-|-/-|A|-/-/-|Drive-thru ordering, $0.0043/min|audio",
        "deepgram/nova-2-atc|nova-2-atc|Nova 2 ATC|C|0.0043/-|-/-|A|-/-/-|Air traffic control, $0.0043/min|audio",
        "deepgram/whisper-cloud|whisper-cloud|Whisper Cloud|C|0.0048/-|-/-|A|-/-/-|OpenAI Whisper on Deepgram, $0.0048/min|audio",
        "deepgram/whisper-cloud-large|whisper-large|Whisper Large|C|0.0048/-|-/-|A|-/-/-|Whisper Large, $0.0048/min|audio",
        "deepgram/whisper-cloud-medium|whisper-medium|Whisper Medium|C|0.0048/-|-/-|A|-/-/-|Whisper Medium, $0.0048/min|audio",
        "deepgram/enhanced|enhanced|Enhanced|C|0.0145/-|-/-|A|-/-/-|Enhanced accuracy, $0.0145/min|audio",
        "deepgram/base|base|Base|C|0.0125/-|-/-|A|-/-/-|Base model, $0.0125/min|audio",
        "deepgram/aura-asteria-en|aura-asteria|Aura Asteria|C|0.0150/-|-/-|A|-/-/-|TTS Asteria voice, $0.015/1K chars|audio",
        "deepgram/aura-luna-en|aura-luna|Aura Luna|C|0.0150/-|-/-|A|-/-/-|TTS Luna voice, $0.015/1K chars|audio",
        "deepgram/aura-stella-en|aura-stella|Aura Stella|C|0.0150/-|-/-|A|-/-/-|TTS Stella voice, $0.015/1K chars|audio",
        "deepgram/aura-orion-en|aura-orion|Aura Orion|C|0.0150/-|-/-|A|-/-/-|TTS Orion voice, $0.015/1K chars|audio",

    // === DEEPSEEK (23 models) ===
        "deepseek/deepseek-v3.2-speciale|deepseek-v3.2-specia|DeepSeek: DeepSeek V3.2 Speciale|C|0.000000/0.000000|163K/65K|JKS|-/-/-|DeepSeek-V3.2-Speciale is a high-compute variant of DeepSeek-V3.2 optimized for |chat",
        "deepseek/deepseek-v3.2|deepseek-v3.2|DeepSeek: DeepSeek V3.2|C|0.000000/0.000000|163K/65K|JKST|-/-/-|DeepSeek-V3.2 is a large language model designed to harmonize high computational|chat",
        "deepseek/deepseek-v3.2-exp|deepseek-v3.2-exp|DeepSeek: DeepSeek V3.2 Exp|C|0.000000/0.000000|163K/65K|JKST|-/-/-|DeepSeek-V3.2-Exp is an experimental large language model released by DeepSeek a|chat",
        "deepseek/deepseek-v3.1-terminus:exacto|deepseek-v3.1-termin|DeepSeek: DeepSeek V3.1 Terminus (exacto)|C|0.000000/0.000001|163K/40K|JKST|-/-/-|DeepSeek-V3.1 Terminus is an update to [DeepSeek V3.1](/deepseek/deepseek-chat-v|chat",
        "deepseek/deepseek-v3.1-terminus|deepseek-v3.1-termin|DeepSeek: DeepSeek V3.1 Terminus|C|0.000000/0.000001|163K/40K|JKST|-/-/-|DeepSeek-V3.1 Terminus is an update to [DeepSeek V3.1](/deepseek/deepseek-chat-v|chat",
        "deepseek/deepseek-chat-v3.1|deepseek-chat-v3.1|DeepSeek: DeepSeek V3.1|C|0.000000/0.000001|32K/7K|JKST|-/-/-|DeepSeek-V3.1 is a large hybrid reasoning model (671B parameters, 37B active) th|chat",
        "deepseek/deepseek-r1-0528-qwen3-8b|deepseek-r1-0528-qwe|DeepSeek: DeepSeek R1 0528 Qwen3 8B|C|0.000000/0.000000|128K/32K|K|-/-/-|DeepSeek-R1-0528 is a lightly upgraded release of DeepSeek R1 that taps more com|chat",
        "deepseek/deepseek-r1-0528:free|deepseek-r1-0528:fre|DeepSeek: R1 0528 (free)|C|-/-|163K/40K|K|-/-/-|May 28th update to the [original DeepSeek R1](/deepseek/deepseek-r1) Performance|chat",
        "deepseek/deepseek-r1-0528|deepseek-r1-0528|DeepSeek: R1 0528|C|0.000000/0.000002|163K/65K|JKST|-/-/-|May 28th update to the [original DeepSeek R1](/deepseek/deepseek-r1) Performance|chat",
        "deepseek/deepseek-prover-v2|deepseek-prover-v2|DeepSeek: DeepSeek Prover V2|C|0.000000/0.000002|163K/40K|J|-/-/-|DeepSeek Prover V2 is a 671B parameter model, speculated to be geared towards lo|chat",
        "deepseek/deepseek-chat-v3-0324|deepseek-chat-v3-032|DeepSeek: DeepSeek V3 0324|C|0.000000/0.000001|163K/40K|JKST|-/-/-|DeepSeek V3, a 685B-parameter, mixture-of-experts model, is the latest iteration|chat",
        "deepseek/deepseek-r1-distill-qwen-32b|deepseek-r1-distill-|DeepSeek: R1 Distill Qwen 32B|C|0.000000/0.000000|131K/32K|JKS|-/-/-|DeepSeek R1 Distill Qwen 32B is a distilled large language model based on [Qwen |chat",
        "deepseek/deepseek-r1-distill-qwen-14b|deepseek-r1-distill-|DeepSeek: R1 Distill Qwen 14B|C|0.000000/0.000000|32K/16K|JKS|-/-/-|DeepSeek R1 Distill Qwen 14B is a distilled large language model based on [Qwen |chat",
        "deepseek/deepseek-r1-distill-llama-70b|deepseek-r1-distill-|DeepSeek: R1 Distill Llama 70B|C|0.000000/0.000000|131K/131K|JKST|-/-/-|DeepSeek R1 Distill Llama 70B is a distilled large language model based on [Llam|chat",
        "deepseek/deepseek-r1|deepseek-r1|DeepSeek: R1|C|0.000000/0.000001|163K/40K|JKST|-/-/-|DeepSeek R1 is here: Performance on par with [OpenAI o1](/openai/o1), but open-s|chat",
        "deepseek/deepseek-chat|deepseek-chat|DeepSeek: DeepSeek V3|C|0.000000/0.000001|163K/163K|JST|-/-/-|DeepSeek-V3 is the latest model from the DeepSeek team, building upon the instru|chat",
        "deepseek/deepseek-reasoner|deepseek-r1|DeepSeek R1|C|0.5500/2.19/0.1400|65K/8K|VTJSK|-/-/-|Reasoning model with chain-of-thought|chat",
        "deepseek/deepseek-reasoner-0528|deepseek-r1-0528|DeepSeek R1 0528|C|0.5500/2.19/0.1400|65K/8K|VTJSK|-/-/-|DeepSeek R1 May 2028 version|chat",
        "deepseek/deepseek-coder|deepseek-coder|DeepSeek Coder|C|0.1400/0.2800/0.0350|65K/8K|TJS|-/-/-|Specialized code generation model|chat",
        "deepseek/deepseek-chat-v2|deepseek-v2|DeepSeek V2|C|0.1400/0.2800|32K/4K|TJS|-/-/-|Previous generation model|chat",
        "deepseek/deepseek-chat-v2.5|deepseek-v2.5|DeepSeek V2.5|C|0.1400/0.2800|32K/8K|TJS|-/-/-|Combined chat and code capabilities|chat",
        "deepseek/deepseek-v3-2-20260104|deepseek-v3-2|DeepSeek V3.2|C|0.000003/0.000008|64K/4K|SVTK|-/-/-|Advanced reasoning with o1-style thinking|chat",
        "deepseek/deepseek-vl2|deepseek-vl2|DeepSeek: DeepSeek-VL2|C|0.000000/0.000005|8K/2K|VST|-/-/-|DeepSeek vision language model with multi-image support|chat",

    // === DETECT (1 models) ===
        "detect/langdetect|langdetect-f|LangDetect|C|0.000010/0.000010|1K/32|VST|-/-/-|Language identification|chat",

    // === DIALOGUE (4 models) ===
        "dialogue/neural-chat-8b|neural-chat-8b|neural-chat|Intel: Neural Chat 8B|C|0.000000/0.000000|8K/2K|VSTJ|-/-/-|Intel Neural Chat 8B for conversations|chat",
        "dialogue/airoboros-7b|airoboros-7b|airoboros|Airoboros: 7B|C|0.000000/0.000000|4K/2K|VSTJ|-/-/-|Airoboros 7B dialogue and instruction model|chat",
        "dialogue/evolutionaryqa-7b|evolutionaryqa|evo-qa|EvolutionaryQA: 7B|C|0.000000/0.000000|4K/2K|VSTJ|-/-/-|EvolutionaryQA for question answering|chat",
        "dialogue/airoboros-13b|airoboros-13b|airoboros-large|Airoboros: 13B|C|0.000000/0.000001|4K/2K|VSTJ|-/-/-|Larger Airoboros 13B variant|chat",

    // === DISTILL (1 models) ===
        "distill/student-teacher-model|distill-f|Student-Teacher Model|C|0.000100/0.000300|512/512|VSTJ|-/-/-|Knowledge distillation pair|chat",

    // === DOCUMENT (2 models) ===
        "document/layoutlm-base-uncased|layoutlm|doc-understanding|Microsoft: LayoutLM Base|C|0.000000/0.000000|512/256|VS|-/-/-|LayoutLM for document layout understanding|chat",
        "document/layoutlmv2-base-uncased|layoutlmv2|doc-layout-v2|Microsoft: LayoutLMv2 Base|C|0.000000/0.000000|512/256|VS|-/-/-|LayoutLMv2 improved document understanding|chat",

    // === DOMAIN (7 models) ===
        "domain/med-gemini-1.5|med-gemini-1.5|Google: Med-Gemini 1.5|C|0.0075/0.0300|1000K/8K|VSTJK|-/-/-|Gemini 1.5 specialized for medical|chat",
        "domain/legal-palm-2|legal-palm-2|Google: Legal PaLM 2|C|0.000500/0.0015|8K/2K|VSTJ|-/-/-|PaLM 2 fine-tuned for legal|chat",
        "domain/financial-llama|financial-llama|Meta: Llama Financial|C|0.000600/0.000600|8K/2K|VSTJ|-/-/-|Llama fine-tuned for finance|chat",
        "domain/legal-llama-2|legal-llama-f|Legal Llama 2|C|0.000600/0.000600|8K/2K|VSTJ|-/-/-|Llama 2 fine-tuned for legal|chat",
        "domain/medical-falcon-40b|medical-falcon-f|Medical Falcon 40B|C|0.000800/0.000800|2K/2K|VSTJ|-/-/-|Falcon 40B medical variant|chat",
        "domain/financial-mpnet|financial-mpnet-f|Financial MPNet|C|0.000100/0.000300|384/512|VSTJ|-/-/-|MPNet financial domain|chat",
        "domain/scientific-bert|scientific-bert-f|SciBERT|C|0.000080/0.000250|512/512|VSTJ|-/-/-|SciBERT scientific papers|chat",

    // === EDGE (7 models) ===
        "edge/mobilelm-500m|mobilelm-500m-e|MobileLM: 500M|C|0.000010/0.000010|2K/256|VST|-/-/-|Ultra-lightweight mobile LLM|chat",
        "edge/phi-2-small|phi-2-small-e|Microsoft: Phi-2 Small|C|0.000080/0.000080|2K/512|VST|-/-/-|Phi-2 2.7B model|chat",
        "edge/distilbert-base|distilbert-f|DistilBERT Base|C|0.000050/0.000150|512/512|VSTJ|-/-/-|Lightweight BERT 40% smaller|chat",
        "edge/mobilebert|mobilebert-f|MobileBERT|C|0.000030/0.000100|512/512|VSTJ|-/-/-|BERT optimized for mobile|chat",
        "edge/tinylm-1.3b|tinylm-f|TinyLM 1.3B|C|0.000080/0.000080|512/512|VSTJ|-/-/-|TinyLM tiny language model|chat",
        "edge/qwen-1.5-0.5b|qwen-0.5b-f|Alibaba: Qwen1.5 0.5B|C|0.000020/0.000020|32K/128|VST|-/-/-|Qwen ultra-small 0.5B|chat",
        "edge/llm-mini|llm-mini-f|ModelSuite: LLM Mini|C|0.000010/0.000010|2K/256|VST|-/-/-|Mini LLM for edge devices|chat",

    // === EDUCATION (3 models) ===
        "education/tutorgpt|tutorgpt|edu-tutor|TutorGPT: Educational|C|0.000001/0.000002|8K/2K|VSTJ|-/-/-|Personalized tutoring and learning assistance|chat",
        "education/assessgpt|assessgpt|edu-assess|AssessGPT: Evaluation|C|0.000001/0.000002|4K/2K|VSTJ|-/-/-|Educational assessment and grading automation|chat",
        "education/currigpt|currigpt|edu-curriculum|CurricGPT: Planning|C|0.000001/0.000003|8K/2K|VSTJ|-/-/-|Curriculum design and educational planning|chat",

    // === ELEUTHERAI (1 models) ===
        "eleutherai/llemma_7b|llemma_7b|EleutherAI: Llemma 7b|C|0.000001/0.000001|4K/4K|-|-/-/-|Llemma 7B is a language model for mathematics. It was initialized with Code Llam|chat",

    // === ELEVENLABS (10 models) ===
        "elevenlabs/eleven_multilingual_v2|multilingual-v2|Multilingual v2|C|0.1800/-|5K/-|A|-/-/-|29 languages, most natural, $0.18/1K chars|audio",
        "elevenlabs/eleven_turbo_v2_5|turbo-v2.5|Turbo v2.5|C|0.1800/-|5K/-|A|-/-/-|32 languages, low latency, $0.18/1K chars|audio",
        "elevenlabs/eleven_turbo_v2|turbo-v2|Turbo v2|C|0.1800/-|5K/-|A|-/-/-|English only, fastest, $0.18/1K chars|audio",
        "elevenlabs/eleven_monolingual_v1|english-v1|English v1|C|0.1800/-|5K/-|A|-/-/-|English only, legacy model, $0.18/1K chars|audio",
        "elevenlabs/eleven_flash_v2_5|flash-v2.5|Flash v2.5|C|0.0900/-|5K/-|A|-/-/-|Ultra-low latency, $0.09/1K chars|audio",
        "elevenlabs/eleven_flash_v2|flash-v2|Flash v2|C|0.0900/-|5K/-|A|-/-/-|Fast English, $0.09/1K chars|audio",
        "elevenlabs/eleven_sound_effects|sound-effects|Sound Effects|C|0.1800/-|450/-|A|-/-/-|Sound effect generation, $0.18/1K chars|audio",
        "elevenlabs/eleven_voice_isolation|voice-isolation|Voice Isolation|C|0.5000/-|-/-|A|-/-/-|Isolate voices from audio, $0.50/min|audio",
        "elevenlabs/eleven_speech_to_speech_v2|speech-to-speech|Speech to Speech v2|C|0.2500/-|-/-|A|-/-/-|Voice conversion, $0.25/1K chars|audio",
        "elevenlabs/eleven_conversational_v1|conversational-v1|Conversational v1|C|0.1800/-|5K/-|A|-/-/-|Optimized for dialogue, $0.18/1K chars|audio",

    // === EMBEDDING (13 models) ===
        "embedding/jina-embeddings-v3|jina-v3-f|Jina: Embeddings v3|C|0.000010/0.000020|8K/1K|VSTJ|-/-/-|Jina embeddings v3 large|chat",
        "embedding/bge-large-zh|bge-large-zh-f|BAAI: BGE Large ZH|C|0.000010/0.000020|512/1K|VSTJ|-/-/-|BGE Chinese embeddings|chat",
        "embedding/multilingual-e5-large|multilingual-e5-f|E5: Multilingual Large|C|0.000010/0.000020|512/1K|VSTJ|-/-/-|E5 multilingual embeddings|chat",
        "embedding/text-embedding-3-large|ada-large-f|OpenAI: Embedding 3 Large|C|0.000130/0.0013|8K/3K|VSTJ|-/-/-|OpenAI text embedding large|chat",
        "embedding/text-embedding-3-small|ada-small-f|OpenAI: Embedding 3 Small|C|0.000020/0.000200|8K/1K|VSTJ|-/-/-|OpenAI text embedding small|chat",
        "embedding/bge-base-en|bge-base|bge-small|BAAI: BGE Base EN|C|0.000000/0.000000|512/256|VS|-/-/-|BAAI BGE Base for English embeddings|chat",
        "embedding/bge-large-en|bge-large|bge-large-en|BAAI: BGE Large EN|C|0.000000/0.000000|512/256|VS|-/-/-|BAAI BGE Large English embeddings|chat",
        "embedding/bge-m3|bge-m3|bge-multilingual|BAAI: BGE-M3|C|0.000000/0.000000|8K/512|VS|-/-/-|BAAI BGE-M3 for 100+ languages|chat",
        "embedding/e5-base|e5-base|e5-small|Hugging Face: E5 Base|C|0.000000/0.000000|512/256|VS|-/-/-|E5 Base for semantic search|chat",
        "embedding/e5-large|e5-large|e5-large-v2|Hugging Face: E5 Large|C|0.000000/0.000000|512/512|VS|-/-/-|E5 Large improved embeddings|chat",
        "embedding/jina-embeddings-v2|jina-embeddings|jina-v2|Jina AI: Embeddings v2|C|0.000000/0.000000|8K/768|VS|-/-/-|Jina Embeddings v2 for long context|chat",
        "embedding/sentence-transformers-base|st-base|sentence-transformer|Sentence Transformers: Base|C|0.000000/0.000000|512/256|VS|-/-/-|Sentence Transformers Base model|chat",
        "embedding/all-minilm-l6-v2|minilm-l6|minilm-small|Sentence Transformers: MiniLM L6 v2|C|0.000000/0.000000|512/256|VS|-/-/-|MiniLM L6 v2 lightweight embeddings|chat",

    // === ENERGY (2 models) ===
        "energy/smartgridgpt|smartgrid|energy-grid|SmartGridGPT: Analytics|C|0.000001/0.000003|8K/2K|VSTJ|-/-/-|Smart grid optimization and forecasting|chat",
        "energy/renewable-optimizer|renewable-opt|energy-renewable|RenewableOptimizer LLM|C|0.000001/0.000004|8K/2K|VSTJ|-/-/-|Renewable energy production optimization|chat",

    // === ENSEMBLE (2 models) ===
        "ensemble/llm-fusion|llm-fusion-e|ModelSuite: LLM Fusion|C|0.0050/0.0100|8K/2K|VSTJK|-/-/-|Ensemble combining multiple models|chat",
        "ensemble/vision-fusion|vision-fusion-e|ModelSuite: Vision Fusion|C|0.0010/0.0020|4K/2K|VSTJK|-/-/-|Ensemble combining vision models|chat",

    // === ENTERPRISE (4 models) ===
        "enterprise/claude-opus-4.5-20251101|claude-4.5-prod|opus-enterprise|Anthropic: Claude Opus 4.5 (Enterprise)|C|5.00/25.00|200K/32K|VTJSKC|-/-/-|Claude Opus 4.5 optimized for enterprise deployments|chat",
        "enterprise/gpt-4o-20250101|gpt-4o-prod|gpt-4o-enterprise|OpenAI: GPT-4o (Enterprise)|C|2.50/10.00|128K/16K|VSTJS|-/-/-|GPT-4o optimized for enterprise production|chat",
        "enterprise/gemini-2.5-flash-prod|gemini-2.5-flash-ent|gemini-enterprise|Google: Gemini 2.5 Flash (Enterprise)|C|0.3750/1.50|1000K/8K|VSTJK|-/-/-|Gemini 2.5 Flash for enterprise-scale inference|chat",
        "enterprise/mistral-large-2-prod|mistral-large-ent|mistral-enterprise|Mistral: Large 2 (Enterprise)|C|1.20/3.60|128K/8K|VSTJ|-/-/-|Mistral Large 2 for production deployments|chat",

    // === ESSENTIALAI (1 models) ===
        "essentialai/rnj-1-instruct|rnj-1-instruct|EssentialAI: Rnj 1 Instruct|C|0.000000/0.000000|32K/8K|JS|-/-/-|Rnj-1 is an 8B-parameter, dense, open-weight model family developed by Essential|chat",

    // === FINANCE (7 models) ===
        "finance/blipbert-finance|blipbert-fin|finance-bert|Bloomberg: BLIPBert Finance|C|0.000000/0.000002|4K/2K|VSTJ|-/-/-|Fine-tuned for financial document analysis|chat",
        "finance/econombert|econombert|finance-econ|EconBERT: Economic|C|0.000000/0.000002|4K/2K|VSTJ|-/-/-|BERT model for economic and financial texts|chat",
        "finance/stockllama|stockllama|finance-stock|StockLlama: Trading|C|0.000001/0.000003|8K/2K|VSTJ|-/-/-|Llama variant for stock market analysis and trading|chat",
        "finance/tradingllm|tradingllm|finance-trading|TradingLLM: Quantitative|C|0.000001/0.000004|8K/2K|VSTJ|-/-/-|Specialized LLM for quantitative trading strategies|chat",
        "finance/financialbert|financialbert|financial-domain|FinancialBERT: Domain|C|0.000000/0.000000|512/256|S|-/-/-|FinancialBERT for financial text|chat",
        "finance/secbert|secbert|sec-bert|SecBERT: SEC Filings|C|0.000000/0.000000|512/256|S|-/-/-|SecBERT trained on SEC filings|chat",
        "finance/stockbert|stockbert|stock-bert|StockBERT: Market|C|0.000000/0.000000|512/256|S|-/-/-|StockBERT for stock market analysis|chat",

    // === FIREWORKS (250 models) ===
        "fireworks/chronos-hermes-13b-v2|chronos-hermes-13b-v2|Chronos Hermes 13B v2|C|0.2000/0.2000|4K/4K|J|-/-/-|(chronos-13b-v2 + Nous-Hermes-Llama2-13b) 75/25 merge. This offers the imaginati|chat",
        "fireworks/codegemma-2b|codegemma-2b|CodeGemma 2B|C|0.2000/0.2000|8K/8K|J|-/-/-|CodeGemma is a collection of lightweight open code models built on top of Gemma.|chat",
        "fireworks/codegemma-7b|codegemma-7b|CodeGemma 7B|C|0.2000/0.2000|8K/8K|J|-/-/-|CodeGemma is a collection of lightweight open code models built on top of Gemma.|chat",
        "fireworks/code-llama-13b|code-llama-13b|Code Llama 13B|C|0.2000/0.2000|16K/16K|J|-/-/-|Code Llama is a collection of pretrained and fine-tuned Large Language Models ra|chat",
        "fireworks/code-llama-13b-instruct|code-llama-13b|Code Llama 13B Instruct|C|0.2000/0.2000|16K/16K|J|-/-/-|Code Llama is a collection of pretrained and fine-tuned Large Language Models ra|chat",
        "fireworks/code-llama-13b-python|code-llama-13b-python|Code Llama 13B Python|C|0.2000/0.2000|16K/16K|J|-/-/-|Code Llama is a collection of pretrained and fine-tuned Large Language Models ra|chat",
        "fireworks/code-llama-34b|code-llama-34b|Code Llama 34B|C|0.1000/0.1000|16K/16K|J|-/-/-|Code Llama is a collection of pretrained and fine-tuned Large Language Models ra|chat",
        "fireworks/code-llama-34b-instruct|code-llama-34b|Code Llama 34B Instruct|C|0.1000/0.1000|16K/16K|J|-/-/-|Code Llama is a collection of pretrained and fine-tuned Large Language Models ra|chat",
        "fireworks/code-llama-34b-python|code-llama-34b-python|Code Llama 34B Python|C|0.1000/0.1000|16K/16K|J|-/-/-|Code Llama is a collection of pretrained and fine-tuned Large Language Models ra|chat",
        "fireworks/code-llama-70b|code-llama-70b|Code Llama 70B|C|0.9000/0.9000|4K/4K|J|-/-/-|Code Llama is a collection of pretrained and fine-tuned Large Language Models ra|chat",
        "fireworks/code-llama-70b-instruct|code-llama-70b|Code Llama 70B Instruct|C|0.9000/0.9000|4K/4K|J|-/-/-|Code Llama is a collection of pretrained and fine-tuned Large Language Models ra|chat",
        "fireworks/code-llama-70b-python|code-llama-70b-python|Code Llama 70B Python|C|0.9000/0.9000|4K/4K|J|-/-/-|Code Llama is a collection of pretrained and fine-tuned Large Language Models ra|chat",
        "fireworks/code-llama-7b|code-llama-7b|Code Llama 7B|C|0.2000/0.2000|16K/16K|J|-/-/-|Code Llama is a collection of pretrained and fine-tuned Large Language Models ra|chat",
        "fireworks/code-llama-7b-instruct|code-llama-7b|Code Llama 7B Instruct|C|0.2000/0.2000|16K/16K|J|-/-/-|Code Llama is a collection of pretrained and fine-tuned Large Language Models ra|chat",
        "fireworks/code-llama-7b-python|code-llama-7b-python|Code Llama 7B Python|C|0.2000/0.2000|16K/16K|J|-/-/-|Code Llama is a collection of pretrained and fine-tuned Large Language Models ra|chat",
        "fireworks/code-qwen-1p5-7b|code-qwen-1p5-7b|CodeQwen 1.5 7B|C|0.2000/0.2000|65K/16K|J|-/-/-|CodeQwen1.5 is based on Qwen1.5, a language model series including decoder langu|chat",
        "fireworks/cogito-v1-preview-llama-3b|cogito-v1-preview-llama-3b|Cogito v1 Preview Llama 3B|C|0.1000/0.1000|131K/16K|TJ|-/-/-|The Cogito LLMs are instruction tuned generative models that are also hybrid rea|chat",
        "fireworks/cogito-v1-preview-llama-70b|cogito-v1-preview-llama-70b|Cogito v1 Preview Llama 70B|C|0.9000/0.9000|131K/16K|TJ|-/-/-|The Cogito LLMs are instruction tuned generative models that are also hybrid rea|chat",
        "fireworks/cogito-v1-preview-llama-8b|cogito-v1-preview-llama-8b|Cogito v1 Preview Llama 8B|C|0.2000/0.2000|131K/16K|TJ|-/-/-|The Cogito LLMs are instruction tuned generative models that are also hybrid rea|chat",
        "fireworks/cogito-v1-preview-qwen-14b|cogito-v1-preview-qwen-14b|Cogito v1 Preview Qwen 14B|C|0.1000/0.1000|131K/16K|TJ|-/-/-|The Cogito LLMs are instruction tuned generative models that are also hybrid rea|chat",
        "fireworks/cogito-v1-preview-qwen-32b|cogito-v1-preview-qwen-32b|Cogito v1 Preview Qwen 32B|C|0.9000/0.9000|131K/16K|TJ|-/-/-|The Cogito LLMs are instruction tuned generative models that are also hybrid rea|chat",
        "fireworks/deepseek-coder-1b-base|deepseek-coder-1b-base|DeepSeek Coder 1.3B Base|C|0.9000/0.9000|16K/16K|J|-/-/-|DeepSeek Coder is composed of a series of code language models, each trained fro|chat",
        "fireworks/deepseek-coder-33b-instruct|deepseek-coder-33b|DeepSeek Coder 33B Instruct|C|0.1000/0.1000|16K/16K|J|-/-/-|Deepseek Coder is composed of a series of code language models, each trained fro|chat",
        "fireworks/deepseek-coder-7b-base|deepseek-coder-7b-base|DeepSeek Coder 7B Base|C|0.2000/0.2000|4K/4K|J|-/-/-|Deepseek Coder is composed of a series of code language models, each trained fro|chat",
        "fireworks/deepseek-coder-7b-base-v1p5|deepseek-coder-7b-base-v1p5|DeepSeek Coder 7B Base v1.5|C|0.2000/0.2000|4K/4K|J|-/-/-|The Deepseek Coder 7B Base v1.5 LLM is pre-trained from Deepseek 7B on 2T tokens|chat",
        "fireworks/deepseek-coder-7b-instruct-v1p5|deepseek-coder-7b-v1p5|DeepSeek Coder 7B Instruct v1.5|C|0.2000/0.2000|4K/4K|J|-/-/-|Deepseek-Coder-7B-Instruct-v1.5 is pre-trained from Deepseek-LLM 7B on 2T tokens|chat",
        "fireworks/deepseek-coder-v2-instruct|deepseek-coder-v2|DeepSeek Coder V2 Instruct|C|0.9000/0.9000|32K/16K|J|-/-/-|DeepSeek Coder V2 Instruct is a 236-billion-parameter open-source Mixture-of-Exp|chat",
        "fireworks/deepseek-coder-v2-lite-base|deepseek-coder-v2-lite-base|DeepSeek Coder V2 Lite Base|C|0.9000/0.9000|163K/16K|J|-/-/-|DeepSeek-Coder-V2 is an open-source Mixture-of-Experts (MoE) code language model|chat",
        "fireworks/deepseek-coder-v2-lite-instruct|deepseek-coder-v2-lite|DeepSeek Coder V2 Lite Instruct|C|0.9000/0.9000|163K/16K|J|-/-/-|DeepSeek Coder V2 Lite Instruct is a 16-billion-parameter open-source Mixture-of|chat",
        "fireworks/deepseek-prover-v2|deepseek-prover-v2|DeepSeek Prover V2|C|0.9000/0.9000|163K/16K|J|-/-/-|DeepSeek-Prover-V2, an open-source large language model designed for formal theo|chat",
        "fireworks/deepseek-r1|deepseek-r1|DeepSeek R1 (Fast)|C|3.00/8.00|163K/16K|J|-/-/-|DeepSeek R1 (Fast) is the speed-optimized serverless deployment of DeepSeek-R1. |chat",
        "fireworks/deepseek-r1-0528|deepseek-r1-0528|Deepseek R1 05/28|C|3.00/8.00|163K/16K|TJ|-/-/-|05/28 updated checkpoint of Deepseek R1. Its overall performance is now approach|chat",
        "fireworks/deepseek-r1-0528-distill-qwen3-8b|deepseek-r1-0528-distill-qwen-3-8b|DeepSeek R1 0528 Distill Qwen3 8B|C|0.2000/0.2000|131K/16K|TJ|-/-/-|We distilled the chain-of-thought from DeepSeek-R1-0528 to post-train Qwen3 8B B|chat",
        "fireworks/deepseek-r1-basic|deepseek-r1|DeepSeek R1 (Basic)|C|3.00/8.00|163K/16K|J|-/-/-|DeepSeek R1 (Basic) is the cost-optimized serverless deployment of DeepSeek-R1. |chat",
        "fireworks/deepseek-r1-distill-llama-70b|deepseek-r1-distill-llama-70b|DeepSeek R1 Distill Llama 70B|C|0.9000/0.9000|131K/16K|J|-/-/-|Llama 70B distilled with reasoning from Deepseek R1|chat",
        "fireworks/deepseek-r1-distill-llama-8b|deepseek-r1-distill-llama-8b|DeepSeek R1 Distill Llama 8B|C|0.2000/0.2000|131K/16K|J|-/-/-|Llama 8B distilled with reasoning from Deepseek R1|chat",
        "fireworks/deepseek-r1-distill-qwen-14b|deepseek-r1-distill-qwen-14b|DeepSeek R1 Distill Qwen 14B|C|0.1000/0.1000|131K/16K|J|-/-/-|Qwen 14B distilled with reasoning from Deepseek R1|chat",
        "fireworks/deepseek-r1-distill-qwen-1p5b|deepseek-r1-distill-qwen-1p5b|DeepSeek R1 Distill Qwen 1.5B|C|3.00/8.00|131K/16K|J|-/-/-|Qwen 1.5B distilled with reasoning from Deepseek R1|chat",
        "fireworks/deepseek-r1-distill-qwen-32b|deepseek-r1-distill-qwen-32b|DeepSeek R1 Distill Qwen 32B|C|0.9000/0.9000|131K/16K|J|-/-/-|Qwen 32B distilled with reasoning from Deepseek R1|chat",
        "fireworks/deepseek-r1-distill-qwen-7b|deepseek-r1-distill-qwen-7b|DeepSeek R1 Distill Qwen 7B|C|0.2000/0.2000|131K/16K|J|-/-/-|Qwen 7B distilled with reasoning from Deepseek R1|chat",
        "fireworks/deepseek-v2-lite-chat|deepseek-v2-lite|DeepSeek V2 Lite Chat|C|0.9000/0.9000|163K/16K|J|-/-/-|DeepSeek-V2, a strong Mixture-of-Experts (MoE) language model characterized by e|chat",
        "fireworks/deepseek-v2p5|deepseek-v2p5|DeepSeek V2.5|C|0.9000/0.9000|32K/16K|J|-/-/-|DeepSeek-V2.5 is an upgraded version that combines DeepSeek-V2-Chat and DeepSeek|chat",
        "fireworks/deepseek-v3|deepseek-v3|DeepSeek V3|C|0.9000/0.9000|131K/16K|TJ|-/-/-|A a strong Mixture-of-Experts (MoE) language model with 671B total parameters wi|chat",
        "fireworks/deepseek-v3-0324|deepseek-v3-0324|Deepseek V3 03-24|C|0.9000/0.9000|163K/16K|TJ|-/-/-|A strong Mixture-of-Experts (MoE) language model with 671B total parameters with|chat",
        "fireworks/deepseek-v3p1|deepseek-v3p1|DeepSeek V3.1|C|0.9000/0.9000|163K/16K|TJ|-/-/-|DeepSeek-V3.1 is post-trained on the top of DeepSeek-V3.1-Base, which is built u|chat",
        "fireworks/deepseek-v3p1-terminus|deepseek-v3p1-terminus|DeepSeek V3.1 Terminus|C|0.9000/0.9000|163K/16K|TJ|-/-/-|DeepSeek-V3.1-Terminus is an updated version of DeepSeek-V3.1 with enhanced lang|chat",
        "fireworks/deepseek-v3p2|deepseek-v3p2|Deepseek v3.2|C|0.9000/0.9000|163K/16K|TJ|-/-/-|Model from Deepseek that harmonizes high computational efficiency with superior |chat",
        "fireworks/devstral-small-2-24b-instruct-2512|devstral-small-2-24b-2512|Devstral Small 2 24B Instruct 2512|C|0.1000/0.1000|4K/4K|VT|-/-/-|Devstral is an agentic LLM for software engineering tasks. Devstral Small 2 exce|chat",
        "fireworks/devstral-small-2505|devstral-small-2505|Devstral-Small-2505|C|0.2000/0.2000|131K/16K|J|-/-/-|Devstral is an agentic LLM for software engineering tasks built under a collabor|chat",
        "fireworks/dolphin-2-9-2-qwen2-72b|dolphin-2-9-2-qwen2-72b|Dolphin 2.9.2 Qwen2 72B|C|0.9000/0.9000|131K/16K|J|-/-/-|Dolphin 2.9.2 Qwen2 72B is a fine-tuned version of the Qwen2 72B Large Language |chat",
        "fireworks/dolphin-2p6-mixtral-8x7b|dolphin-2p6-mixtral-8x7b|Dolphin 2.6 Mixtral 8x7b|C|0.2000/0.2000|32K/16K|J|-/-/-|Dolphin 2.6 Mixtral 8x7b is a fine-tuned version of the Mixtral-8x7b Large Langu|chat",
        "fireworks/eagle1-kimi-k2-instruct-0905-v0|eagle1-kimi-k2-0905-v0|EAGLE1 kimi-k2-instruct-0905 v0|C|0.3500/1.40|4K/4K|-|-/-/-|EAGLE1 for kimi-k2-instruct-0905 v0|chat",
        "fireworks/eagle3-kimi-k2-instruct-0905-v0|eagle3-kimi-k2-0905-v0|EAGLE3 kimi-k2-instruct-0905 v0|C|0.3500/1.40|4K/4K|-|-/-/-|EAGLE3 for kimi-k2-instruct-0905 v0|chat",
        "fireworks/eagle-llama-v3-8b-instruct-v1|eagle-llama-v3-8b-v1|EAGLE Llama 3 8B V1|C|0.2000/0.2000|4K/4K|-|-/-/-|EAGLE draft model for Llama 3.x 8B instruct models|chat",
        "fireworks/eagle-llama-v3-8b-instruct-v2|eagle-llama-v3-8b-v2|EAGLE Llama 3 8B V2|C|0.2000/0.2000|4K/4K|-|-/-/-|EAGLE draft model for Llama 3.x 8B instruct models|chat",
        "fireworks/eagle-qwen-v2p5-3b-instruct-v2|eagle-qwen-v2p5-3b-v2|EAGLE Qwen 2.5 3B Instruct V2|C|0.1000/0.1000|4K/4K|-|-/-/-|EAGLE draft model for Qwen 2.5 3B instruct models|chat",
        "fireworks/ernie-4p5-21b-a3b-pt|ernie-4p5-21b-a3b-pt|ERNIE-4.5-21B-A3B-PT|C|0.1000/0.1000|131K/16K|J|-/-/-|ERNIE-4.5-21B-A3B is a text MoE Post-trained model, with 21B total parameters an|chat",
        "fireworks/ernie-4p5-300b-a47b-pt|ernie-4p5-300b-a47b-pt|ERNIE-4.5-300B-A47B-PT|C|0.2000/0.2000|131K/16K|J|-/-/-|ERNIE-4.5-300B-A47B is a text MoE Post-trained model, with 300B total parameters|chat",
        "fireworks/fare-20b|fare-20b|FARE-20B|C|0.2000/0.2000|131K/16K|J|-/-/-|FARE-20B is a multi-task evaluator fine-tuned from gpt-oss-20B. Its trained on |chat",
        "fireworks/firefunction-v1|firefunction-v1|FireFunction V1|C|0.2000/0.2000|32K/16K|T|-/-/-|Fireworks' open-source function calling model.|chat",
        "fireworks/firefunction-v2|firefunction-v2|FireFunction V2|C|0.2000/0.2000|4K/4K|T|-/-/-|Fireworks' latest and most performant function-calling model. Firefunction-v2 is|chat",
        "fireworks/firesearch-ocr-v6|firesearch-ocr-v6|Firesearch OCR V6|C|0.2000/0.2000|8K/8K|VJ|-/-/-|OCR model provided by Fireworks|chat",
        "fireworks/flux-1-dev-controlnet-union|flux-1-dev-controlnet-union|FLUX.1 [dev] ControlNet|C|0.2000/0.2000|4K/4K|-|-/-/-|Unified ControlNet for FLUX.1-dev model jointly released by researchers from Ins|chat",
        "fireworks/flux-1-dev-fp8|flux-1-dev-fp8|FLUX.1 [dev] FP8|C|0.0250/-|4K/-|I|-/-/-|FLUX.1 [dev] is a 12 billion parameter rectified flow transformer capable of gen|image",
        "fireworks/flux-1-schnell|flux-1-schnell|FLUX.1 [schnell]|C|0.0250/-|4K/-|I|-/-/-|FLUX.1 [schnell] is a 12 billion parameter rectified flow transformer capable of|image",
        "fireworks/flux-1-schnell-fp8|flux-1-schnell-fp8|FLUX.1 [schnell] FP8|C|0.0250/-|4K/-|I|-/-/-|FLUX.1 [schnell] is a 12 billion parameter rectified flow transformer capable of|image",
        "fireworks/flux-kontext-max|flux-kontext-max|Flux Kontext Max|C|0.0500/-|4K/-|I|-/-/-|FLUX Kontext Max is Black Forest Labs' new premium model that brings maximum per|image",
        "fireworks/flux-kontext-pro|flux-kontext-pro|Flux Kontext Pro|C|0.0500/-|4K/-|I|-/-/-|FLUX Kontext Pro is a specialized model for generating contextually-aware images|image",
        "fireworks/full-llama-v3p1-8b-instruct-8b-fp8|full-llama-1-8b-8b-fp8|Llama 3.1 8B Instruct FP8 [Full]|C|0.2000/0.2000|4K/4K|-|-/-/-|The Meta Llama 3.1 collection of multilingual large language models (LLMs) is a |chat",
        "fireworks/full-llama-v3p1-8b-instruct-8b-fp8-amd|full-llama-1-8b-8b-fp8-amd|Llama 3.1 8B Instruct FP8 AMD [Full]|C|0.2000/0.2000|4K/4K|-|-/-/-|The Meta Llama 3.1 collection of multilingual large language models (LLMs) is a |chat",
        "fireworks/gemma2-9b-it|gemma2-9b-it|Gemma 2 9B Instruct|C|0.2000/0.2000|8K/8K|J|-/-/-|Gemma is a family of lightweight, state-of-the-art open models from Google, buil|chat",
        "fireworks/gemma-2b-it|gemma-2b-it|Gemma 2B Instruct|C|0.2000/0.2000|8K/8K|J|-/-/-|Gemma is a family of lightweight, state-of-the-art open models from Google, buil|chat",
        "fireworks/gemma-3-12b-it|gemma-3-12b-it|Gemma 3 12B Instruct|C|0.2000/0.2000|131K/16K|J|-/-/-|Gemma is a family of lightweight, state-of-the-art open models from Google, buil|chat",
        "fireworks/gemma-3-27b-it|gemma-3-27b-it|Gemma 3 27B Instruct|C|0.2000/0.2000|131K/16K|J|-/-/-|Gemma 3 27B Instruct|chat",
        "fireworks/gemma-3-4b-it|gemma-3-4b-it|Gemma 3 4B Instruct|C|0.1000/0.1000|131K/16K|J|-/-/-|Gemma is a family of lightweight, state-of-the-art open models from Google, buil|chat",
        "fireworks/gemma-7b|gemma-7b|Gemma 7B|C|0.2000/0.2000|8K/8K|J|-/-/-|Gemma is a family of lightweight, state-of-the-art open models from Google, buil|chat",
        "fireworks/gemma-7b-it|gemma-7b-it|Gemma 7B Instruct|C|0.2000/0.2000|8K/8K|J|-/-/-|Gemma is a family of lightweight, state-of-the-art open models from Google, buil|chat",
        "fireworks/glm-4p5|glm-4p5|GLM-4.5|C|0.3500/0.3500|131K/16K|TJ|-/-/-|The GLM-4.5 series models are foundation models designed for intelligent agents.|chat",
        "fireworks/glm-4p5-air|glm-4p5-air|GLM-4.5-Air|C|0.3500/0.3500|131K/16K|TJ|-/-/-|The GLM-4.5 series models are foundation models designed for intelligent agents.|chat",
        "fireworks/glm-4p5v|glm-4p5v|GLM-4.5V|C|0.3500/0.3500|131K/16K|VTJ|-/-/-|GLM-4.5V is based on ZhipuAIs next-generation flagship text foundation model GL|chat",
        "fireworks/glm-4p6|glm-4p6|GLM-4.6|C|0.3500/0.3500|202K/16K|TJ|-/-/-|As the latest iteration in the GLM series, GLM-4.6 achieves comprehensive enhanc|chat",
        "fireworks/glm-4p7|glm-4p7|GLM-4.7|C|0.3500/0.3500|202K/16K|TJ|-/-/-|GLM-4.7 is a next-generation general-purpose model optimized for coding, reasoni|chat",
        "fireworks/gpt-oss-120b|gpt-oss-120b|OpenAI gpt-oss-120b|C|0.5000/0.5000|131K/16K|TJ|-/-/-|Welcome to the gpt-oss series, OpenAI's open-weight models designed for powerful|chat",
        "fireworks/gpt-oss-20b|gpt-oss-20b|OpenAI gpt-oss-20b|C|0.5000/0.5000|131K/16K|J|-/-/-|Welcome to the gpt-oss series, OpenAI's open-weight models designed for powerful|chat",
        "fireworks/gpt-oss-20b-eagle3-v1|gpt-oss-20b-eagle3-v1|gpt-oss-20b-drafter|C|0.5000/0.5000|4K/4K|-|-/-/-|gpt-oss-20b drafter|chat",
        "fireworks/gpt-oss-safeguard-120b|gpt-oss-safeguard-120b|OpenAI gpt-oss-safeguard-120b|C|0.5000/0.5000|131K/16K|TJ|-/-/-|gpt-oss-safeguard-120b is a safety-focused language model with 117B total parame|chat",
        "fireworks/gpt-oss-safeguard-20b|gpt-oss-safeguard-20b|OpenAI gpt-oss-safeguard-20b|C|0.5000/0.5000|131K/16K|TJ|-/-/-|gpt-oss-safeguard-20b is a safety-focused language model with 21B total paramete|chat",
        "fireworks/hermes-2-pro-mistral-7b|hermes-2-pro-mistral-7b|Hermes 2 Pro Mistral 7B|C|0.2000/0.2000|32K/16K|TJ|-/-/-|Latest version of Nous Research's Hermes series of models, using an updated and |chat",
        "fireworks/internvl3-38b|internvl3-38b|InternVL3 38B|C|0.2000/0.2000|16K/16K|VJ|-/-/-|The InternVL3 collection of models are advanced multimodal large language models|chat",
        "fireworks/internvl3-78b|internvl3-78b|InternVL3 78B|C|0.2000/0.2000|16K/16K|VJ|-/-/-|The InternVL3 collection of models are advanced multimodal large language models|chat",
        "fireworks/internvl3-8b|internvl3-8b|InternVL3 8B|C|0.2000/0.2000|16K/16K|VJ|-/-/-|The InternVL3 collection of models are advanced multimodal large language models|chat",
        "fireworks/kat-coder|kat-coder|KAT Coder|C|0.2000/0.2000|262K/16K|J|-/-/-|KAT-Coder-Pro V1 is KwaiKAT's most advanced agentic coding model in the KwaiKAT |chat",
        "fireworks/kat-dev-32b|kat-dev-32b|KAT Dev 32B|C|0.9000/0.9000|131K/16K|J|-/-/-|KAT-Dev-32B is an open-source 32B-parameter model for software engineering tasks|chat",
        "fireworks/kat-dev-72b-exp|kat-dev-72b-exp|KAT Dev 72B Exp|C|0.9000/0.9000|131K/16K|TJ|-/-/-|KAT-Dev-72B-Exp is an open-source 72B-parameter model for software engineering t|chat",
        "fireworks/kimi-k2-instruct|kimi-k2|Kimi K2 Instruct|C|0.3500/1.40|131K/16K|TJ|-/-/-|Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 bi|chat",
        "fireworks/kimi-k2-instruct-0905|kimi-k2-0905|Kimi K2 Instruct 0905|C|0.3500/1.40|262K/16K|TJ|-/-/-|Kimi K2 0905 is an updated version of Kimi K2, a state-of-the-art mixture-of-exp|chat",
        "fireworks/kimi-k2-thinking|kimi-k2-thinking|Kimi K2 Thinking|C|0.3500/1.40|4K/4K|TK|-/-/-|Kimi K2 Thinking is the latest, most capable version of open-source thinking mod|chat",
        "fireworks/llama4-maverick-instruct-basic|llama4-maverick|Llama 4 Maverick Instruct (Basic)|C|0.2200/0.8800|1048K/16K|VTJ|-/-/-|The Llama 4 collection of models are natively multimodal AI models that enable t|chat",
        "fireworks/llama4-scout-instruct-basic|llama4-scout|Llama 4 Scout Instruct (Basic)|C|0.1500/0.6000|1048K/16K|VTJ|-/-/-|The Llama 4 collection of models are natively multimodal AI models that enable t|chat",
        "fireworks/llama-guard-2-8b|llama-guard-2-8b|Llama Guard v2 8B|C|0.2000/0.2000|8K/8K|J|-/-/-|Meta Llama Guard 2 is an 8B parameter Llama 3-based LLM safeguard model. Similar|chat",
        "fireworks/llama-guard-3-1b|llama-guard-3-1b|Llama Guard v3 1B|C|0.2000/0.2000|131K/16K|J|-/-/-|Llama Guard 3-1B is a fine-tuned Llama-3.2-1B pretrained model for content safet|chat",
        "fireworks/llama-guard-3-8b|llama-guard-3-8b|Llama Guard 3 8B|C|0.2000/0.2000|131K/16K|J|-/-/-|Llama Guard 3 is a Llama-3.1-8B pretrained model, fine-tuned for content safety |chat",
        "fireworks/llamaguard-7b|llamaguard-7b|Llama Guard 7B|C|0.2000/0.2000|4K/4K|J|-/-/-|Llama-Guard is a 7B parameter Llama 2-based input-output safeguard model. It can|chat",
        "fireworks/llama-v2-13b|llama-v2-13b|Llama 2 13B|C|0.2000/0.2000|4K/4K|J|-/-/-|Llama 2 is a collection of pretrained and fine-tuned generative text models rang|chat",
        "fireworks/llama-v2-13b-chat|llama-v2-13b|Llama 2 13B Chat|C|0.2000/0.2000|4K/4K|J|-/-/-|Llama 2 is a collection of pretrained and fine-tuned generative text models rang|chat",
        "fireworks/llama-v2-70b|llama-v2-70b|Llama 2 70B|C|0.9000/0.9000|4K/4K|J|-/-/-|Meta developed and publicly released the Llama 2 family of large language models|chat",
        "fireworks/llama-v2-7b|llama-v2-7b|Llama 2 7B|C|0.2000/0.2000|4K/4K|J|-/-/-|Meta's Llama 2 model family is a collection of pretrained and fine-tuned generat|chat",
        "fireworks/llama-v2-7b-chat|llama-v2-7b|Llama 2 7B Chat|C|0.2000/0.2000|4K/4K|J|-/-/-|Llama 2 is a collection of pretrained and fine-tuned generative text models rang|chat",
        "fireworks/llama-v3-70b-instruct|llama-v3-70b|Llama 3 70B Instruct|C|0.9000/0.9000|8K/8K|J|-/-/-|Meta developed and released the Meta Llama 3 family of large language models (LL|chat",
        "fireworks/llama-v3-70b-instruct-hf|llama-v3-70b-hf|Llama 3 70B Instruct (HF version)|C|0.9000/0.9000|8K/8K|J|-/-/-|Metas Llama 3 instruction tuned models are optimized for dialogue use cases and|chat",
        "fireworks/llama-v3-70b-instruct-v2|llama-v3-70b-v2|Llama v3 70B Instruct V2 Draft Model|C|0.9000/0.9000|4K/4K|-|-/-/-|Meta developed and released the Meta Llama 3 family of large language models (LL|chat",
        "fireworks/llama-v3-8b|llama-v3-8b|Llama 3 8B|C|0.2000/0.2000|8K/8K|J|-/-/-|Llama 3 is an auto-regressive language model that uses an optimized transformer |chat",
        "fireworks/llama-v3-8b-instruct|llama-v3-8b|Llama 3 8B Instruct|C|0.2000/0.2000|8K/8K|J|-/-/-|Meta developed and released the Meta Llama 3 family of large language models (LL|chat",
        "fireworks/llama-v3-8b-instruct-hf|llama-v3-8b-hf|Llama 3 8B Instruct (HF version)|C|0.2000/0.2000|8K/8K|J|-/-/-|Meta's Llama 3 instruction tuned models are optimized for dialogue use cases and|chat",
        "fireworks/llama-v3-8b-instruct-v0|llama-v3-8b-v0|Llama v3 8B Instruct V0 Draft Model|C|0.2000/0.2000|4K/4K|-|-/-/-|Meta developed and released the Meta Llama 3 family of large language models (LL|chat",
        "fireworks/llama-v3p1-405b-instruct|llama-1-405b|Llama 3.1 405B Instruct|C|3.00/3.00|131K/16K|TJ|-/-/-|The Meta Llama 3.1 collection of multilingual large language models (LLMs) is a |chat",
        "fireworks/llama-v3p1-405b-instruct-long|llama-1-405b-long|Llama 3.1 405B Instruct Long|C|3.00/3.00|4K/4K|-|-/-/-|The Meta Llama 3.1 collection of multilingual large language models (LLMs) is a |chat",
        "fireworks/llama-v3p1-70b-instruct|llama-1-70b|Llama 3.1 70B Instruct|C|0.9000/0.9000|131K/16K|TJ|-/-/-|The Meta Llama 3.1 collection of multilingual large language models (LLMs) is a |chat",
        "fireworks/llama-v3p1-70b-instruct-1b|llama-1-70b-1b|Llama 3.1 70B Instruct 1B|C|0.9000/0.9000|4K/4K|-|-/-/-|The Meta Llama 3.1 collection of multilingual large language models (LLMs) is a |chat",
        "fireworks/llama-v3p1-8b-instruct|llama-1-8b|Llama 3.1 8B Instruct|C|0.2000/0.2000|131K/16K|J|-/-/-|The Meta Llama 3.1 collection of multilingual large language models (LLMs) is a |chat",
        "fireworks/llama-v3p1-nemotron-70b-instruct|llama-1-nemotron-70b|Llama 3.1 Nemotron 70B|C|0.9000/0.9000|131K/16K|J|-/-/-|Llama-3.1-Nemotron-70B-Instruct is a large language model customized by NVIDIA t|chat",
        "fireworks/llama-v3p2-11b-vision-instruct|llama-2-11b-vision|Llama 3.2 11B Vision Instruct|C|0.2000/0.2000|131K/16K|VJ|-/-/-|Instruction-tuned image reasoning model from Meta with 11B parameters. Optimized|chat",
        "fireworks/llama-v3p2-1b|llama-2-1b|Llama 3.2 1B|C|0.2000/0.2000|131K/16K|J|-/-/-|The Llama 3.2 collection of multilingual large language models (LLMs) is a colle|chat",
        "fireworks/llama-v3p2-1b-instruct|llama-2-1b|Llama 3.2 1B Instruct|C|0.2000/0.2000|131K/16K|J|-/-/-|The Llama 3.2 collection of multilingual large language models (LLMs) is a colle|chat",
        "fireworks/llama-v3p2-3b|llama-2-3b|Llama 3.2 3B|C|0.1000/0.1000|131K/16K|J|-/-/-|The Llama 3.2 collection of multilingual large language models (LLMs) is a colle|chat",
        "fireworks/llama-v3p2-3b-instruct|llama-2-3b|Llama 3.2 3B Instruct|C|0.1000/0.1000|131K/16K|J|-/-/-|The Llama 3.2 collection of multilingual large language models (LLMs) is a colle|chat",
        "fireworks/llama-v3p2-90b-vision-instruct|llama-2-90b-vision|Llama 3.2 90B Vision Instruct|C|0.2000/0.2000|131K/16K|VJ|-/-/-|Instruction-tuned image reasoning model with 90B parameters from Meta. Optimized|chat",
        "fireworks/llama-v3p3-70b-instruct|llama-3-70b|Llama 3.3 70B Instruct|C|0.9000/0.9000|131K/16K|J|-/-/-|Llama 3.3 70B Instruct is the December update of Llama 3.1 70B. The model improv|chat",
        "fireworks/minimax-m1-80k|minimax-m1-80k|MiniMax-M1-80k|C|0.2000/0.2000|4K/4K|-|-/-/-|We introduce MiniMax-M1, the world's first open-weight, large-scale hybrid-atten|chat",
        "fireworks/minimax-m2|minimax-m2|MiniMax-M2|C|0.2000/0.2000|196K/16K|TJ|-/-/-|**Preview:** This model is currently in preview. Full production support coming |chat",
        "fireworks/minimax-m2p1|minimax-m2p1|MiniMax-M2.1|C|0.2000/0.2000|204K/16K|TJ|-/-/-|MiniMax M2.1 is built for strong real-world performance across complex, multi-la|chat",
        "fireworks/ministral-3-14b-instruct-2512|ministral-3-14b-2512|Ministral 3 14B Instruct 2512|C|0.1000/0.1000|256K/16K|VTJ|-/-/-|Mistral's Ministral 3 14B dense model with vision encoder. The largest model in |chat",
        "fireworks/ministral-3-3b-instruct-2512|ministral-3-3b-2512|Ministral 3 3B Instruct 2512|C|0.1000/0.1000|256K/16K|VTJ|-/-/-|Mistral's Ministral 3 3B dense model with vision encoder. The smallest model in |chat",
        "fireworks/ministral-3-8b-instruct-2512|ministral-3-8b-2512|Ministral 3 8B Instruct 2512|C|0.2000/0.2000|256K/16K|VTJ|-/-/-|Mistral's Ministral 3 8B dense model with vision encoder. A balanced model in th|chat",
        "fireworks/mistral-7b|mistral-7b|Mistral 7B|C|0.2000/0.2000|32K/16K|J|-/-/-|The Mistral-7B-v0.1 Large Language Model (LLM) is a pretrained generative text m|chat",
        "fireworks/mistral-7b-instruct-4k|mistral-7b-4k|Mistal 7B Instruct V0.1|C|0.2000/0.2000|32K/16K|J|-/-/-|The Mistral-7B-Instruct-v0.1 Large Language Model (LLM) is a instruct fine-tuned|chat",
        "fireworks/mistral-7b-instruct-v0p2|mistral-7b-v0p2|Mistral 7B Instruct v0.2|C|0.2000/0.2000|32K/16K|J|-/-/-|Mistral 7B Instruct v0.2 is an instruction fine-tuned version of the Mistral 7B |chat",
        "fireworks/mistral-7b-instruct-v3|mistral-7b-v3|Mistral 7B Instruct v0.3|C|0.2000/0.2000|32K/16K|TJ|-/-/-|Mistral 7B Instruct v0.3 is an instruction fine-tuned version of the Mistral 7B |chat",
        "fireworks/mistral-7b-v0p2|mistral-7b-v0p2|Mistral 7B v0.2|C|0.2000/0.2000|32K/16K|J|-/-/-|The Mistral-7B-v0.2 Large Language Model (LLM) is the successor to the Mistral-7|chat",
        "fireworks/mistral-large-3-fp8|mistral-large-3-fp8|Mistral Large 3 675B Instruct 2512|C|0.2000/0.2000|256K/16K|VTJ|-/-/-|Mistral Large 3 is a state-of-the-art general-purpose Multimodal granular Mixtur|chat",
        "fireworks/mistral-nemo-base-2407|mistral-nemo-base-2407|Mistral Nemo Base 2407|C|0.2000/0.2000|128K/16K|J|-/-/-|The Mistral-Nemo-Base-2407 Large Language Model (LLM) is a pretrained generative|chat",
        "fireworks/mistral-nemo-instruct-2407|mistral-nemo-2407|Mistral Nemo Instruct 2407|C|0.2000/0.2000|128K/16K|J|-/-/-|The Mistral-Nemo-Instruct-2407 Large Language Model (LLM) is the instruction-tun|chat",
        "fireworks/mistral-small-24b-instruct-2501|mistral-small-24b-2501|Mistral Small 24B Instruct 2501|C|0.1000/0.1000|32K/16K|J|-/-/-|Mistral Small 3 ( 2501 ) sets a new benchmark in the "small" Large Language Mode|chat",
        "fireworks/mixtral-8x22b|mixtral-8x22b|Mixtral Moe 8x22B|C|1.20/1.20|65K/16K|J|-/-/-|The Mixtral MoE 8x22B v0.1 Large Language Model (LLM) is a pretrained generative|chat",
        "fireworks/mixtral-8x22b-instruct|mixtral-8x22b|Mixtral MoE 8x22B Instruct|C|1.20/1.20|65K/16K|TJ|-/-/-|Mixtral MoE 8x22B Instruct v0.1 is the instruction-tuned version of Mixtral MoE |chat",
        "fireworks/mixtral-8x7b|mixtral-8x7b|Mixtral 8x7B v0.1|C|0.2000/0.2000|32K/16K|J|-/-/-|Mixtral 8x7B v0.1 is a sparse mixture-of-experts (SMoE) large language model dev|chat",
        "fireworks/mixtral-8x7b-instruct|mixtral-8x7b|Mixtral MoE 8x7B Instruct|C|0.2000/0.2000|32K/16K|J|-/-/-|Mixtral MoE 8x7B Instruct is the instruction-tuned version of Mixtral MoE 8x7B a|chat",
        "fireworks/mixtral-8x7b-instruct-hf|mixtral-8x7b-hf|Mixtral MoE 8x7B Instruct (HF version)|C|0.2000/0.2000|32K/16K|J|-/-/-|Mixtral MoE 8x7B Instruct (HF Version) is the original, FP16 version of Mixtral |chat",
        "fireworks/mixtral-8x7b-instruct-v0-oss|mixtral-8x7b-v0-oss|Mixtral 8x7b Instruct V0 Draft Model|C|0.2000/0.2000|4K/4K|-|-/-/-|Mixtral MoE 8x7B Instruct is the instruction-tuned version of Mixtral MoE 8x7B a|chat",
        "fireworks/mythomax-l2-13b|mythomax-l2-13b|MythoMax L2 13B|C|0.2000/0.2000|4K/4K|J|-/-/-|An improved, potentially even perfected variant of MythoMix, a MythoLogic-L2 and|chat",
        "fireworks/nemotron-nano-3-30b-a3b|nemotron-nano-3-30b-a3b|NVIDIA Nemotron Nano 3 30B A3B|C|0.9000/0.9000|262K/16K|TJ|-/-/-|Nemotron-Nano-3-30B-A3B is a 
large language model trained by NVIDIA, designed 
|chat",
        "fireworks/nemotron-nano-v2-12b-vl|nemotron-nano-v2-12b-vl|NVIDIA Nemotron Nano 2 VL|C|0.2000/0.2000|131K/16K|VJ|-/-/-|NVIDIA Nemotron Nano 2 VL is an open 12B multimodal reasoning model for document|chat",
        "fireworks/nous-capybara-7b-v1p9|nous-capybara-7b-v1p9|Nous Capybara 7B V1.9|C|0.2000/0.2000|32K/16K|J|-/-/-|Nous-Capybara 7B V1.9 is a new model trained for multiple epochs on a dataset of|chat",
        "fireworks/nous-hermes-2-mixtral-8x7b-dpo|nous-hermes-2-mixtral-8x7b-dpo|Nouse Hermes 2 Mixtral 8x7B DPO|C|0.2000/0.2000|32K/16K|J|-/-/-|Nous Hermes 2 Mixtral 8x7B DPO is the new flagship Nous Research model trained o|chat",
        "fireworks/nous-hermes-llama2-13b|nous-hermes-llama2-13b|Nous Hermes Llama2 13B|C|0.2000/0.2000|4K/4K|J|-/-/-|Nous-Hermes-Llama2-13b is a state-of-the-art language model fine-tuned on over 3|chat",
        "fireworks/nous-hermes-llama2-70b|nous-hermes-llama2-70b|Nous Hermes Llama2 70B|C|0.9000/0.9000|4K/4K|J|-/-/-|Nous-Hermes-Llama2-70b is a state-of-the-art language model fine-tuned on over 3|chat",
        "fireworks/nous-hermes-llama2-7b|nous-hermes-llama2-7b|Nous Hermes Llama2 7B|C|0.2000/0.2000|4K/4K|J|-/-/-|Nous-Hermes-Llama2-7b is a state-of-the-art language model fine-tuned on over 30|chat",
        "fireworks/nvidia-nemotron-nano-12b-v2|nvidia-nemotron-nano-12b-v2|NVIDIA Nemotron Nano 12B v2|C|0.2000/0.2000|128K/16K|TJ|-/-/-|NVIDIA-Nemotron-Nano-12B-v2 is a large language model (LLM) trained from scratch|chat",
        "fireworks/nvidia-nemotron-nano-9b-v2|nvidia-nemotron-nano-9b-v2|NVIDIA Nemotron Nano 9B v2|C|0.2000/0.2000|128K/16K|TJ|-/-/-|NVIDIA-Nemotron-Nano-9B-v2 is a large language model (LLM) trained from scratch |chat",
        "fireworks/openchat-3p5-0106-7b|openchat-3p5-0106-7b|OpenChat 3.5 0106|C|0.2000/0.2000|8K/8K|J|-/-/-|OpenChat is an innovative library of open-source language models, fine-tuned wit|chat",
        "fireworks/openhermes-2-mistral-7b|openhermes-2-mistral-7b|OpenHermes 2 Mistral 7B|C|0.2000/0.2000|32K/16K|J|-/-/-|OpenHermes 2 Mistral 7B is a state of the art Mistral Fine-tune. OpenHermes was |chat",
        "fireworks/openhermes-2p5-mistral-7b|openhermes-2p5-mistral-7b|OpenHermes 2.5 Mistral 7B|C|0.2000/0.2000|32K/16K|J|-/-/-|OpenHermes 2.5 Mistral 7B is a state of the art Mistral Fine-tune, a continuatio|chat",
        "fireworks/openorca-7b|openorca-7b|Mistral 7B OpenOrca|C|0.2000/0.2000|32K/16K|J|-/-/-|A fine-tuned version of Mistral-7B trained on the OpenOrca dataset, based on the|chat",
        "fireworks/phi-3-mini-128k-instruct|phi-3-mini-128k|Phi-3 Mini 128k Instruct|C|0.2000/0.2000|131K/16K|J|-/-/-|Phi-3-Mini-128K-Instruct is a 3.8 billion-parameter, lightweight, state-of-the-a|chat",
        "fireworks/phi-3-vision-128k-instruct|phi-3-vision-128k|Phi-3.5 Vision Instruct|C|0.2000/0.2000|32K/16K|VJ|-/-/-|Phi-3-Vision-128K-Instruct is a lightweight, state-of-the-art open multimodal mo|chat",
        "fireworks/phi4-eagle|phi4-eagle|Phi-4 Eagle|C|0.2000/0.2000|4K/4K|-|-/-/-|EAGLE draft model for Phi4|chat",
        "fireworks/phind-code-llama-34b-python-v1|phind-code-llama-34b-python-v1|Phind CodeLlama 34B Python v1|C|0.1000/0.1000|16K/16K|J|-/-/-|Phind CodeLlama 34B Python V1 is a fine-tuned version of the CodeLlama 34B Pytho|chat",
        "fireworks/phind-code-llama-34b-v1|phind-code-llama-34b-v1|Phind CodeLlama 34B v1|C|0.1000/0.1000|16K/16K|J|-/-/-|Phind CodeLlama 34B V1 is a fine-tuned version of the Code-Llama 34B LLM using a|chat",
        "fireworks/phind-code-llama-34b-v2|phind-code-llama-34b-v2|Phind CodeLlama 34B v2|C|0.1000/0.1000|16K/16K|J|-/-/-|This model is fine-tuned from Phind-CodeLlama-34B-v1 and achieves 73.8% pass@1 o|chat",
        "fireworks/pythia-12b|pythia-12b|Pythia 12B|C|0.2000/0.2000|2K/2K|J|-/-/-|The Pythia model suite was deliberately designed to promote scientific research |chat",
        "fireworks/qwen1p5-72b-chat|qwen1p5-72b|Qwen1.5 72B Chat|C|0.9000/0.9000|32K/16K|J|-/-/-|Qwen1.5 is the beta version of Qwen2, a transformer-based decoder-only language |chat",
        "fireworks/qwen2-72b-instruct|qwen2-72b|Qwen2 72B Instruct|C|0.9000/0.9000|32K/16K|J|-/-/-|Qwen2 72B Instruct is a 72 billion parameter model developed by Alibaba for inst|chat",
        "fireworks/qwen2-7b-instruct|qwen2-7b|Qwen2 7B Instruct|C|0.2000/0.2000|32K/16K|J|-/-/-|Qwen2 7B Instruct is a 7-billion-parameter instruction-tuned language model deve|chat",
        "fireworks/qwen2p5-0p5b-instruct|qwen-2.5-0p5b|Qwen2.5 0.5B Instruct|C|0.2000/0.2000|32K/16K|J|-/-/-|Qwen2.5 are a series of decoder-only language models developed by Qwen team, Ali|chat",
        "fireworks/qwen2p5-14b|qwen-2.5-14b|Qwen2.5 14B|C|0.1000/0.1000|131K/16K|J|-/-/-|Qwen2.5 are a series of decoder-only language models developed by Qwen team, Ali|chat",
        "fireworks/qwen2p5-14b-instruct|qwen-2.5-14b|Qwen2.5 14B Instruct|C|0.1000/0.1000|32K/16K|J|-/-/-|Qwen2.5 are a series of decoder-only language models developed by Qwen team, Ali|chat",
        "fireworks/qwen2p5-1p5b-instruct|qwen-2.5-1p5b|Qwen2.5 1.5B Instruct|C|0.2000/0.2000|32K/16K|J|-/-/-|Qwen2.5 are a series of decoder-only language models developed by Qwen team, Ali|chat",
        "fireworks/qwen2p5-32b|qwen-2.5-32b|Qwen2.5 32B|C|0.9000/0.9000|131K/16K|J|-/-/-|Qwen2.5 are a series of decoder-only language models developed by Qwen team, Ali|chat",
        "fireworks/qwen2p5-32b-instruct|qwen-2.5-32b|Qwen2.5 32B Instruct|C|0.9000/0.9000|32K/16K|J|-/-/-|Qwen2.5 are a series of decoder-only language models developed by Qwen team, Ali|chat",
        "fireworks/qwen2p5-72b|qwen-2.5-72b|Qwen2.5 72B|C|0.9000/0.9000|131K/16K|J|-/-/-|Qwen2.5 are a series of decoder-only language models developed by Qwen team, Ali|chat",
        "fireworks/qwen2p5-72b-instruct|qwen-2.5-72b|Qwen2.5 72B Instruct|C|0.9000/0.9000|32K/16K|TJ|-/-/-|Qwen2.5 are a series of decoder-only language models developed by Qwen team, Ali|chat",
        "fireworks/qwen2p5-7b|qwen-2.5-7b|Qwen2.5 7B|C|0.2000/0.2000|131K/16K|J|-/-/-|Qwen2.5 are a series of decoder-only language models developed by Qwen team, Ali|chat",
        "fireworks/qwen2p5-7b-instruct|qwen-2.5-7b|Qwen2.5 7B Instruct|C|0.2000/0.2000|32K/16K|J|-/-/-|Qwen2.5 are a series of decoder-only language models developed by Qwen team, Ali|chat",
        "fireworks/qwen2p5-coder-0p5b|qwen-2.5-coder-0p5b|Qwen2.5-Coder 0.5B|C|0.2000/0.2000|32K/16K|J|-/-/-|Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (|chat",
        "fireworks/qwen2p5-coder-0p5b-instruct|qwen-2.5-coder-0p5b|Qwen2.5-Coder 0.5B Instruct|C|0.2000/0.2000|32K/16K|J|-/-/-|Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (|chat",
        "fireworks/qwen2p5-coder-14b|qwen-2.5-coder-14b|Qwen2.5-Coder 14B|C|0.1000/0.1000|32K/16K|J|-/-/-|Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (|chat",
        "fireworks/qwen2p5-coder-14b-instruct|qwen-2.5-coder-14b|Qwen2.5-Coder 14B Instruct|C|0.1000/0.1000|32K/16K|J|-/-/-|Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (|chat",
        "fireworks/qwen2p5-coder-1p5b|qwen-2.5-coder-1p5b|Qwen2.5-Coder 1.5B|C|0.2000/0.2000|32K/16K|J|-/-/-|Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (|chat",
        "fireworks/qwen2p5-coder-1p5b-instruct|qwen-2.5-coder-1p5b|Qwen2.5-Coder 1.5B Instruct|C|0.2000/0.2000|32K/16K|J|-/-/-|Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (|chat",
        "fireworks/qwen2p5-coder-32b|qwen-2.5-coder-32b|Qwen2.5-Coder 32B|C|0.9000/0.9000|32K/16K|J|-/-/-|Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (|chat",
        "fireworks/qwen2p5-coder-32b-instruct|qwen-2.5-coder-32b|Qwen2.5-Coder 32B Instruct|C|0.9000/0.9000|32K/16K|J|-/-/-|Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (|chat",
        "fireworks/qwen2p5-coder-32b-instruct-128k|qwen-2.5-coder-32b-128k|Qwen2.5-Coder 32B Instruct 128K|C|0.9000/0.9000|131K/16K|J|-/-/-|Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (|chat",
        "fireworks/qwen2p5-coder-32b-instruct-32k-rope|qwen-2.5-coder-32b-32k-rope|Qwen2.5-Coder 32B Instruct 32K RoPE|C|0.9000/0.9000|32K/16K|J|-/-/-|Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (|chat",
        "fireworks/qwen2p5-coder-32b-instruct-64k|qwen-2.5-coder-32b-64k|Qwen2.5-Coder 32B Instruct 64k|C|0.9000/0.9000|65K/16K|J|-/-/-|Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (|chat",
        "fireworks/qwen2p5-coder-3b|qwen-2.5-coder-3b|Qwen2.5-Coder 3B|C|0.1000/0.1000|32K/16K|J|-/-/-|Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (|chat",
        "fireworks/qwen2p5-coder-3b-instruct|qwen-2.5-coder-3b|Qwen2.5-Coder 3B Instruct|C|0.1000/0.1000|32K/16K|J|-/-/-|Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (|chat",
        "fireworks/qwen2p5-coder-7b|qwen-2.5-coder-7b|Qwen2.5-Coder 7B|C|0.2000/0.2000|32K/16K|J|-/-/-|Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (|chat",
        "fireworks/qwen2p5-coder-7b-instruct|qwen-2.5-coder-7b|Qwen2.5-Coder 7B Instruct|C|0.2000/0.2000|32K/16K|J|-/-/-|Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (|chat",
        "fireworks/qwen2p5-math-72b-instruct|qwen-2.5-math-72b|Qwen2.5-Math 72B Instruct|C|0.9000/0.9000|4K/4K|J|-/-/-|Qwen2.5-Math series is expanded to support using both CoT and Tool-integrated Re|chat",
        "fireworks/qwen2p5-vl-32b-instruct|qwen-2.5-vl-32b|Qwen2.5-VL 32B Instruct|C|0.9000/0.9000|128K/16K|VJ|-/-/-|Qwen2.5-VL is a multimodal large language model series developed by Qwen team, A|chat",
        "fireworks/qwen2p5-vl-3b-instruct|qwen-2.5-vl-3b|Qwen2.5-VL 3B Instruct|C|0.1000/0.1000|128K/16K|VJ|-/-/-|Qwen2.5-VL is a multimodal large language model series developed by Qwen team, A|chat",
        "fireworks/qwen2p5-vl-72b-instruct|qwen-2.5-vl-72b|Qwen2.5-VL 72B Instruct|C|0.9000/0.9000|128K/16K|VJ|-/-/-|Qwen2.5-VL is a multimodal large language model series developed by Qwen team, A|chat",
        "fireworks/qwen2p5-vl-7b-instruct|qwen-2.5-vl-7b|Qwen2.5-VL 7B Instruct|C|0.2000/0.2000|128K/16K|VJ|-/-/-|Qwen2.5-VL is a multimodal large language model series developed by Qwen team, A|chat",
        "fireworks/qwen2-vl-2b-instruct|qwen2-vl-2b|Qwen2-VL 2B Instruct|C|0.2000/0.2000|32K/16K|VJ|-/-/-|Qwen2-VL is a multimodal large language model series developed by Qwen team, Ali|chat",
        "fireworks/qwen2-vl-72b-instruct|qwen2-vl-72b|Qwen2-VL 72B Instruct|C|0.9000/0.9000|32K/16K|VJ|-/-/-|Qwen2-VL is a multimodal large language model series developed by Qwen team, Ali|chat",
        "fireworks/qwen2-vl-7b-instruct|qwen2-vl-7b|Qwen2-VL 7B Instruct|C|0.2000/0.2000|32K/16K|VJ|-/-/-|Qwen2-VL is a multimodal large language model series developed by Qwen team, Ali|chat",
        "fireworks/qwen3-0p6b|qwen-3-0p6b|Qwen3 0.6B|C|0.2000/0.2000|40K/16K|TJ|-/-/-|Qwen3 0.6B model developed by Qwen team, Alibaba Cloud,|chat",
        "fireworks/qwen3-14b|qwen-3-14b|Qwen3 14B|C|0.1000/0.1000|40K/16K|TJ|-/-/-|Qwen3 14B model developed by Qwen team, Alibaba Cloud,|chat",
        "fireworks/qwen3-1p7b|qwen-3-1p7b|Qwen3 1.7B|C|0.2000/0.2000|131K/16K|TJ|-/-/-|Qwen 1.7B Model developed by Qwen team, Alibaba Cloud,|chat",
        "fireworks/qwen3-1p7b-fp8-draft|qwen-3-1p7b-fp8-draft|Qwen3 1.7B fp8 model used for drafting|C|0.2000/0.2000|262K/16K|J|-/-/-|qwen 1.7b fp8 used as draft model|chat",
        "fireworks/qwen3-1p7b-fp8-draft-131072|qwen-3-1p7b-fp8-draft-131072|Qwen3 1.7B fp8 model used for drafting for 131072 context|C|0.2000/0.2000|131K/16K|J|-/-/-|qwen 1.7b fp8 used as draft model for 131072 context length|chat",
        "fireworks/qwen3-1p7b-fp8-draft-40960|qwen-3-1p7b-fp8-draft-40960|Qwen3 1.7B fp8 model used for drafting for 40960 context len|C|0.2000/0.2000|40K/16K|J|-/-/-|qwen 1.7b fp8 used as draft model for 40960 context length|chat",
        "fireworks/qwen3-235b-a22b|qwen-3-235b-a22b|Qwen3 235B A22B|C|0.9000/0.9000|131K/16K|TJ|-/-/-|Latest Qwen3 state of the art model, 235B with 22B active parameter model|chat",
        "fireworks/qwen3-235b-a22b-instruct-2507|qwen-3-235b-a22b-2507|Qwen3 235B A22B Instruct 2507|C|0.9000/0.9000|262K/16K|TJ|-/-/-|Updated FP8 version of Qwen3-235B-A22B non-thinking mode, with better tool use, |chat",
        "fireworks/qwen3-235b-a22b-thinking-2507|qwen-3-235b-a22b-thinking-2507|Qwen3 235B A22B Thinking 2507|C|0.9000/0.9000|262K/16K|JK|-/-/-|Latest Qwen3 thinking model, competitive against the best close source models in|chat",
        "fireworks/qwen3-30b-a3b|qwen-3-30b-a3b|Qwen3 30B-A3B|C|0.9000/0.9000|131K/16K|TJ|-/-/-|Latest Qwen3 state of the art model, 30B with 3B active parameter model|chat",
        "fireworks/qwen3-30b-a3b-instruct-2507|qwen-3-30b-a3b-2507|Qwen3 30B A3B Instruct 2507|C|0.9000/0.9000|262K/16K|J|-/-/-|Updated FP8 version of Qwen3-30B-A3B non-thinking mode, with better tool use, co|chat",
        "fireworks/qwen3-30b-a3b-thinking-2507|qwen-3-30b-a3b-thinking-2507|Qwen3 30B A3B Thinking 2507|C|0.9000/0.9000|262K/16K|TJK|-/-/-|Updated FP8 version of Qwen3-30B-A3B thinking mode, with better tool use, coding|chat",
        "fireworks/qwen3-32b|qwen-3-32b|Qwen3 32B|C|0.9000/0.9000|131K/16K|TJ|-/-/-|Latest Qwen3 state of the art model, 32B model|chat",
        "fireworks/qwen3-32b-eagle3-v2|qwen-3-32b-eagle3-v2|qwen3-32b-eagle3-drafter|C|0.9000/0.9000|4K/4K|-|-/-/-|qwen3 32b eagle3|chat",
        "fireworks/qwen3-4b|qwen-3-4b|Qwen3 4B|C|0.1000/0.1000|40K/16K|TJ|-/-/-|Latest Qwen3 state of the art model, 4B model|chat",
        "fireworks/qwen3-4b-instruct-2507|qwen-3-4b-2507|Qwen 3 4B Instruct 2507|C|0.1000/0.1000|262K/16K|J|-/-/-|Introducing Qwen3-4B-Instruct-2507, with improved instruction following, reasoni|chat",
        "fireworks/qwen3-8b|qwen-3-8b|Qwen3 8B|C|0.2000/0.2000|40K/16K|TJ|-/-/-|Latest Qwen3 state of the art model, FP8 version 8B Model|chat",
        "fireworks/qwen3-coder-30b-a3b-instruct|qwen-3-coder-30b-a3b|Qwen3 Coder 30B A3B Instruct|C|0.9000/0.9000|262K/16K|J|-/-/-|Latest Qwen3 coder model, 30B with 3B active parameter model|chat",
        "fireworks/qwen3-coder-480b-a35b-instruct|qwen-3-coder-480b-a35b|Qwen3 Coder 480B A35B Instruct|C|0.9000/0.9000|262K/16K|TJ|-/-/-|Qwen3's most agentic code model to date|chat",
        "fireworks/qwen3-coder-480b-instruct-bf16|qwen-3-coder-480b-bf16|Qwen3 Coder 480B Instruct BF16|C|0.9000/0.9000|262K/16K|J|-/-/-|The BF16 version of the 480B coder model|chat",
        "fireworks/qwen3-embedding-0p6b|qwen-3-embedding-0p6b|Qwen3 Embedding 0.6B|C|0.0080/-|32K/32K|E|-/-/-|significant advancements in multiple text embedding and ranking tasks, including|embed",
        "fireworks/qwen3-embedding-4b|qwen-3-embedding-4b|Qwen3 Embedding 4B|C|0.0080/-|40K/40K|E|-/-/-|significant advancements in multiple text embedding and ranking tasks, including|embed",
        "fireworks/qwen3-embedding-8b|qwen-3-embedding-8b|Qwen3 Embedding 8B|C|0.0080/-|40K/40K|E|-/-/-|The Qwen3 Embedding 8B model is the latest proprietary model of the Qwen family,|embed",
        "fireworks/qwen3-next-80b-a3b-instruct|qwen-3-next-80b-a3b|Qwen3 Next 80B A3B Instruct|C|0.1000/0.1000|4K/4K|-|-/-/-|Qwen3 Next 80B A3B Instruct is a state-of-the-art mixture-of-experts (MoE) langu|chat",
        "fireworks/qwen3-next-80b-a3b-thinking|qwen-3-next-80b-a3b-thinking|Qwen3 Next 80B A3B Thinking|C|0.1000/0.1000|4K/4K|K|-/-/-|Qwen3 Next 80B A3B Thinking is a state-of-the-art mixture-of-experts (MoE) langu|chat",
        "fireworks/qwen3-omni-30b-a3b-instruct|qwen-3-omni-30b-a3b|Qwen3 Omni 30B A3B Instruct|C|0.9000/0.9000|65K/16K|VTJ|-/-/-|Qwen3-Omni is a natively end-to-end multilingual omni-modal foundation model. It|chat",
        "fireworks/qwen3-reranker-0p6b|qwen-3-reranker-0p6b|Qwen3 Reranker 0.6B|C|0.0080/-|40K/40K|E|-/-/-|significant advancements in multiple text embedding and ranking tasks, including|embed",
        "fireworks/qwen3-reranker-4b|qwen-3-reranker-4b|Qwen3 Reranker 4B|C|0.0080/-|40K/40K|E|-/-/-|significant advancements in multiple text embedding and ranking tasks, including|embed",
        "fireworks/qwen3-reranker-8b|qwen-3-reranker-8b|Qwen3 Reranker 8B|C|0.0080/-|40K/40K|E|-/-/-|significant advancements in multiple text embedding and ranking tasks, including|embed",
        "fireworks/qwen3-vl-235b-a22b-instruct|qwen-3-vl-235b-a22b|Qwen3 VL 235B A22B Instruct|C|0.9000/0.9000|262K/16K|VTJ|-/-/-|Qwen3 VL 235B A22B Instruct is a state-of-the-art vision-language model with 22 |chat",
        "fireworks/qwen3-vl-235b-a22b-thinking|qwen-3-vl-235b-a22b-thinking|Qwen3 VL 235B A22B Thinking|C|0.9000/0.9000|262K/16K|VTJK|-/-/-|Qwen3 VL 235B A22B Thinking is a state-of-the-art vision-language model with 22 |chat",
        "fireworks/qwen3-vl-30b-a3b-instruct|qwen-3-vl-30b-a3b|Qwen3 VL 30B A3B Instruct|C|0.9000/0.9000|262K/16K|VTJ|-/-/-|Qwen3-VL series delivers superior text understanding & generation, deeper visual|chat",
        "fireworks/qwen3-vl-30b-a3b-thinking|qwen-3-vl-30b-a3b-thinking|Qwen3 VL 30B A3B Thinking|C|0.9000/0.9000|262K/16K|VTJK|-/-/-|Qwen3-VL series delivers superior text understanding & generation, deeper visual|chat",
        "fireworks/qwen3-vl-32b-instruct|qwen-3-vl-32b|Qwen3 VL 32B Instruct|C|0.9000/0.9000|4K/4K|V|-/-/-|The Qwen3-VL-32B-Instruct model is an advanced vision-language model that signif|chat",
        "fireworks/qwen3-vl-8b-instruct|qwen-3-vl-8b|Qwen3-VL-8B-Instruct|C|0.2000/0.2000|4K/4K|V|-/-/-|The Qwen3-VL-8B-Instruct model is an advanced vision-language model that signifi|chat",
        "fireworks/qwen-qwq-32b-preview|qwen-qwq-32b-preview|Qwen QWQ 32B Preview|C|0.9000/0.9000|32K/16K|JK|-/-/-|Qwen QwQ model focuses on advancing AI reasoning, and showcases the power of ope|chat",
        "fireworks/qwen-v2p5-14b-instruct|qwen-v2p5-14b|Qwen2.5 14B Instruct|C|0.1000/0.1000|32K/16K|J|-/-/-|Qwen2.5 are a series of decoder-only language models developed by Qwen team, Ali|chat",
        "fireworks/qwen-v2p5-7b|qwen-v2p5-7b|Qwen2.5 7B|C|0.2000/0.2000|131K/16K|J|-/-/-|Qwen2.5 are a series of decoder-only language models developed by Qwen team, Ali|chat",
        "fireworks/qwq-32b|qwq-32b|QWQ 32B|C|0.9000/0.9000|131K/16K|JK|-/-/-|Medium-sized reasoning model from Qwen.|chat",
        "fireworks/rolm-ocr|rolm-ocr|Rolm OCR|C|0.2000/0.2000|128K/16K|VJ|-/-/-|RolmOCR is an open-source document OCR model developed by Reducto AI as a drop-i|chat",
        "fireworks/seed-oss-36b-instruct|seed-oss-36b|Seed OSS 36B Instruct|C|0.2000/0.2000|524K/16K|TJ|-/-/-|Seed-OSS is a series of open-source large language models developed by ByteDance|chat",
        "fireworks/snorkel-mistral-7b-pairrm-dpo|snorkel-mistral-7b-pairrm-dpo|Snorkel Mistral PairRM DPO|C|0.2000/0.2000|32K/16K|J|-/-/-|A fine-tuned version of the Mistral-7B model developed by Snorkel using PairRM f|chat",
        "fireworks/toppy-m-7b|toppy-m-7b|Toppy M 7B|C|0.2000/0.2000|32K/16K|J|-/-/-|A wild 7B parameter model that merges several models using the new task_arithmet|chat",
        "fireworks/zephyr-7b-beta|zephyr-7b-beta|Zephyr 7B Beta|C|0.2000/0.2000|32K/16K|J|-/-/-|Zephyr is a series of language models that are trained to act as helpful assista|chat",

    // === FOLLOW (2 models) ===
        "follow/alpaca-7b-instruction|alpaca-7b-inst|alpaca|Stanford: Alpaca 7B Instruction|C|0.000000/0.000000|4K/2K|VSTJ|-/-/-|Alpaca 7B fine-tuned for instruction following|chat",
        "follow/vicuna-13b-instruction|vicuna-13b-inst|vicuna|LMSYS: Vicuna 13B Instruction|C|0.000000/0.000001|4K/2K|VSTJ|-/-/-|Vicuna 13B instruction-tuned for conversational AI|chat",

    // === GOOGLE (45 models) ===
        "google/gemini-3-flash-preview|gemini-3-flash-previ|Google: Gemini 3 Flash Preview|C|0.000000/0.000003|1048K/65K|JKSTV|-/-/-|Gemini 3 Flash Preview is a high speed, high value thinking model designed for a|chat",
        "google/gemini-3-pro-image-preview|gemini-3-pro-image-p|Google: Nano Banana Pro (Gemini 3 Pro Image Preview)|C|0.000002/0.000012|65K/32K|JKSV|-/-/-|Nano Banana Pro is Google's most advanced image-generation and editing model, bu|chat",
        "google/gemini-3-pro-preview|gemini-3-pro-preview|Google: Gemini 3 Pro Preview|C|0.000002/0.000012|1048K/65K|JKSTV|-/-/-|Gemini 3 Pro is Google's flagship frontier model for high-precision multimodal r|chat",
        "google/gemini-2.5-flash-image|gemini-2.5-flash-ima|Google: Gemini 2.5 Flash Image (Nano Banana)|C|0.000000/0.000003|32K/32K|JSV|-/-/-|Gemini 2.5 Flash Image, a.k.a. "Nano Banana," is now generally available. It is |chat",
        "google/gemini-2.5-flash-preview-09-2025|gemini-2.5-flash-pre|Google: Gemini 2.5 Flash Preview 09-2025|C|0.000000/0.000003|1048K/65K|JKSTV|-/-/-|Gemini 2.5 Flash Preview September 2025 Checkpoint is Google's state-of-the-art |chat",
        "google/gemini-2.5-flash-lite-preview-09-2025|gemini-2.5-flash-lit|Google: Gemini 2.5 Flash Lite Preview 09-2025|C|0.000000/0.000000|1048K/65K|JKSTV|-/-/-|Gemini 2.5 Flash-Lite is a lightweight reasoning model in the Gemini 2.5 family,|chat",
        "google/gemini-2.5-flash-image-preview|gemini-2.5-flash-ima|Google: Gemini 2.5 Flash Image Preview (Nano Banana)|C|0.000000/0.000003|32K/32K|JSV|-/-/-|Gemini 2.5 Flash Image Preview, a.k.a. "Nano Banana," is a state of the art imag|chat",
        "google/gemini-2.5-flash-lite|gemini-2.5-flash-lit|Google: Gemini 2.5 Flash Lite|C|0.000000/0.000000|1048K/65K|JKSTV|-/-/-|Gemini 2.5 Flash-Lite is a lightweight reasoning model in the Gemini 2.5 family,|chat",
        "google/gemma-3n-e2b-it:free|gemma-3n-e2b-it:free|Google: Gemma 3n 2B (free)|C|-/-|8K/2K|J|-/-/-|Gemma 3n E2B IT is a multimodal, instruction-tuned model developed by Google Dee|chat",
        "google/gemini-2.5-flash|gemini-2.5-flash|Google: Gemini 2.5 Flash|C|0.000000/0.000003|1048K/65K|JKSTV|-/-/-|Gemini 2.5 Flash is Google's state-of-the-art workhorse model, specifically desi|chat",
        "google/gemini-2.5-pro|gemini-2.5-pro|Google: Gemini 2.5 Pro|C|0.000001/0.000010|1048K/65K|JKSTV|-/-/-|Gemini 2.5 Pro is Google's state-of-the-art AI model designed for advanced reaso|chat",
        "google/gemini-2.5-pro-preview|gemini-2.5-pro-previ|Google: Gemini 2.5 Pro Preview 06-05|C|0.000001/0.000010|1048K/65K|JKSTV|-/-/-|Gemini 2.5 Pro is Google's state-of-the-art AI model designed for advanced reaso|chat",
        "google/gemma-3n-e4b-it:free|gemma-3n-e4b-it:free|Google: Gemma 3n 4B (free)|C|-/-|8K/2K|J|-/-/-|Gemma 3n E4B-it is optimized for efficient execution on mobile and low-resource |chat",
        "google/gemma-3n-e4b-it|gemma-3n-e4b-it|Google: Gemma 3n 4B|C|0.000000/0.000000|32K/8K|-|-/-/-|Gemma 3n E4B-it is optimized for efficient execution on mobile and low-resource |chat",
        "google/gemini-2.5-pro-preview-05-06|gemini-2.5-pro-previ|Google: Gemini 2.5 Pro Preview 05-06|C|0.000001/0.000010|1048K/65K|JKSTV|-/-/-|Gemini 2.5 Pro is Google's state-of-the-art AI model designed for advanced reaso|chat",
        "google/gemma-3-4b-it:free|gemma-3-4b-it:free|Google: Gemma 3 4B (free)|C|-/-|32K/8K|JSV|-/-/-|Gemma 3 introduces multimodality, supporting vision-language input and text outp|chat",
        "google/gemma-3-4b-it|gemma-3-4b-it|Google: Gemma 3 4B|C|0.000000/0.000000|96K/24K|JV|-/-/-|Gemma 3 introduces multimodality, supporting vision-language input and text outp|chat",
        "google/gemma-3-12b-it:free|gemma-3-12b-it:free|Google: Gemma 3 12B (free)|C|-/-|32K/8K|V|-/-/-|Gemma 3 introduces multimodality, supporting vision-language input and text outp|chat",
        "google/gemma-3-12b-it|gemma-3-12b-it|Google: Gemma 3 12B|C|0.000000/0.000000|131K/131K|JSV|-/-/-|Gemma 3 introduces multimodality, supporting vision-language input and text outp|chat",
        "google/gemma-3-27b-it:free|gemma-3-27b-it:free|Google: Gemma 3 27B (free)|C|-/-|131K/32K|JSTV|-/-/-|Gemma 3 introduces multimodality, supporting vision-language input and text outp|chat",
        "google/gemma-3-27b-it|gemma-3-27b-it|Google: Gemma 3 27B|C|0.000000/0.000000|131K/32K|JSTV|-/-/-|Gemma 3 introduces multimodality, supporting vision-language input and text outp|chat",
        "google/gemini-2.0-flash-lite-001|gemini-2.0-flash-lit|Google: Gemini 2.0 Flash Lite|C|0.000000/0.000000|1048K/8K|JSTV|-/-/-|Gemini 2.0 Flash Lite offers a significantly faster time to first token (TTFT) c|chat",
        "google/gemini-2.0-flash-001|gemini-2.0-flash-001|Google: Gemini 2.0 Flash|C|0.000000/0.000000|1048K/8K|JSTV|-/-/-|Gemini Flash 2.0 offers a significantly faster time to first token (TTFT) compar|chat",
        "google/gemini-2.0-flash-exp:free|gemini-2.0-flash-exp|Google: Gemini 2.0 Flash Experimental (free)|C|-/-|1048K/8K|JTV|-/-/-|Gemini Flash 2.0 offers a significantly faster time to first token (TTFT) compar|chat",
        "google/gemma-2-27b-it|gemma-2-27b-it|Google: Gemma 2 27B|C|0.000001/0.000001|8K/2K|JS|-/-/-|Gemma 2 27B by Google is an open model built from the same research and technolo|chat",
        "google/gemma-2-9b-it|gemma-2-9b-it|Google: Gemma 2 9B|C|0.000000/0.000000|8K/2K|-|-/-/-|Gemma 2 9B by Google is an advanced, open-source language model that sets a new |chat",
        "google/gemini-2.5-pro-preview-06-05|gemini-2.5-pro|Gemini 2.5 Pro|C|1.25/10.00/0.3100|1048K/65K|VTJSK|-/-/-|Latest flagship model with thinking, 1M context|chat",
        "google/gemini-2.5-flash-preview-05-20|gemini-2.5-flash|Gemini 2.5 Flash|C|0.1500/0.6000/0.0375|1048K/65K|VTJSK|-/-/-|Fast thinking model with 1M context|chat",
        "google/gemini-2.0-flash|gemini-2.0-flash|Gemini 2.0 Flash|C|0.1000/0.4000/0.0250|1048K/8K|VTJS|-/-/-|Fast multimodal model with tool use|chat",
        "google/gemini-2.0-flash-lite|gemini-2.0-flash-lite|Gemini 2.0 Flash Lite|C|0.0750/0.3000|1048K/8K|VTJ|-/-/-|Lightweight and cost-effective Flash variant|chat",
        "google/gemini-1.5-pro|gemini-1.5-pro|Gemini 1.5 Pro|C|1.25/5.00/0.3100|2097K/8K|VTJSC|-/-/-|2M context for complex reasoning and analysis|chat",
        "google/gemini-1.5-flash|gemini-1.5-flash|Gemini 1.5 Flash|C|0.0750/0.3000/0.0187|1048K/8K|VTJSC|-/-/-|Fast and efficient with 1M context|chat",
        "google/gemini-1.5-flash-8b|gemini-1.5-flash-8b|Gemini 1.5 Flash 8B|C|0.0375/0.1500/0.0100|1048K/8K|VTJS|-/-/-|Smallest Flash variant, highly efficient|chat",
        "google/gemini-1.0-pro|gemini-1.0-pro|Gemini 1.0 Pro|C|0.5000/1.50|32K/8K|TJ|-/-/-|Original Gemini model, legacy support|chat",
        "google/text-embedding-004|text-embedding-004|Text Embedding 004|C|0.000010/-|2K/768|E|-/-/-|Text embeddings, 768 dimensions|embed",
        "google/text-multilingual-embedding-002|multilingual-embed|Multilingual Embedding 002|C|0.000010/-|2K/768|E|-/-/-|Multilingual text embeddings|embed",
        "google/imagen-3.0-generate-002|imagen-3|Imagen 3|C|0.0400/-|480/-|I|-/-/-|Image generation, $0.04/image|image",
        "google/imagen-3.0-fast-generate-001|imagen-3-fast|Imagen 3 Fast|C|0.0200/-|480/-|I|-/-/-|Fast image generation, $0.02/image|image",
        "google/veo-2.0-generate-001|veo-2|Veo 2|C|0.3500/-|480/-|D|-/-/-|Video generation, $0.35/second|video",
        "google/gemini-3-pro-20260101|gemini-3-pro|Gemini 3 Pro|C|0.000003/0.000009|1000K/8K|SVTJC|-/-/-|Latest Google frontier model|chat",
        "google/gemini-3-flash-20260101|gemini-3-flash|Gemini 3 Flash|C|0.000001/0.000003|1000K/8K|SVTJC|-/-/-|Fast generation model from Google|chat",
        "google/gemini-1.5-pro-finetuned-rag|gemini-pro-rag|gemini-rag-ft|Google: Gemini 1.5 Pro RAG FT|C|1.25/5.00|1000K/8K|VSTJK|-/-/-|Gemini Pro fine-tuned for RAG and retrieval|chat",
        "google/gemini-1.5-pro-finetuned-translation|gemini-pro-trans|gemini-trans-ft|Google: Gemini 1.5 Pro Translation FT|C|1.25/5.00|1000K/8K|VSTJK|-/-/-|Gemini Pro fine-tuned for multilingual translation|chat",
        "google/gemini-2.5-vision|gemini-2.5-vision|Google: Gemini 2.5 Vision|C|0.000004/0.000015|1000K/8K|VSTJK|-/-/-|Gemini 2.5 with advanced visual understanding and reasoning|chat",
        "google/gemini-1.5-vision|gemini-1.5-vision|Google: Gemini 1.5 Vision|L|0.000003/0.000008|1000K/4K|VSTJ|-/-/-|Previous-generation Gemini vision model|chat",

    // === GOVERNMENT (2 models) ===
        "government/compliance-bot|compliance-bot|gov-compliance|ComplianceBot: Regulations|C|0.000001/0.000004|8K/2K|VSTJ|-/-/-|Regulatory compliance and policy analysis|chat",
        "government/citizen-services|citizensvc|gov-citizen|CitizenServices LLM|C|0.000001/0.000003|8K/2K|VSTJ|-/-/-|Public sector citizen service improvements|chat",

    // === GRAPH (1 models) ===
        "graph/graphormer-base|graphormer|graph-transformer|Meta: Graphormer|C|0.000000/0.000000|512/256|S|-/-/-|Transformer for graph-structured data|chat",

    // === GROQ (33 models) ===
        "groq/llama-4-scout-17b-16e-instruct|llama-4-scout|Llama 4 Scout 17B|C|0.1100/0.3400|131K/8K|VTJS|-/-/-|Meta's Llama 4 Scout 17B on Groq hardware|chat",
        "groq/meta-llama/llama-4-maverick-17b-128e-instruct|llama-4-maverick|Llama 4 Maverick 17B|C|0.2000/0.6000|131K/8K|VTJS|-/-/-|Meta's Llama 4 Maverick 17B on Groq|chat",
        "groq/llama-3.3-70b-versatile|llama-3.3-70b|Llama 3.3 70B|C|0.5900/0.7900|131K/32K|TJS|-/-/-|Meta's Llama 3.3 70B, ultra-fast inference|chat",
        "groq/llama-3.3-70b-specdec|llama-3.3-70b-spec|Llama 3.3 70B SpecDec|C|0.5900/0.9900|8K/8K|TJS|-/-/-|Llama 3.3 70B with speculative decoding|chat",
        "groq/llama-3.2-90b-vision-preview|llama-3.2-90b-vision|Llama 3.2 90B Vision|C|0.9000/0.9000|131K/8K|VTJ|-/-/-|Vision-enabled Llama 3.2 90B|chat",
        "groq/llama-3.2-11b-vision-preview|llama-3.2-11b-vision|Llama 3.2 11B Vision|C|0.1800/0.1800|131K/8K|VTJ|-/-/-|Compact vision model|chat",
        "groq/llama-3.2-3b-preview|llama-3.2-3b|Llama 3.2 3B|C|0.0600/0.0600|131K/8K|TJ|-/-/-|Smallest Llama 3.2 model|chat",
        "groq/llama-3.2-1b-preview|llama-3.2-1b|Llama 3.2 1B|C|0.0400/0.0400|131K/8K|TJ|-/-/-|Tiny Llama model for edge devices|chat",
        "groq/llama-3.1-70b-versatile|llama-3.1-70b|Llama 3.1 70B|C|0.5900/0.7900|131K/8K|TJS|-/-/-|Llama 3.1 70B on Groq hardware|chat",
        "groq/llama-3.1-8b-instant|llama-3.1-8b|Llama 3.1 8B|C|0.0500/0.0800|131K/8K|TJS|-/-/-|Fast and efficient Llama 3.1 8B|chat",
        "groq/mixtral-8x7b-32768|mixtral-8x7b|Mixtral 8x7B|C|0.2400/0.2400|32K/8K|TJ|-/-/-|Mistral's MoE model on Groq|chat",
        "groq/gemma2-9b-it|gemma2-9b|Gemma 2 9B|C|0.2000/0.2000|8K/8K|TJ|-/-/-|Google's Gemma 2 9B on Groq|chat",
        "groq/deepseek-r1-distill-llama-70b|deepseek-r1-70b|DeepSeek R1 Distill 70B|C|0.7500/0.9900|131K/16K|TJK|-/-/-|DeepSeek R1 distilled to Llama 70B|chat",
        "groq/deepseek-r1-distill-qwen-32b|deepseek-r1-32b|DeepSeek R1 Distill 32B|C|0.6900/0.6900|131K/16K|TJK|-/-/-|DeepSeek R1 distilled to Qwen 32B|chat",
        "groq/qwen-qwq-32b|qwq-32b|Qwen QWQ 32B|C|0.2900/0.3900|131K/16K|TJK|-/-/-|Alibaba's Qwen QWQ reasoning model|chat",
        "groq/whisper-large-v3|whisper-v3|Whisper Large v3|C|0.1110/-|-/-|A|-/-/-|Speech-to-text, $0.111/hour|audio",
        "groq/whisper-large-v3-turbo|whisper-v3-turbo|Whisper Large v3 Turbo|C|0.0400/-|-/-|A|-/-/-|Fast speech-to-text, $0.04/hour|audio",
        "groq/distil-whisper-large-v3-en|distil-whisper|Distil Whisper Large v3|C|0.0200/-|-/-|A|-/-/-|Efficient English speech-to-text, $0.02/hour|audio",
        "groq/openai/gpt-oss-120b|gpt-oss-120b|GPT-OSS 120B|C|0.5900/0.7900|131K/16K|TJS|-/-/-|OpenAI open-weight 120B MoE model|chat",
        "groq/openai/gpt-oss-20b|gpt-oss-20b|GPT-OSS 20B|C|0.4000/0.4000|131K/16K|TJS|-/-/-|OpenAI open-weight 20B model|chat",
        "groq/openai/gpt-oss-safeguard-20b|gpt-oss-safeguard|GPT-OSS Safeguard 20B|C|0.4000/0.4000|131K/16K|TJS|-/-/-|Safety-focused 20B model|chat",
        "groq/qwen/qwen3-32b|qwen3-32b|Qwen3 32B|C|0.2900/0.5900|131K/16K|TJS|-/-/-|Alibaba Qwen3 32B on Groq|chat",
        "groq/moonshotai/kimi-k2-instruct-0905|kimi-k2|Kimi K2 Instruct|C|0.3500/1.40|262K/16K|TJS|-/-/-|Moonshot Kimi K2 on Groq|chat",
        "groq/meta-llama/llama-guard-4-12b|llama-guard-4|Llama Guard 4 12B|C|0.2000/0.2000|131K/8K|M|-/-/-|Content moderation model|chat",
        "groq/groq/compound|compound|Groq Compound|C|0/0|131K/8K|TJS|-/-/-|Compound AI with built-in tools|chat",
        "groq/groq/compound-mini|compound-mini|Groq Compound Mini|C|0/0|131K/8K|TJS|-/-/-|Lightweight compound AI|chat",
        "groq/mixtral-8x7b-instruct|mixtral-8x7b-instruct|mixtral-8x7b-inst|Groq: Mixtral 8x7b Instruct|C|0.000000/0.000001|32K/4K|JT|-/-/-|Instruction-tuned Mixtral 8x7b for Groq LPU - optimized for chat and instruction|chat",
        "groq/mixtral-8x22b-32768|mixtral-8x22b|mixtral-8x22b-groq|Groq: Mixtral 8x22b|C|0.000001/0.000002|32K/4K|JT|-/-/-|Larger Mixtral 8x22b with increased model depth - exceptional performance on Gro|chat",
        "groq/mixtral-8x22b-instruct|mixtral-8x22b-instruct|mixtral-8x22b-inst|Groq: Mixtral 8x22b Instruct|C|0.000001/0.000002|32K/4K|JT|-/-/-|Instruction-tuned Mixtral 8x22b - optimal for complex reasoning on Groq LPU|chat",
        "groq/llama2-70b-4096|llama-70b|llama2-70b-groq|Groq: Llama 2 70B|C|0.000001/0.000002|4K/2K|JT|-/-/-|Meta Llama 2 70B parameter model optimized for Groq LPU fast inference|chat",
        "groq/llama2-70b-chat|llama-70b-chat|llama2-70b-chat-groq|Groq: Llama 2 70B Chat|C|0.000001/0.000002|4K/2K|JT|-/-/-|Instruction and chat-optimized Llama 2 70B - excellent for conversational AI on |chat",
        "groq/gemma-7b-it|gemma-7b|gemma-7b-it-groq|Groq: Gemma 7B Instruct|C|0.000000/0.000000|8K/2K|JT|-/-/-|Google Gemma 7B instruction-tuned model - lightweight and fast on Groq LPU|chat",
        "groq/t5-base|t5-base|t5-base-groq|Groq: T5 Base|C|0.000000/0.000000|512/768|JT|-/-/-|Google T5 base text-to-text transfer transformer - lightweight sequence tasks|chat",

    // === GRYPHE (1 models) ===
        "gryphe/mythomax-l2-13b|mythomax-l2-13b|MythoMax 13B|C|0.000000/0.000000|4K/1K|JS|-/-/-|One of the highest performing and most popular fine-tunes of Llama 2 13B, with r|chat",

    // === HEALTHCARE (4 models) ===
        "healthcare/biogpt|biogpt|healthcare-bio|Microsoft: BioGPT|C|0.000000/0.000001|4K/2K|VSTJ|-/-/-|BioGPT for biomedical literature understanding|chat",
        "healthcare/scigpt|scigpt|healthcare-sci|SciGPT: Scientific|C|0.000000/0.000001|4K/2K|VSTJ|-/-/-|Scientific language model for healthcare research|chat",
        "healthcare/med-gemini|med-gemini|healthcare-med|Google: Med-Gemini|C|0.000003/0.000008|1000K/8K|VSTJ|-/-/-|Gemini variant specialized for medical applications|chat",
        "healthcare/clinical-llama|clinical-llama|healthcare-clinical|Meta: Clinical Llama|C|0.000001/0.000003|8K/2K|VSTJ|-/-/-|Llama variant fine-tuned for clinical notes|chat",

    // === HF (2 models) ===
        "hf/meta-llama/Llama-2-70b-chat|llama-2-70b-hf|Meta: Llama 2 70B Chat (HF)|C|0.0010/0.0020|4K/2K|VSTJ|-/-/-|Llama 2 70B Chat via HF Inference|chat",
        "hf/mistralai/Mistral-7B-Instruct-v0.2|mistral-7b-v2-hf|Mistral: 7B Instruct v0.2 (HF)|C|0.000050/0.000150|32K/2K|VSTJ|-/-/-|Mistral 7B v0.2 via HF Inference|chat",

    // === HOSPITALITY (2 models) ===
        "hospitality/bookingbot|bookingbot|hospitality-book|BookingBot: Reservations|C|0.000001/0.000002|8K/2K|VSTJ|-/-/-|Hotel and travel booking optimization|chat",
        "hospitality/concierge|concierge|hospitality-concierge|ConciergeGPT: Service|C|0.000001/0.000003|8K/2K|VSTJ|-/-/-|Personalized concierge and travel recommendations|chat",

    // === HR (3 models) ===
        "hr/recruiterbot|recruiterbot|hr-recruit|RecruiterBot: Hiring|C|0.000001/0.000002|8K/2K|VSTJ|-/-/-|Resume screening and candidate ranking|chat",
        "hr/hranalytics|hranalytics|hr-analytics|HRAnalytics: Insights|C|0.000001/0.000002|8K/2K|VSTJ|-/-/-|HR analytics and employee engagement prediction|chat",
        "hr/traininggpt|traininggpt|hr-training|TrainingGPT: Learning|C|0.000001/0.000003|8K/2K|VSTJ|-/-/-|Personalized training and skill development|chat",

    // === HUGGINGFACE (67 models) ===
        "huggingface/nous-hermes-3-70b|nous-hermes-3-70b|Nous: Hermes 3 70B|C|0.000001/0.000002|4K/2K|JT|-/-/-|Nous Research 70B instruct model with strong reasoning capabilities|chat",
        "huggingface/neural-chat-7b|neural-chat-7b|Intel: Neural Chat 7B|C|0.000000/0.000000|8K/2K|JT|-/-/-|Lightweight Intel chat model optimized for edge inference|chat",
        "huggingface/tinyllama-1.1b|tinyllama-1.1b|tinyllama|TinyLlama: 1.1B|C|0.000000/0.000000|2K/512|J|-/-/-|Ultra-lightweight 1.1B model for resource-constrained devices|chat",
        "huggingface/orca-2-13b|orca-2-13b|Microsoft: Orca 2 13B|C|0.000000/0.000001|4K/2K|JT|-/-/-|Microsoft Orca 2 for complex reasoning and instruction following|chat",
        "huggingface/orca-2-7b|orca-2-7b|orca-mini|Microsoft: Orca 2 7B|C|0.000000/0.000000|4K/2K|JT|-/-/-|Compact Orca 2 variant for efficient instruction understanding|chat",
        "huggingface/openhermes-2.5-mistral-7b|openhermes-2.5|OpenHermes: Mistral 7B|C|0.000000/0.000000|8K/2K|JT|-/-/-|OpenHermes variant of Mistral optimized for multi-turn conversations|chat",
        "huggingface/starling-7b|starling-7b|starling-lm|Starling: 7B|C|0.000000/0.000000|4K/2K|JT|-/-/-|Open LM foundation model with strong chat capabilities|chat",
        "huggingface/solar-10.7b|solar-10.7b|solar|Upstage: Solar 10.7B|C|0.000000/0.000000|4K/2K|JT|-/-/-|Upstage Solar base model with efficient inference|chat",
        "huggingface/openchat-3.5-0106|openchat-3.5|openchat|OpenChat: 3.5|C|0.000000/0.000000|8K/2K|JT|-/-/-|Community-driven chat model optimized for instruction following|chat",
        "huggingface/zephyr-7b-beta|zephyr-7b|zephyr|HuggingFace: Zephyr 7B|C|0.000000/0.000000|4K/2K|JT|-/-/-|Community chat model from HuggingFace with strong performance|chat",
        "huggingface/neural-7b-chat|neural-7b|Intel: Neural 7B Chat|C|0.000000/0.000000|4K/2K|JT|-/-/-|Intel Neural 7B optimized for conversational AI|chat",
        "huggingface/stablelm-3b|stablelm-3b|stable-3b|Stability: StableLM 3B|C|0.000000/0.000000|4K/1K|J|-/-/-|Lightweight StableLM model for mobile and edge devices|chat",
        "huggingface/stablelm-base-alpha-7b|stablelm-7b|stablelm-base|Stability: StableLM 7B Alpha|C|0.000000/0.000000|4K/2K|JT|-/-/-|Base StableLM 7B for instruct-tuning|chat",
        "huggingface/wizard-lm-1.3b|wizard-lm-1.3b|wizard-mini|WizardLM: 1.3B|C|0.000000/0.000000|2K/512|J|-/-/-|Lightweight WizardLM variant for instruction following|chat",
        "huggingface/wizard-lm-13b|wizard-lm-13b|wizard|WizardLM: 13B|C|0.000000/0.000001|4K/2K|JT|-/-/-|WizardLM 13B optimized for instruction following|chat",
        "huggingface/mpt-7b-instruct|mpt-7b-instruct|mpt-7b|MosaicML: MPT 7B Instruct|C|0.000000/0.000000|8K/2K|JT|-/-/-|MosaicML open foundation model with 8K context|chat",
        "huggingface/mpt-30b-instruct|mpt-30b|mpt-30b-instruct|MosaicML: MPT 30B Instruct|C|0.000000/0.000001|8K/4K|JT|-/-/-|Larger MosaicML model for complex tasks|chat",
        "huggingface/llama-2-7b-hf|llama2-7b|llama2-7b-hf|Meta: Llama 2 7B HF|C|0.000000/0.000000|4K/2K|JT|-/-/-|Llama 2 7B from HuggingFace collection|chat",
        "huggingface/llama-2-13b-hf|llama2-13b|llama2-13b-hf|Meta: Llama 2 13B HF|C|0.000000/0.000001|4K/2K|JT|-/-/-|Llama 2 13B base model from HuggingFace|chat",
        "huggingface/codellama-7b-instruct|codellama-7b|codellama|Meta: Code Llama 7B Instruct|C|0.000000/0.000000|8K/2K|JT|-/-/-|Specialized Llama variant for code generation|chat",
        "huggingface/codegemma-7b-it|codegemma-7b|codegemma|Google: CodeGemma 7B|C|0.000000/0.000000|8K/2K|JT|-/-/-|Google Gemma variant specialized for code|chat",
        "huggingface/biollm-7b|biollm-7b|biollm|DNABERT: BioLLM 7B|C|0.000000/0.000000|4K/2K|VS|-/-/-|Specialized model for biomedical NLP tasks|chat",
        "huggingface/sciBERT-base|scibert|scibert-base|AllenAI: SciBERT Base|C|0.000000/0.000000|512/256|S|-/-/-|BERT model for scientific text understanding|chat",
        "huggingface/lawbert-base-uncased|lawbert|lawbert-uncased|LawBERT: Base Uncased|C|0.000000/0.000000|512/256|S|-/-/-|BERT model specialized for legal document analysis|chat",
        "huggingface/finbert-base-uncased|finbert|finbert-uncased|FinBERT: Base Uncased|C|0.000000/0.000000|512/256|S|-/-/-|BERT model for financial sentiment analysis|chat",
        "huggingface/medalpaca-7b|medalpaca|medalpaca-7b|MedAlpaca: 7B|C|0.000000/0.000000|4K/2K|J|-/-/-|Specialized medical domain instruction model|chat",
        "huggingface/umtf-llama2-7b-medical|umtf-medical|medical-llama2|UMTF: Medical Llama2 7B|C|0.000000/0.000000|4K/2K|J|-/-/-|Llama 2 7B fine-tuned for medical domain|chat",
        "huggingface/flan-t5-base|flan-t5-base|flan-base|Google: FLAN-T5 Base|C|0.000000/0.000000|512/256|T|-/-/-|FLAN instruction-tuned T5 base model|chat",
        "huggingface/flan-t5-large|flan-t5-large|flan-large|Google: FLAN-T5 Large|C|0.000000/0.000000|512/512|T|-/-/-|Larger FLAN-T5 for complex instruction tasks|chat",
        "huggingface/peft-adapter-mistral-7b|peft-mistral|peft-adapter|HuggingFace: PEFT Mistral 7B|C|0.000000/0.000000|4K/2K|JT|-/-/-|Mistral 7B with PEFT adapter architecture|chat",
        "huggingface/dolly-v2-3b|dolly-v2-3b|dolly-3b|Databricks: Dolly v2 3B|C|0.000000/0.000000|2K/1K|J|-/-/-|Lightweight Dolly instruction model|chat",
        "huggingface/dolly-v2-12b|dolly-v2-12b|dolly-12b|Databricks: Dolly v2 12B|C|0.000000/0.000001|4K/2K|JT|-/-/-|Databricks Dolly v2 12B instruction-tuned|chat",
        "huggingface/pythia-1b-deduped|pythia-1b|pythia-1b-dedup|EleutherAI: Pythia 1B|C|0.000000/0.000000|2K/512|J|-/-/-|EleutherAI small model for research|chat",
        "huggingface/pythia-6.9b-deduped|pythia-7b|pythia-7b-dedup|EleutherAI: Pythia 7B|C|0.000000/0.000000|4K/2K|JT|-/-/-|EleutherAI 7B model with deduplication|chat",
        "huggingface/pythia-12b-deduped|pythia-12b|pythia-12b-dedup|EleutherAI: Pythia 12B|C|0.000000/0.000001|4K/2K|JT|-/-/-|EleutherAI 12B model for advanced tasks|chat",
        "huggingface/neox-20b|neox-20b|gpt-neox-20b|EleutherAI: GPT-NeoX 20B|C|0.000000/0.000001|8K/2K|JT|-/-/-|EleutherAI 20B parameter autoregressive language model|chat",
        "huggingface/bloom-560m|bloom-560m|bloom-small|BigScience: BLOOM 560M|C|0.000000/0.000000|2K/512|J|-/-/-|Lightweight BLOOM model for research|chat",
        "huggingface/bloom-1b1|bloom-1b1|bloom-1b|BigScience: BLOOM 1.1B|C|0.000000/0.000000|2K/512|J|-/-/-|Smaller BLOOM variant|chat",
        "huggingface/bloom-3b|bloom-3b|bloom|BigScience: BLOOM 3B|C|0.000000/0.000000|2K/1K|J|-/-/-|3B BLOOM for instruction tasks|chat",
        "huggingface/bloom-7b1|bloom-7b1|bloom-7b|BigScience: BLOOM 7B|C|0.000000/0.000000|2K/1K|JT|-/-/-|7B BLOOM multilingual model|chat",
        "huggingface/opt-125m|opt-125m|opt-small|Meta: OPT 125M|C|0.000000/0.000000|2K/512|J|-/-/-|Meta OPT smallest variant|chat",
        "huggingface/opt-350m|opt-350m|opt-350|Meta: OPT 350M|C|0.000000/0.000000|2K/512|J|-/-/-|Meta OPT 350M model|chat",
        "huggingface/opt-1.3b|opt-1.3b|opt-1b|Meta: OPT 1.3B|C|0.000000/0.000000|2K/512|J|-/-/-|Meta OPT 1.3B autoregressive model|chat",
        "huggingface/opt-2.7b|opt-2.7b|opt-2.7b|Meta: OPT 2.7B|C|0.000000/0.000000|2K/1K|J|-/-/-|Meta OPT 2.7B for various tasks|chat",
        "huggingface/opt-6.7b|opt-6.7b|opt-6.7b|Meta: OPT 6.7B|C|0.000000/0.000000|2K/1K|JT|-/-/-|Meta OPT 6.7B with improved performance|chat",
        "huggingface/gpt-neo-2.7b|gpt-neo-2.7b|gpt-neo|EleutherAI: GPT-Neo 2.7B|C|0.000000/0.000000|2K/1K|J|-/-/-|EleutherAI GPT-Neo 2.7B model|chat",
        "huggingface/gpt2-xl|gpt2-xl|gpt2-large|OpenAI: GPT-2 XL|C|0.000000/0.000000|1K/512|J|-/-/-|OpenAI GPT-2 XL 1.5B parameter model|chat",
        "huggingface/xlnet-base-cased|xlnet-base|xlnet|Google: XLNet Base|C|0.000000/0.000000|512/256|S|-/-/-|XLNet base bidirectional transformer|chat",
        "huggingface/albert-base-v2|albert-base|albert-v2|Google: ALBERT Base|C|0.000000/0.000000|512/256|S|-/-/-|ALBERT lightweight model|chat",
        "huggingface/electra-base-discriminator|electra-base|electra|Google: ELECTRA Base|C|0.000000/0.000000|512/256|S|-/-/-|ELECTRA replaced token detection|chat",
        "huggingface/deberta-v3-base|deberta-base|deberta|Microsoft: DeBERTa v3 Base|C|0.000000/0.000000|512/256|S|-/-/-|DeBERTa disentangled attention|chat",
        "huggingface/distilbert-base-uncased|distilbert|distilbert-base|HuggingFace: DistilBERT Base|C|0.000000/0.000000|512/256|S|-/-/-|Lightweight DistilBERT 66M parameters|chat",
        "huggingface/roberta-base|roberta|roberta-base|Facebook: RoBERTa Base|C|0.000000/0.000000|512/256|S|-/-/-|RoBERTa robust BERT pretraining|chat",
        "huggingface/distilbert-base-multilingual-cased-finetuned-ner|distilbert-ner-ft|distil-ner|Hugging Face: DistilBERT NER FT|C|0.000000/0.000000|512/256|S|-/-/-|DistilBERT fine-tuned for multilingual NER|chat",
        "huggingface/distilgpt2-finetuned-wikitext|distilgpt2-wiki|distil-gpt2-ft|Hugging Face: DistilGPT2 WikiText FT|C|0.000000/0.000000|1K/512|T|-/-/-|DistilGPT2 fine-tuned on WikiText|chat",
        "huggingface/distilroberta-base-finetuned-sst2|distilroberta-sst2|distil-robin-ft|Hugging Face: DistilRoBERTa SST2 FT|C|0.000000/0.000000|512/256|S|-/-/-|DistilRoBERTa fine-tuned for sentiment|chat",
        "huggingface/mobilenet-v2-finetuned-imagenet|mobilenet-v2-ft|mobile-net-ft|Hugging Face: MobileNet v2 ImageNet FT|C|0.000000/0.000000|224/256|V|-/-/-|MobileNet v2 fine-tuned on ImageNet|chat",
        "huggingface/bert-base-chinese-finetuned-nlcke|bert-chinese-ft|bert-zh-ft|Hugging Face: BERT Chinese NLCKE FT|C|0.000000/0.000000|512/256|S|-/-/-|BERT fine-tuned for Chinese entity extraction|chat",
        "huggingface/japanese-bert-finetuned-dep-parser|bert-japanese-ft|bert-ja-ft|Hugging Face: Japanese BERT Dep FT|C|0.000000/0.000000|512/256|S|-/-/-|Japanese BERT fine-tuned for dependency parsing|chat",
        "huggingface/xlm-roberta-base-finetuned-cross-lingual-sentiment|xlm-roberta-sent-ft|xlm-sent-ft|Hugging Face: XLM-RoBERTa Sentiment FT|C|0.000000/0.000000|512/256|S|-/-/-|XLM-RoBERTa fine-tuned for cross-lingual sentiment|chat",
        "huggingface/t5-small-finetuned-question-generation|t5-qa-gen-ft|t5-qg-ft|Hugging Face: T5 Small QA Generation FT|C|0.000000/0.000000|512/256|T|-/-/-|T5 Small fine-tuned for question generation|chat",
        "huggingface/bart-large-finetuned-scientific-abstractive-summarization|bart-sci-sum-ft|bart-sci-ft|Hugging Face: BART Scientific Sum FT|C|0.000000/0.000000|1K/512|T|-/-/-|BART fine-tuned for scientific paper summarization|chat",
        "huggingface/instrublip-flan-t5-xl|instructblip-xl|Hugging Face: InstructBLIP XL|C|0.000000/0.000001|2K/512|VS|-/-/-|Open-source InstructBLIP model with vision-language understanding|chat",
        "huggingface/idefics2-8b|idefics2-8b|Hugging Face: Idefics2 8B|C|0.000000/0.000001|4K/1K|VS|-/-/-|French-centric multimodal LLM with vision|chat",
        "huggingface/idefics2-27b|idefics2-27b|Hugging Face: Idefics2 27B|C|0.000000/0.000003|4K/2K|VS|-/-/-|Larger Idefics2 for advanced vision tasks|chat",
        "huggingface/phi-3-vision-128k|phi-3-vision-128k|Microsoft: Phi 3 Vision|C|0.000000/0.000005|128K/4K|VST|-/-/-|Compact vision model from Microsoft with 128K context|chat",
        "huggingface/moondream2|moondream2|Moondream: Moondream2|C|0.000000/0.000000|2K/512|VS|-/-/-|Ultra-lightweight vision model optimized for edge|chat",

    // === HYPERBOLIC (13 models) ===
        "hyperbolic/DeepSeek-V3|deepseek-v3|DeepSeek V3|C|0.5000/1.00|131K/8K|TJS|-/-/-|DeepSeek V3 on Hyperbolic|chat",
        "hyperbolic/DeepSeek-R1|deepseek-r1|DeepSeek R1|C|0.5000/2.00|65K/8K|TJK|-/-/-|DeepSeek R1 reasoning|chat",
        "hyperbolic/DeepSeek-R1-Zero|deepseek-r1-zero|DeepSeek R1 Zero|C|0.5000/2.00|65K/8K|TJK|-/-/-|DeepSeek R1 Zero-shot|chat",
        "hyperbolic/Meta-Llama-3.1-405B-Instruct|llama-3.1-405b|Llama 3.1 405B|C|3.00/3.00|131K/8K|TJS|-/-/-|Llama 3.1 405B on Hyperbolic|chat",
        "hyperbolic/Meta-Llama-3.1-70B-Instruct|llama-3.1-70b|Llama 3.1 70B|C|0.4000/0.4000|131K/8K|TJS|-/-/-|Llama 3.1 70B on Hyperbolic|chat",
        "hyperbolic/Meta-Llama-3.1-8B-Instruct|llama-3.1-8b|Llama 3.1 8B|C|0.0400/0.0400|131K/8K|TJS|-/-/-|Llama 3.1 8B on Hyperbolic|chat",
        "hyperbolic/Llama-3.3-70B-Instruct|llama-3.3-70b|Llama 3.3 70B|C|0.4000/0.4000|131K/8K|TJS|-/-/-|Llama 3.3 70B on Hyperbolic|chat",
        "hyperbolic/Qwen2.5-72B-Instruct|qwen-2.5-72b|Qwen 2.5 72B|C|0.4000/0.4000|131K/8K|TJS|-/-/-|Qwen 2.5 72B on Hyperbolic|chat",
        "hyperbolic/QwQ-32B-Preview|qwq-32b|QwQ 32B|C|0.2000/0.2000|32K/32K|TJK|-/-/-|QwQ reasoning model|chat",
        "hyperbolic/Qwen2.5-Coder-32B-Instruct|qwen-coder-32b|Qwen 2.5 Coder 32B|C|0.2000/0.2000|131K/8K|TJS|-/-/-|Qwen Coder on Hyperbolic|chat",
        "hyperbolic/Hermes-3-Llama-3.1-70B|hermes-3-70b|Hermes 3 70B|C|0.4000/0.4000|131K/8K|TJS|-/-/-|Hermes 3 Llama 70B|chat",
        "hyperbolic/stable-diffusion-xl-base-1.0|sdxl|SDXL|C|0.0020/-|77/-|I|-/-/-|SDXL image generation|image",
        "hyperbolic/FLUX.1-dev|flux-dev|FLUX.1 Dev|C|0.0030/-|77/-|I|-/-/-|FLUX image generation|image",

    // === IBM-GRANITE (1 models) ===
        "ibm-granite/granite-4.0-h-micro|granite-4.0-h-micro|IBM: Granite 4.0 Micro|C|0.000000/0.000000|131K/32K|-|-/-/-|Granite-4.0-H-Micro is a 3B parameter from the Granite 4 family of models. These|chat",

    // === INCEPTION (2 models) ===
        "inception/mercury|mercury|Inception: Mercury|C|0.000000/0.000001|128K/16K|JST|-/-/-|Mercury is the first diffusion large language model (dLLM). Applying a breakthro|chat",
        "inception/mercury-coder|mercury-coder|Inception: Mercury Coder|C|0.000000/0.000001|128K/16K|JST|-/-/-|Mercury Coder is the first diffusion large language model (dLLM). Applying a bre|chat",

    // === INCONTEXT (1 models) ===
        "incontext/incontext-learner|incontext-f|In-Context Learner|C|0.000300/0.000900|4K/1K|VSTJ|-/-/-|In-context learning model|chat",

    // === INFLECTION (2 models) ===
        "inflection/inflection-3-pi|inflection-3-pi|Inflection: Inflection 3 Pi|C|0.000003/0.000010|8K/1K|-|-/-/-|Inflection 3 Pi powers Inflection's [Pi](https://pi.ai) chatbot, including backs|chat",
        "inflection/inflection-3-productivity|inflection-3-product|Inflection: Inflection 3 Productivity|C|0.000003/0.000010|8K/1K|-|-/-/-|Inflection 3 Productivity is optimized for following instructions. It is better |chat",

    // === INSTRUCT (5 models) ===
        "instruct/openchat-3.5|openchat-3.5-i|OpenChat: 3.5 Instruct|C|0.000080/0.000080|8K/2K|VSTJ|-/-/-|OpenChat 3.5 instruction tuned|chat",
        "instruct/neural-chat-7b-v3.3|neural-chat-v33-i|Intel: Neural Chat 7B v3.3|C|0.000100/0.000200|8K/2K|VSTJ|-/-/-|Intel Neural Chat v3.3|chat",
        "instruct/llama-3-8b-instruct-multilingual|llama-3-8b-inst|llama-3-inst|Meta: Llama 3 8B Instruct Multilingual|C|0.000000/0.000000|8K/2K|VSTJ|-/-/-|Llama 3 8B instruction-tuned for multilingual use|chat",
        "instruct/mistral-7b-instruct-v0.2-toolcalling|mistral-tools|mistral-tool|Mistral: 7B Instruct Tool-Calling|C|0.000000/0.000000|32K/4K|VSTJ|-/-/-|Mistral 7B instruction-tuned for tool calling|chat",
        "instruct/qwen-7b-instruct-specialized|qwen-7b-inst|qwen-inst|Alibaba: Qwen 7B Instruct Specialized|C|0.000000/0.000000|4K/2K|VSTJ|-/-/-|Qwen 7B instruction-tuned for specialized domains|chat",

    // === IR (2 models) ===
        "ir/cross-encoder-ms-marco-MiniLM-L-6-v2|cross-encoder-msmarco|ir-ranker|Sentence Transformers: Cross-Encoder MARCO|C|0.000000/0.000000|512/256|S|-/-/-|Cross-encoder for passage ranking|chat",
        "ir/cross-encoder-qnli-distilroberta-base|cross-encoder-qnli|ir-classifier|Sentence Transformers: Cross-Encoder QNLI|C|0.000000/0.000000|512/256|S|-/-/-|Cross-encoder for query-document classification|chat",

    // === JAPAN (1 models) ===
        "japan/cyberagent-llama-70b|cyberagent-llama-j|CyberAgent: Llama 70B JP|C|0.000900/0.000900|8K/2K|VSTJ|-/-/-|Llama 70B Japanese optimized|chat",

    // === JAPANESE (3 models) ===
        "japanese/mistral-large-jp|mistral-jp|mistral-japanese|Mistral: Large Japanese|C|0.000000/0.000000|8K/2K|VSTJ|-/-/-|Mistral Large optimized for Japanese language|chat",
        "japanese/llama-2-13b-jp|llama-jp|llama-japanese|Meta: Llama 2 13B Japanese|C|0.000000/0.000001|4K/2K|VT|-/-/-|Llama 2 13B fine-tuned for Japanese|chat",
        "japanese/rinna-3.6b-instruction|rinna-3.6b|rinna|Rinna: 3.6B Instruction|C|0.000000/0.000000|2K/1K|T|-/-/-|Rinna 3.6B Japanese instruction model|chat",

    // === JINA (16 models) ===
        "jina/jina-embeddings-v3|jina-embed-v3|Jina Embeddings v3|C|0.0200/-|8K/1K|E|-/-/-|Latest multilingual embeddings|embed",
        "jina/jina-embeddings-v2-base-en|jina-embed-v2-en|Jina Embeddings v2 Base EN|C|0.0200/-|8K/768|E|-/-/-|English embeddings, 768 dimensions|embed",
        "jina/jina-embeddings-v2-base-de|jina-embed-v2-de|Jina Embeddings v2 Base DE|C|0.0200/-|8K/768|E|-/-/-|German embeddings|embed",
        "jina/jina-embeddings-v2-base-es|jina-embed-v2-es|Jina Embeddings v2 Base ES|C|0.0200/-|8K/768|E|-/-/-|Spanish embeddings|embed",
        "jina/jina-embeddings-v2-base-zh|jina-embed-v2-zh|Jina Embeddings v2 Base ZH|C|0.0200/-|8K/768|E|-/-/-|Chinese embeddings|embed",
        "jina/jina-embeddings-v2-base-code|jina-embed-v2-code|Jina Embeddings v2 Code|C|0.0200/-|8K/768|E|-/-/-|Code embeddings|embed",
        "jina/jina-embeddings-v2-small-en|jina-embed-v2-small|Jina Embeddings v2 Small EN|C|0.0100/-|8K/512|E|-/-/-|Small English embeddings|embed",
        "jina/jina-clip-v2|jina-clip-v2|Jina CLIP v2|C|0.0200/-|8K/1K|VE|-/-/-|Multimodal image-text embeddings|embed",
        "jina/jina-clip-v1|jina-clip-v1|Jina CLIP v1|C|0.0200/-|8K/768|VE|-/-/-|Image-text embeddings|embed",
        "jina/jina-colbert-v2|jina-colbert-v2|Jina ColBERT v2|C|0.0200/-|8K/128|E|-/-/-|Late interaction retrieval|embed",
        "jina/jina-colbert-v1-en|jina-colbert-v1|Jina ColBERT v1 EN|C|0.0200/-|8K/128|E|-/-/-|English ColBERT|embed",
        "jina/jina-reranker-v2-base-multilingual|jina-rerank-v2|Jina Reranker v2 Multilingual|C|0.0200/-|8K/-|R|-/-/-|Multilingual reranking|rerank",
        "jina/jina-reranker-v1-base-en|jina-rerank-v1|Jina Reranker v1 EN|C|0.0200/-|8K/-|R|-/-/-|English reranking|rerank",
        "jina/jina-reranker-v1-turbo-en|jina-rerank-turbo|Jina Reranker v1 Turbo|C|0.0100/-|8K/-|R|-/-/-|Fast English reranking|rerank",
        "jina/jina-reranker-v1-tiny-en|jina-rerank-tiny|Jina Reranker v1 Tiny|C|0.0050/-|8K/-|R|-/-/-|Tiny reranking model|rerank",
        "jina/jina-reader-v1|jina-reader|Jina Reader v1|C|0/-|-/-|-|-/-/-|URL/PDF to markdown|chat",

    // === KOREAN (2 models) ===
        "korean/solar-ko-7b|solar-ko|solar-korean|Upstage: Solar 7B Korean|C|0.000000/0.000000|4K/2K|VSTJ|-/-/-|Solar 7B optimized for Korean language|chat",
        "korean/koalpaca-13b|koalpaca|koalpaca-korean|KoAlpaca: 13B|C|0.000000/0.000001|4K/2K|T|-/-/-|KoAlpaca 13B for Korean instruction tasks|chat",

    // === KWAIPILOT (1 models) ===
        "kwaipilot/kat-coder-pro:free|kat-coder-pro:free|Kwaipilot: KAT-Coder-Pro V1 (free)|C|-/-|256K/32K|JST|-/-/-|KAT-Coder-Pro V1 is KwaiKAT's most advanced agentic coding model in the KAT-Code|chat",

    // === LABELING (2 models) ===
        "labeling/label-studio|labelstudio-f|Label Studio Model|C|0.000150/0.000450|4K/512|VSTJ|-/-/-|Active learning labeling|chat",
        "labeling/prodigy-model|prodigy-f|Prodigy Annotation|C|0.000200/0.000600|4K/512|VSTJ|-/-/-|Active learning annotation|chat",

    // === LATINAMERICAN (2 models) ===
        "latinamerican/latamgpt-large|latamgpt-large|latam-large|LatamGPT: Large|C|0.000000/0.000000|4K/2K|VSTJ|-/-/-|LatamGPT Large optimized for Spanish and Portuguese|chat",
        "latinamerican/latamgpt-base|latamgpt|latam-base|LatamGPT: Base|C|0.000000/0.000000|2K/1K|VT|-/-/-|LatamGPT Base for efficient Latin American use|chat",

    // === LEGAL (6 models) ===
        "legal/legalai-llama|legalai-llama|legal-llama|LegalAI: Llama Legal|C|0.000001/0.000003|8K/2K|VSTJ|-/-/-|Llama fine-tuned for legal document analysis|chat",
        "legal/contractgpt|contractgpt|legal-contract|ContractGPT: Agreements|C|0.000001/0.000004|8K/2K|VSTJ|-/-/-|GPT variant specialized for contract analysis|chat",
        "legal/regulationgpt|regulationgpt|legal-reg|RegulationGPT: Compliance|C|0.000001/0.000003|8K/2K|VSTJ|-/-/-|Model for regulatory compliance and legal research|chat",
        "legal/legalbert|legalbert|legal-domain|LegalBERT: Domain|C|0.000000/0.000000|512/256|S|-/-/-|LegalBERT specialized for legal documents|chat",
        "legal/legalberta|legalberta|legal-albert|LegalBERTa: ALBERTa|C|0.000000/0.000000|512/256|S|-/-/-|LegalBERTa efficient legal model|chat",
        "legal/contractnorm-legal|contractnorm|contract-legal|ContractNorm: Legal|C|0.000000/0.000000|512/256|S|-/-/-|ContractNorm for contract analysis|chat",

    // === LIQUID (2 models) ===
        "liquid/lfm2-8b-a1b|lfm2-8b-a1b|LiquidAI/LFM2-8B-A1B|C|0.000000/0.000000|32K/8K|-|-/-/-|Model created via inbox interface|chat",
        "liquid/lfm-2.2-6b|lfm-2.2-6b|LiquidAI/LFM2-2.6B|C|0.000000/0.000000|32K/8K|-|-/-/-|LFM2 is a new generation of hybrid models developed by Liquid AI, specifically d|chat",

    // === LLAMA (2 models) ===
        "llama/llama-3.2-90b-vision-instruct|llama-3.2-90b-vision|Meta: Llama 3.2 90B Vision|C|0.000001/0.000009|8K/4K|VST|-/-/-|Llama 3.2 90B with native vision understanding for images and charts|chat",
        "llama/llama-3.2-11b-vision-instruct|llama-3.2-11b-vision|Meta: Llama 3.2 11B Vision|C|0.000000/0.000001|8K/4K|VST|-/-/-|Compact Llama 3.2 vision model optimized for edge deployment|chat",

    // === LOCAL (4 models) ===
        "local/mistral-7b-ggml-q5|mistral-local-q5|mistral-ggml|Local: Mistral 7B GGML Q5|C|0/0|32K/4K|JT|-/-/-|Mistral 7B GGML format for local deployment|chat",
        "local/neural-chat-7b-ggml-q4|neural-local-q4|neural-ggml|Local: Neural Chat GGML Q4|C|0/0|8K/2K|JT|-/-/-|Neural Chat GGML format for edge devices|chat",
        "local/tinyllama-1.1b-ggml-q8|tinyllama-local|tinyllama-ggml|Local: TinyLlama GGML Q8|C|0/0|2K/512|JT|-/-/-|TinyLlama GGML for ultra-lightweight local inference|chat",
        "local/openhermes-2.5-ggml-q5|openhermes-local|openhermes-ggml|Local: OpenHermes GGML Q5|C|0/0|4K/2K|JT|-/-/-|OpenHermes GGML for local chat deployment|chat",

    // === LONGCONTEXT (3 models) ===
        "longcontext/claude-opus-4-200k|claude-opus-200k-lc|Anthropic: Claude Opus 200K Context|C|5.00/25.00/0.5000|200K/32K|VSTJKC|-/-/-|Claude Opus with 200K context window|chat",
        "longcontext/gpt-4-turbo-128k|gpt-4-128k-lc|OpenAI: GPT-4 Turbo 128K|C|0.0010/0.0030/0.000500|128K/4K|VSTJK|-/-/-|GPT-4 Turbo with 128K context|chat",
        "longcontext/gemini-1.5-pro-1m|gemini-1m-lc|Google: Gemini 1.5 Pro 1M|C|0.0075/0.0300|1000K/8K|VSTJK|-/-/-|Gemini 1.5 Pro with 1M context|chat",

    // === MANCER (1 models) ===
        "mancer/weaver|weaver|Mancer: Weaver (alpha)|C|0.000001/0.000001|8K/2K|J|-/-/-|An attempt to recreate Claude-style verbosity, but don't expect the same level o|chat",

    // === MANUFACTURING (2 models) ===
        "manufacturing/qualitygpt|qualitygpt|mfg-quality|QualityGPT: Inspection|C|0.000001/0.000003|8K/2K|VSTJ|-/-/-|Quality control and defect detection|chat",
        "manufacturing/predictivemaint|predictmaint|mfg-maintenance|PredictiveMaintGPT|C|0.000001/0.000004|8K/2K|VSTJ|-/-/-|Predictive maintenance and equipment optimization|chat",

    // === MARKETING (4 models) ===
        "marketing/copygpt|copygpt|marketing-copy|CopyGPT: Marketing|C|0.000001/0.000002|8K/2K|VSTJ|-/-/-|Marketing copy generation and optimization|chat",
        "marketing/seo-optimizer|seo-optimizer|marketing-seo|SEOGPT: Optimizer|C|0.000001/0.000002|4K/2K|VSTJ|-/-/-|SEO content generation and optimization|chat",
        "marketing/socialmediagpt|socialgpt|marketing-social|SocialMediaGPT: Posts|C|0.000001/0.000002|4K/2K|VSTJ|-/-/-|Social media content creation and scheduling|chat",
        "marketing/newsgpt|newsgpt|marketing-news|NewsGPT: Journalism|C|0.000001/0.000003|8K/2K|VSTJ|-/-/-|News article generation and fact-checking|chat",

    // === MEDICAL (4 models) ===
        "medical/biobert-base|biobert|biobert-model|BioBERT: Base|C|0.000000/0.000000|512/256|S|-/-/-|BioBERT for biomedical text mining|chat",
        "medical/pubmedbert|pubmedbert|pubmed-bert|PubMedBERT: Domain|C|0.000000/0.000000|512/256|S|-/-/-|PubMedBERT trained on PubMed abstracts|chat",
        "medical/biolinkbert-base|biolinkbert|biolink-bert|BioLinkBERT: Base|C|0.000000/0.000000|512/256|S|-/-/-|BioLinkBERT with biomedical entity linking|chat",
        "medical/clinical-bert|clinical-bert|clinbert|ClinicalBERT: MIMIC|C|0.000000/0.000000|512/256|S|-/-/-|ClinicalBERT trained on MIMIC clinical notes|chat",

    // === MEITUAN (1 models) ===
        "meituan/longcat-flash-chat|longcat-flash-chat|Meituan: LongCat Flash Chat|C|0.000000/0.000001|131K/131K|-|-/-/-|LongCat-Flash-Chat is a large-scale Mixture-of-Experts (MoE) model with 560B tot|chat",

    // === META (5 models) ===
        "meta/llama-4-405b-20260115|llama-4-405b|Llama 4 405B|C|0.000005/0.000016|128K/8K|ST|-/-/-|Latest Meta flagship reasoning model|chat",
        "meta/llama-4-70b-20260115|llama-4-70b|Llama 4 70B|C|0.000001/0.000003|128K/8K|ST|-/-/-|Mid-range Llama 4 model|chat",
        "meta/llama-2-70b-finetuned-instruct-medical|llama-medical|llama-med-ft|Meta: Llama 2 70B Medical FT|C|0.000001/0.000003|8K/2K|VSTJ|-/-/-|Llama 70B fine-tuned for medical domain|chat",
        "meta/llama-2-70b-finetuned-code-llama|llama-code|llama-code-ft|Meta: Llama 2 70B Code FT|C|0.000001/0.000003|8K/2K|VSTJ|-/-/-|Llama 70B fine-tuned for code generation|chat",
        "meta/llama-3-70b-finetuned-chat|llama-3-chat|llama-3-chat-ft|Meta: Llama 3 70B Chat FT|C|0.000002/0.000005|8K/2K|VSTJ|-/-/-|Llama 3 70B fine-tuned for chat|chat",

    // === META-LLAMA (19 models) ===
        "meta-llama/llama-guard-4-12b|llama-guard-4-12b|Meta: Llama Guard 4 12B|C|0.000000/0.000000|163K/40K|JV|-/-/-|Llama Guard 4 is a Llama 4 Scout-derived multimodal pretrained model, fine-tuned|chat",
        "meta-llama/llama-4-maverick|llama-4-maverick|Meta: Llama 4 Maverick|C|0.000000/0.000001|1048K/16K|JSTV|-/-/-|Llama 4 Maverick 17B Instruct (128E) is a high-capacity multimodal language mode|chat",
        "meta-llama/llama-4-scout|llama-4-scout|Meta: Llama 4 Scout|C|0.000000/0.000000|327K/16K|JSTV|-/-/-|Llama 4 Scout 17B Instruct (16E) is a mixture-of-experts (MoE) language model de|chat",
        "meta-llama/llama-guard-3-8b|llama-guard-3-8b|Llama Guard 3 8B|C|0.000000/0.000000|131K/32K|J|-/-/-|Llama Guard 3 is a Llama-3.1-8B pretrained model, fine-tuned for content safety |chat",
        "meta-llama/llama-3.3-70b-instruct:free|llama-3.3-70b-instru|Meta: Llama 3.3 70B Instruct (free)|C|-/-|131K/32K|T|-/-/-|The Meta Llama 3.3 multilingual large language model (LLM) is a pretrained and i|chat",
        "meta-llama/llama-3.3-70b-instruct|llama-3.3-70b-instru|Meta: Llama 3.3 70B Instruct|C|0.000000/0.000000|131K/16K|JST|-/-/-|The Meta Llama 3.3 multilingual large language model (LLM) is a pretrained and i|chat",
        "meta-llama/llama-3.2-3b-instruct:free|llama-3.2-3b-instruc|Meta: Llama 3.2 3B Instruct (free)|C|-/-|131K/32K|-|-/-/-|Llama 3.2 3B is a 3-billion-parameter multilingual large language model, optimiz|chat",
        "meta-llama/llama-3.2-3b-instruct|llama-3.2-3b-instruc|Meta: Llama 3.2 3B Instruct|C|0.000000/0.000000|131K/16K|JT|-/-/-|Llama 3.2 3B is a 3-billion-parameter multilingual large language model, optimiz|chat",
        "meta-llama/llama-3.2-1b-instruct|llama-3.2-1b-instruc|Meta: Llama 3.2 1B Instruct|C|0.000000/0.000000|60K/15K|-|-/-/-|Llama 3.2 1B is a 1-billion-parameter language model focused on efficiently perf|chat",
        "meta-llama/llama-3.2-90b-vision-instruct|llama-3.2-90b-vision|Meta: Llama 3.2 90B Vision Instruct|C|0.000000/0.000000|32K/16K|JV|-/-/-|The Llama 90B Vision model is a top-tier, 90-billion-parameter multimodal model |chat",
        "meta-llama/llama-3.2-11b-vision-instruct|llama-3.2-11b-vision|Meta: Llama 3.2 11B Vision Instruct|C|0.000000/0.000000|131K/16K|JV|-/-/-|Llama 3.2 11B Vision is a multimodal model with 11 billion parameters, designed |chat",
        "meta-llama/llama-3.1-405b|llama-3.1-405b|Meta: Llama 3.1 405B (base)|C|0.000004/0.000004|32K/32K|-|-/-/-|Meta's latest class of model (Llama 3.1) launched with a variety of sizes & flav|chat",
        "meta-llama/llama-3.1-8b-instruct|llama-3.1-8b-instruc|Meta: Llama 3.1 8B Instruct|C|0.000000/0.000000|131K/16K|JST|-/-/-|Meta's latest class of model (Llama 3.1) launched with a variety of sizes & flav|chat",
        "meta-llama/llama-3.1-405b-instruct:free|llama-3.1-405b-instr|Meta: Llama 3.1 405B Instruct (free)|C|-/-|131K/32K|-|-/-/-|The highly anticipated 400B class of Llama3 is here! Clocking in at 128k context|chat",
        "meta-llama/llama-3.1-405b-instruct|llama-3.1-405b-instr|Meta: Llama 3.1 405B Instruct|C|0.000003/0.000003|10K/2K|JST|-/-/-|The highly anticipated 400B class of Llama3 is here! Clocking in at 128k context|chat",
        "meta-llama/llama-3.1-70b-instruct|llama-3.1-70b-instru|Meta: Llama 3.1 70B Instruct|C|0.000000/0.000000|131K/32K|JT|-/-/-|Meta's latest class of model (Llama 3.1) launched with a variety of sizes & flav|chat",
        "meta-llama/llama-guard-2-8b|llama-guard-2-8b|Meta: LlamaGuard 2 8B|C|0.000000/0.000000|8K/2K|-|-/-/-|This safeguard model has 8B parameters and is based on the Llama 3 family. Just |chat",
        "meta-llama/llama-3-70b-instruct|llama-3-70b-instruct|Meta: Llama 3 70B Instruct|C|0.000000/0.000000|8K/16K|JST|-/-/-|Meta's latest class of model (Llama 3) launched with a variety of sizes & flavor|chat",
        "meta-llama/llama-3-8b-instruct|llama-3-8b-instruct|Meta: Llama 3 8B Instruct|C|0.000000/0.000000|8K/16K|JT|-/-/-|Meta's latest class of model (Llama 3) launched with a variety of sizes & flavor|chat",

    // === MICROSOFT (7 models) ===
        "microsoft/phi-4-reasoning-plus|phi-4-reasoning-plus|Microsoft: Phi 4 Reasoning Plus|C|0.000000/0.000000|32K/8K|JK|-/-/-|Phi-4-reasoning-plus is an enhanced 14B parameter model from Microsoft, fine-tun|chat",
        "microsoft/phi-4-multimodal-instruct|phi-4-multimodal-ins|Microsoft: Phi 4 Multimodal Instruct|C|0.000000/0.000000|131K/32K|JV|-/-/-|Phi-4 Multimodal Instruct is a versatile 5.6B parameter foundation model that co|chat",
        "microsoft/phi-4|phi-4|Microsoft: Phi 4|C|0.000000/0.000000|16K/4K|JS|-/-/-|[Microsoft Research](/microsoft) Phi-4 is designed to perform well in complex re|chat",
        "microsoft/phi-3.5-mini-128k-instruct|phi-3.5-mini-128k-in|Microsoft: Phi-3.5 Mini 128K Instruct|C|0.000000/0.000000|128K/32K|T|-/-/-|Phi-3.5 models are lightweight, state-of-the-art open models. These models were |chat",
        "microsoft/phi-3-mini-128k-instruct|phi-3-mini-128k-inst|Microsoft: Phi-3 Mini 128K Instruct|C|0.000000/0.000000|128K/32K|T|-/-/-|Phi-3 Mini is a powerful 3.8B parameter model designed for advanced language und|chat",
        "microsoft/phi-3-medium-128k-instruct|phi-3-medium-128k-in|Microsoft: Phi-3 Medium 128K Instruct|C|0.000001/0.000001|128K/32K|T|-/-/-|Phi-3 128K Medium is a powerful 14-billion parameter model designed for advanced|chat",
        "microsoft/wizardlm-2-8x22b|wizardlm-2-8x22b|WizardLM-2 8x22B|C|0.000000/0.000000|65K/16K|J|-/-/-|WizardLM-2 8x22B is Microsoft AI's most advanced Wizard model. It demonstrates h|chat",

    // === MIDDLE_EAST (2 models) ===
        "middle_east/arabert-base|arabert|arabert-large|AraGPT: AraBoRT Base|C|0.000000/0.000000|512/256|S|-/-/-|AraBoRT for Arabic language understanding|chat",
        "middle_east/gpt2-arabic|gpt2-ar|gpt2-arabic|GPT2: Arabic|C|0.000000/0.000000|1K/512|T|-/-/-|GPT-2 Arabic variant for text generation|chat",

    // === MILESTONE (11 models) ===
        "milestone/modelsuite-comprehensive-registry|registry-final|ModelSuite: Comprehensive Registry 1200+|C|0/0|1000K/32K|VSTJSKC|-/-/-|Complete 1200+ unified model registry|chat",
        "milestone/universal-agent|universal-agent-m|ModelSuite: Universal Agent|C|0.0050/0.0150|200K/8K|VSTJSKC|-/-/-|Unified multi-modal agent|chat",
        "milestone/expert-routing-system|expert-routing-m|ModelSuite: Expert Routing System|C|0.0030/0.0090|128K/4K|VSTJKC|-/-/-|Intelligent model selection engine|chat",
        "milestone/knowledge-synthesis|knowledge-synthesis-m|ModelSuite: Knowledge Synthesis|C|0.0020/0.0060|100K/4K|VSTJ|-/-/-|Cross-model knowledge synthesis|chat",
        "milestone/performance-optimizer|perf-optim-m|ModelSuite: Performance Optimizer|C|0.0010/0.0030|64K/2K|VSTJ|-/-/-|Model performance optimization|chat",
        "milestone/cost-efficiency-engine|cost-engine-m|ModelSuite: Cost Efficiency Engine|C|0.000500/0.0015|32K/2K|VSTJ|-/-/-|Optimal cost-performance selection|chat",
        "milestone/latency-reducer|latency-m|ModelSuite: Latency Reducer|C|0.000300/0.000900|16K/1K|VST|-/-/-|Ultra-low latency optimization|chat",
        "milestone/reliability-orchestrator|reliability-m|ModelSuite: Reliability Orchestrator|C|0.000200/0.000600|8K/512|VSTJ|-/-/-|High-reliability model orchestration|chat",
        "milestone/security-guardian|security-m|ModelSuite: Security Guardian|C|0.000400/0.0012|16K/1K|VSTJ|-/-/-|Security-focused model wrapper|chat",
        "milestone/compliance-monitor|compliance-m|ModelSuite: Compliance Monitor|C|0.000600/0.0018|32K/2K|VSTJ|-/-/-|Regulatory compliance enforcer|chat",
        "milestone/ecosystem-connector|ecosystem-m|ModelSuite: Ecosystem Connector|C|0.000800/0.0024|64K/2K|VSTJK|-/-/-|Universal AI ecosystem integration|chat",

    // === MINIMAX (17 models) ===
        "minimax/minimax-m2.1|minimax-m2.1|MiniMax: MiniMax M2.1|C|0.000000/0.000000|196K/49K|JKST|-/-/-|MiniMax-M2.1 is a lightweight, state-of-the-art large language model optimized f|chat",
        "minimax/minimax-m2|minimax-m2|MiniMax: MiniMax M2|C|0.000000/0.000001|196K/65K|JKST|-/-/-|MiniMax-M2 is a compact, high-efficiency large language model optimized for end-|chat",
        "minimax/minimax-m1|minimax-m1|MiniMax: MiniMax M1|C|0.000000/0.000002|1000K/40K|KT|-/-/-|MiniMax-M1 is a large-scale, open-weight reasoning model designed for extended c|chat",
        "minimax/minimax-01|minimax-01|MiniMax: MiniMax-01|C|0.000000/0.000001|1000K/1000K|V|-/-/-|MiniMax-01 is a combines MiniMax-Text-01 for text generation and MiniMax-VL-01 f|chat",
        "minimax/abab7-chat-preview|abab7|ABAB 7|C|0.7000/0.7000|4000K/16K|VTJS|-/-/-|Flagship 4M context model for complex tasks|chat",
        "minimax/abab6.5s-chat|abab6.5s|ABAB 6.5s|C|0.1400/0.1400|245K/16K|VTJS|-/-/-|Fast 245K context model|chat",
        "minimax/abab6.5t-chat|abab6.5t|ABAB 6.5t|C|0.0700/0.0700|8K/8K|TJS|-/-/-|Efficient text model|chat",
        "minimax/abab6.5g-chat|abab6.5g|ABAB 6.5g|C|0.1400/0.1400|8K/8K|TJS|-/-/-|General purpose model|chat",
        "minimax/abab6-chat|abab6|ABAB 6|C|0.2100/0.2100|32K/8K|VTJ|-/-/-|Previous generation ABAB|chat",
        "minimax/abab5.5s-chat|abab5.5s|ABAB 5.5s|C|0.0700/0.0700|16K/8K|TJ|-/-/-|Legacy efficient model|chat",
        "minimax/abab5.5-chat|abab5.5|ABAB 5.5|C|0.2100/0.2100|16K/8K|VTJ|-/-/-|Legacy vision model|chat",
        "minimax/speech-01-turbo|speech-turbo|Speech 01 Turbo|C|0.0070/-|-/-|A|-/-/-|Fast TTS model|audio",
        "minimax/speech-01-hd|speech-hd|Speech 01 HD|C|0.1000/-|-/-|A|-/-/-|High-quality TTS|audio",
        "minimax/speech-02-turbo|speech-02|Speech 02 Turbo|C|0.0070/-|-/-|A|-/-/-|Next-gen TTS|audio",
        "minimax/video-01|video-01|Video 01|C|0.3000/-|-/-|D|-/-/-|Video generation|video",
        "minimax/music-01|music-01|Music 01|C|0.0100/-|-/-|A|-/-/-|Music generation|audio",
        "minimax/embo-01|embo-01|Embo 01|C|0.000100/-|4K/1K|E|-/-/-|Text embeddings|embed",

    // === MISTRAL (34 models) ===
        "mistral/mistral-large-3-20260101|mistral-large-3|Mistral Large 3|C|0.000001/0.000002|200K/64K|TJ|-/-/-|Latest flagship from Mistral|chat",
        "mistral/mistral-large-2411|mistral-large|Mistral Large 24.11|C|2.00/6.00|131K/131K|VTJS|-/-/-|Flagship model for complex tasks and reasoning|chat",
        "mistral/mistral-large-2407|mistral-large-2407|Mistral Large 24.07|C|2.00/6.00|131K/131K|VTJS|-/-/-|Previous Mistral Large version|chat",
        "mistral/pixtral-large-2411|pixtral-large|Pixtral Large|C|2.00/6.00|131K/131K|VTJS|-/-/-|Vision-enabled large model|chat",
        "mistral/mistral-medium-2505|mistral-medium|Mistral Medium 25.05|C|0.4000/2.00|131K/131K|VTJS|-/-/-|Balanced performance and cost|chat",
        "mistral/mistral-small-2503|mistral-small|Mistral Small 25.03|C|0.1000/0.3000|32K/32K|VTJS|-/-/-|Fast and efficient for most tasks|chat",
        "mistral/mistral-small-2409|mistral-small-2409|Mistral Small 24.09|C|0.1000/0.3000|32K/32K|VTJS|-/-/-|Previous Mistral Small version|chat",
        "mistral/pixtral-12b-2409|pixtral-12b|Pixtral 12B|C|0.1500/0.1500|131K/131K|VTJ|-/-/-|Vision model with 12B parameters|chat",
        "mistral/mistral-nemo-2407|mistral-nemo|Mistral Nemo|C|0.1500/0.1500|131K/131K|TJ|-/-/-|12B parameter free tier model|chat",
        "mistral/codestral-2501|codestral|Codestral 25.01|C|0.3000/0.9000|262K/262K|TJ|-/-/-|Specialized code generation model|chat",
        "mistral/codestral-2405|codestral-2405|Codestral 24.05|C|0.2000/0.6000|32K/32K|TJ|-/-/-|Previous Codestral version|chat",
        "mistral/codestral-mamba-2407|codestral-mamba|Codestral Mamba|C|0.2000/0.6000|262K/262K|TJ|-/-/-|Mamba architecture for code|chat",
        "mistral/mistral-embed|mistral-embed|Mistral Embed|C|0.1000/-|8K/1K|E|-/-/-|Text embeddings model|embed",
        "mistral/mistral-moderation-2411|mistral-moderation|Mistral Moderation|C|0.1000/-|8K/-|M|-/-/-|Content moderation model|chat",
        "mistral/open-mixtral-8x7b|mixtral-8x7b|Mixtral 8x7B|C|0.7000/0.7000|32K/32K|TJ|-/-/-|Open-weight MoE model|chat",
        "mistral/open-mixtral-8x22b|mixtral-8x22b|Mixtral 8x22B|C|2.00/6.00|65K/65K|TJ|-/-/-|Large open-weight MoE model|chat",
        "mistral/open-mistral-7b|mistral-7b|Mistral 7B|C|0.2500/0.2500|32K/32K|TJ|-/-/-|Open-weight 7B model|chat",
        "mistral/ministral-3b-2410|ministral-3b|Ministral 3B|C|0.0400/0.0400|131K/131K|TJ|-/-/-|Smallest Mistral model|chat",
        "mistral/ministral-8b-2410|ministral-8b|Ministral 8B|C|0.1000/0.1000|131K/131K|TJ|-/-/-|Efficient 8B model|chat",
        "mistral/mistral-large-latest|mistral-large-3|Mistral Large 3|C|0.5000/1.50|131K/131K|VTJS|-/-/-|675B MoE flagship model|chat",
        "mistral/mistral-medium-latest|mistral-medium-3|Mistral Medium 3|C|0.4000/2.00|131K/131K|VTJS|-/-/-|Balanced medium tier model|chat",
        "mistral/magistral-medium-latest|magistral-medium|Magistral Medium|C|2.00/5.00|131K/131K|TJK|-/-/-|Reasoning-focused medium model|chat",
        "mistral/magistral-small-latest|magistral-small|Magistral Small|C|0.5000/1.50|131K/131K|TJK|-/-/-|Reasoning-focused small model|chat",
        "mistral/devstral-medium-latest|devstral-2|Devstral 2|C|0.4000/2.00|131K/131K|TJS|-/-/-|Agentic coding model 123B|chat",
        "mistral/devstral-small-latest|devstral-small-2|Devstral Small 2|C|0.1000/0.3000|131K/131K|TJS|-/-/-|Agentic coding model 24B|chat",
        "mistral/codestral-embed-2505|codestral-embed|Codestral Embed|C|0.1500/-|8K/1K|E|-/-/-|Code embeddings model|embed",
        "mistral/voxtral-mini-latest|voxtral-mini|Voxtral Mini|C|0.1200/-|-/-|A|-/-/-|Speech transcription, $0.002/minute|audio",
        "mistral/mistral-small-latest|mistral-small-3.2|Mistral Small 3.2|C|0.1000/0.3000|32K/32K|VTJS|-/-/-|Latest small model with vision|chat",
        "mistral/ministral-3b-latest|ministral-3b-3|Ministral 3B|C|0.1000/0.1000|131K/131K|VTJ|-/-/-|Smallest Ministral with vision|chat",
        "mistral/ministral-8b-latest|ministral-8b-3|Ministral 8B|C|0.1500/0.1500|131K/131K|VTJ|-/-/-|Efficient Ministral with vision|chat",
        "mistral/mistral-large-finetuned-technical|mistral-technical|mistral-tech-ft|Mistral: Large Technical FT|C|1.50/4.50|128K/8K|VSTJ|-/-/-|Mistral Large fine-tuned for technical documentation|chat",
        "mistral/mistral-medium-finetuned-customer-service|mistral-service|mistral-service-ft|Mistral: Medium Service FT|C|0.7000/2.10|32K/4K|VSTJ|-/-/-|Mistral Medium fine-tuned for customer service|chat",
        "mistral/pixtral-large|pixtral-large|Mistral: Pixtral Large|C|0.000002/0.000006|64K/2K|VST|-/-/-|Mistral native multimodal model for advanced vision tasks|chat",
        "mistral/pixtral-12b|pixtral-12b|Mistral: Pixtral 12B|C|0.000000/0.000001|8K/1K|VST|-/-/-|Efficient 12B vision model from Mistral|chat",

    // === MISTRALAI (35 models) ===
        "mistralai/mistral-small-creative|mistral-small-creati|Mistral: Mistral Small Creative|C|0.000000/0.000000|32K/8K|T|-/-/-|Mistral Small Creative is an experimental small model designed for creative writ|chat",
        "mistralai/devstral-2512:free|devstral-2512:free|Mistral: Devstral 2 2512 (free)|C|-/-|262K/65K|JST|-/-/-|Devstral 2 is a state-of-the-art open-source model by Mistral AI specializing in|chat",
        "mistralai/devstral-2512|devstral-2512|Mistral: Devstral 2 2512|C|0.000000/0.000000|262K/65K|JST|-/-/-|Devstral 2 is a state-of-the-art open-source model by Mistral AI specializing in|chat",
        "mistralai/ministral-14b-2512|ministral-14b-2512|Mistral: Ministral 3 14B 2512|C|0.000000/0.000000|262K/65K|JSTV|-/-/-|The largest model in the Ministral 3 family, Ministral 3 14B offers frontier cap|chat",
        "mistralai/ministral-8b-2512|ministral-8b-2512|Mistral: Ministral 3 8B 2512|C|0.000000/0.000000|262K/65K|JSTV|-/-/-|A balanced model in the Ministral 3 family, Ministral 3 8B is a powerful, effici|chat",
        "mistralai/ministral-3b-2512|ministral-3b-2512|Mistral: Ministral 3 3B 2512|C|0.000000/0.000000|131K/32K|JSTV|-/-/-|The smallest model in the Ministral 3 family, Ministral 3 3B is a powerful, effi|chat",
        "mistralai/mistral-large-2512|mistral-large-2512|Mistral: Mistral Large 3 2512|C|0.000000/0.000002|262K/65K|JSTV|-/-/-|Mistral Large 3 2512 is Mistral's most capable model to date, featuring a sparse|chat",
        "mistralai/voxtral-small-24b-2507|voxtral-small-24b-25|Mistral: Voxtral Small 24B 2507|C|0.000000/0.000000|32K/8K|JST|-/-/-|Voxtral Small is an enhancement of Mistral Small 3, incorporating state-of-the-a|chat",
        "mistralai/mistral-medium-3.1|mistral-medium-3.1|Mistral: Mistral Medium 3.1|C|0.000000/0.000002|131K/32K|JSTV|-/-/-|Mistral Medium 3.1 is an updated version of Mistral Medium 3, which is a high-pe|chat",
        "mistralai/codestral-2508|codestral-2508|Mistral: Codestral 2508|C|0.000000/0.000001|256K/64K|JST|-/-/-|Mistral's cutting-edge language model for coding released end of July 2025. Code|chat",
        "mistralai/devstral-medium|devstral-medium|Mistral: Devstral Medium|C|0.000000/0.000002|131K/32K|JST|-/-/-|Devstral Medium is a high-performance code generation and agentic reasoning mode|chat",
        "mistralai/devstral-small|devstral-small|Mistral: Devstral Small 1.1|C|0.000000/0.000000|128K/32K|JST|-/-/-|Devstral Small 1.1 is a 24B parameter open-weight language model for software en|chat",
        "mistralai/mistral-small-3.2-24b-instruct|mistral-small-3.2-24|Mistral: Mistral Small 3.2 24B|C|0.000000/0.000000|131K/131K|JSTV|-/-/-|Mistral-Small-3.2-24B-Instruct-2506 is an updated 24B parameter model from Mistr|chat",
        "mistralai/devstral-small-2505|devstral-small-2505|Mistral: Devstral Small 2505|C|0.000000/0.000000|128K/32K|J|-/-/-|Devstral-Small-2505 is a 24B parameter agentic LLM fine-tuned from Mistral-Small|chat",
        "mistralai/mistral-medium-3|mistral-medium-3|Mistral: Mistral Medium 3|C|0.000000/0.000002|131K/32K|JSTV|-/-/-|Mistral Medium 3 is a high-performance enterprise-grade language model designed |chat",
        "mistralai/mistral-small-3.1-24b-instruct:free|mistral-small-3.1-24|Mistral: Mistral Small 3.1 24B (free)|C|-/-|128K/32K|JSTV|-/-/-|Mistral Small 3.1 24B Instruct is an upgraded variant of Mistral Small 3 (2501),|chat",
        "mistralai/mistral-small-3.1-24b-instruct|mistral-small-3.1-24|Mistral: Mistral Small 3.1 24B|C|0.000000/0.000000|131K/131K|JSTV|-/-/-|Mistral Small 3.1 24B Instruct is an upgraded variant of Mistral Small 3 (2501),|chat",
        "mistralai/mistral-saba|mistral-saba|Mistral: Saba|C|0.000000/0.000001|32K/8K|JST|-/-/-|Mistral Saba is a 24B-parameter language model specifically designed for the Mid|chat",
        "mistralai/mistral-small-24b-instruct-2501|mistral-small-24b-in|Mistral: Mistral Small 3|C|0.000000/0.000000|32K/32K|JST|-/-/-|Mistral Small 3 is a 24B-parameter language model optimized for low-latency perf|chat",
        "mistralai/mistral-large-2411|mistral-large-2411|Mistral Large 2411|C|0.000002/0.000006|131K/32K|JST|-/-/-|Mistral Large 2 2411 is an update of [Mistral Large 2](/mistralai/mistral-large)|chat",
        "mistralai/mistral-large-2407|mistral-large-2407|Mistral Large 2407|C|0.000002/0.000006|131K/32K|JST|-/-/-|This is Mistral AI's flagship model, Mistral Large 2 (version mistral-large-2407|chat",
        "mistralai/pixtral-large-2411|pixtral-large-2411|Mistral: Pixtral Large 2411|C|0.000002/0.000006|131K/32K|JSTV|-/-/-|Pixtral Large is a 124B parameter, open-weight, multimodal model built on top of|chat",
        "mistralai/ministral-8b|ministral-8b|Mistral: Ministral 8B|C|0.000000/0.000000|131K/32K|JST|-/-/-|Ministral 8B is an 8B parameter model featuring a unique interleaved sliding-win|chat",
        "mistralai/ministral-3b|ministral-3b|Mistral: Ministral 3B|C|0.000000/0.000000|131K/32K|JST|-/-/-|Ministral 3B is a 3B parameter model optimized for on-device and edge computing.|chat",
        "mistralai/pixtral-12b|pixtral-12b|Mistral: Pixtral 12B|C|0.000000/0.000000|32K/8K|JSTV|-/-/-|The first multi-modal, text+image-to-text model from Mistral AI. Its weights wer|chat",
        "mistralai/mistral-nemo|mistral-nemo|Mistral: Mistral Nemo|C|0.000000/0.000000|131K/16K|JST|-/-/-|A 12B parameter model with a 128k token context length built by Mistral in colla|chat",
        "mistralai/mistral-7b-instruct:free|mistral-7b-instruct:|Mistral: Mistral 7B Instruct (free)|C|-/-|32K/16K|JT|-/-/-|A high-performing, industry-standard 7.3B parameter model, with optimizations fo|chat",
        "mistralai/mistral-7b-instruct|mistral-7b-instruct|Mistral: Mistral 7B Instruct|C|0.000000/0.000000|32K/16K|JT|-/-/-|A high-performing, industry-standard 7.3B parameter model, with optimizations fo|chat",
        "mistralai/mistral-7b-instruct-v0.3|mistral-7b-instruct-|Mistral: Mistral 7B Instruct v0.3|C|0.000000/0.000000|32K/4K|-|-/-/-|A high-performing, industry-standard 7.3B parameter model, with optimizations fo|chat",
        "mistralai/mixtral-8x22b-instruct|mixtral-8x22b-instru|Mistral: Mixtral 8x22B Instruct|C|0.000002/0.000006|65K/16K|JST|-/-/-|Mistral's official instruct fine-tuned version of [Mixtral 8x22B](/models/mistra|chat",
        "mistralai/mistral-large|mistral-large|Mistral Large|C|0.000002/0.000006|128K/32K|JST|-/-/-|This is Mistral AI's flagship model, Mistral Large 2 (version 'mistral-large-240|chat",
        "mistralai/mistral-tiny|mistral-tiny|Mistral Tiny|C|0.000000/0.000000|32K/8K|JST|-/-/-|Note: This model is being deprecated. Recommended replacement is the newer [Mini|chat",
        "mistralai/mistral-7b-instruct-v0.2|mistral-7b-instruct-|Mistral: Mistral 7B Instruct v0.2|C|0.000000/0.000000|32K/8K|-|-/-/-|A high-performing, industry-standard 7.3B parameter model, with optimizations fo|chat",
        "mistralai/mixtral-8x7b-instruct|mixtral-8x7b-instruc|Mistral: Mixtral 8x7B Instruct|C|0.000001/0.000001|32K/16K|JT|-/-/-|Mixtral 8x7B Instruct is a pretrained generative Sparse Mixture of Experts, by M|chat",
        "mistralai/mistral-7b-instruct-v0.1|mistral-7b-instruct-|Mistral: Mistral 7B Instruct v0.1|C|0.000000/0.000000|2K/1K|-|-/-/-|A 7.3B parameter model that outperforms Llama 2 13B on all benchmarks, with opti|chat",

    // === MOBILE (7 models) ===
        "mobile/mobilebert|mobilebert|mobile-bert|Google: MobileBERT|C|0.000000/0.000000|512/256|S|-/-/-|MobileBERT for mobile NLP deployment|chat",
        "mobile/distilbert-mobile|distilbert-mobile|distilbert-tiny|Hugging Face: DistilBERT Mobile|C|0.000000/0.000000|512/256|S|-/-/-|Lightweight DistilBERT for edge devices|chat",
        "mobile/mobilevibert|mobilevit|mobilevit-small|Apple: MobileViT|C|0.000000/0.000000|512/256|VS|-/-/-|Vision transformer for mobile devices|chat",
        "mobile/tinybert-6l-768d|tinybert|tinybert-6l|Huawei: TinyBERT 6L|C|0.000000/0.000000|512/256|S|-/-/-|TinyBERT 6L distilled for edge inference|chat",
        "mobile/xlnet-tiny|xlnet-tiny|xlnet-mobile|Google: XLNet Tiny|C|0.000000/0.000000|512/256|S|-/-/-|Tiny XLNet for lightweight deployment|chat",
        "mobile/squeezebert|squeezebert|squeeze-bert|SqueezeBERT: Base|C|0.000000/0.000000|512/256|S|-/-/-|SqueezeBERT for efficient inference|chat",
        "mobile/mobilenetv3-small|mobilenetv3|mobilenet-small|Google: MobileNetV3 Small|C|0.000000/0.000000|224/256|V|-/-/-|MobileNetV3 Small for image classification|chat",

    // === MOONSHOT (6 models) ===
        "moonshot/kimi-k2-0711-preview|kimi-k2|Kimi K2|C|0.5500/2.00|131K/8K|VTJSK|-/-/-|1T MoE model with thinking capabilities|chat",
        "moonshot/moonshot-v1-256k|moonshot-256k|Moonshot V1 256K|C|0.8400/0.8400|262K/8K|TJS|-/-/-|256K context for long document processing|chat",
        "moonshot/moonshot-v1-128k|moonshot-128k|Moonshot V1 128K|C|0.5600/0.5600|131K/8K|TJS|-/-/-|128K context balanced model|chat",
        "moonshot/moonshot-v1-32k|moonshot-32k|Moonshot V1 32K|C|0.1700/0.1700|32K/8K|TJS|-/-/-|32K context efficient model|chat",
        "moonshot/moonshot-v1-8k|moonshot-8k|Moonshot V1 8K|C|0.0800/0.0800|8K/8K|TJS|-/-/-|8K context fast model|chat",
        "moonshot/moonshot-v1-8k-web|moonshot-web|Moonshot V1 Web|C|0.0800/0.0800|8K/8K|TJS|-/-/-|8K with web search capability|chat",

    // === MOONSHOTAI (6 models) ===
        "moonshotai/kimi-k2-thinking|kimi-k2-thinking|MoonshotAI: Kimi K2 Thinking|C|0.000000/0.000002|262K/65K|JKST|-/-/-|Kimi K2 Thinking is Moonshot AI's most advanced open reasoning model to date, ex|chat",
        "moonshotai/kimi-k2-0905|kimi-k2-0905|MoonshotAI: Kimi K2 0905|C|0.000000/0.000002|262K/262K|JST|-/-/-|Kimi K2 0905 is the September update of [Kimi K2 0711](moonshotai/kimi-k2). It i|chat",
        "moonshotai/kimi-k2-0905:exacto|kimi-k2-0905:exacto|MoonshotAI: Kimi K2 0905 (exacto)|C|0.000001/0.000003|262K/65K|JST|-/-/-|Kimi K2 0905 is the September update of [Kimi K2 0711](moonshotai/kimi-k2). It i|chat",
        "moonshotai/kimi-k2:free|kimi-k2:free|MoonshotAI: Kimi K2 0711 (free)|C|-/-|32K/8K|-|-/-/-|Kimi K2 Instruct is a large-scale Mixture-of-Experts (MoE) language model develo|chat",
        "moonshotai/kimi-k2|kimi-k2|MoonshotAI: Kimi K2 0711|C|0.000000/0.000002|131K/131K|JST|-/-/-|Kimi K2 Instruct is a large-scale Mixture-of-Experts (MoE) language model develo|chat",
        "moonshotai/kimi-dev-72b|kimi-dev-72b|MoonshotAI: Kimi Dev 72B|C|0.000000/0.000001|131K/131K|JKS|-/-/-|Kimi-Dev-72B is an open-source large language model fine-tuned for software engi|chat",

    // === MORPH (2 models) ===
        "morph/morph-v3-large|morph-v3-large|Morph: Morph V3 Large|C|0.000001/0.000002|262K/131K|-|-/-/-|Morph's high-accuracy apply model for complex code edits. ~4,500 tokens/sec with|chat",
        "morph/morph-v3-fast|morph-v3-fast|Morph: Morph V3 Fast|C|0.000001/0.000001|81K/38K|-|-/-/-|Morph's fastest apply model for code edits. ~10,500 tokens/sec with 96% accuracy|chat",

    // === MULTILINGUAL (5 models) ===
        "multilingual/xlm-roberta-large|xlm-roberta-f|XLM-RoBERTa Large|C|0.000100/0.000300|512/512|VSTJ|-/-/-|100+ language RoBERTa|chat",
        "multilingual/xlm-v|xlm-v-f|XLM-V|C|0.000120/0.000350|512/512|VSTJ|-/-/-|Vision language 100+ langs|chat",
        "multilingual/bloom-560m|bloom-560m-f|BigScience: BLOOM 560M|C|0.000050/0.000150|2K/1K|VSTJ|-/-/-|BLOOM 560M multilingual|chat",
        "multilingual/bloom-1b1|bloom-1b1-f|BigScience: BLOOM 1.1B|C|0.000100/0.000300|2K/1K|VSTJ|-/-/-|BLOOM 1.1B lightweight|chat",
        "multilingual/mpt-7b-instruct|mpt-7b-f|MPT-7B Instruct|C|0.000100/0.000300|8K/2K|VSTJ|-/-/-|MPT instruction tuned|chat",

    // === MULTIMODAL (6 models) ===
        "multimodal/clip-audio|clip-audio-m|CLIP with Audio|C|0.000200/0.000500|16K/512|VSTJ|-/-/-|Audio-visual CLIP|chat",
        "multimodal/av-hubert|av-hubert-m|Audio-Visual HuBERT|C|0.000180/0.000400|16K/512|VSTJ|-/-/-|Audio-visual pre-training|chat",
        "multimodal/vision-audio-fusion|fusion-va-m|Vision-Audio Fusion|C|0.000250/0.000600|16K/512|VSTJ|-/-/-|Unified audio-visual fusion|chat",
        "multimodal/layoutlm-v3|layoutlm-v3-m|LayoutLMv3|C|0.000150/0.000450|512/512|VSTJ|-/-/-|Document layout understanding|chat",
        "multimodal/uniter|uniter-m|UNITER|C|0.000200/0.000600|512/512|VSTJ|-/-/-|Universal image-text representation|chat",
        "multimodal/albef|albef-m|ALBEF Contrastive|C|0.000180/0.000550|256/256|VSTJ|-/-/-|Align before fusing multimodal|chat",

    // === MULTITASK (1 models) ===
        "multitask/multitask-nlp|multitask-f|MultiTask NLP|C|0.000200/0.000600|512/512|VSTJ|-/-/-|Multi-task NLP model|chat",

    // === NEBIUS (15 models) ===
        "nebius/Meta-Llama-3.1-405B-Instruct|llama-3.1-405b|Llama 3.1 405B|C|2.35/2.35|131K/16K|TJS|-/-/-|Largest Llama on Nebius|chat",
        "nebius/Meta-Llama-3.1-70B-Instruct|llama-3.1-70b|Llama 3.1 70B|C|0.3500/0.4000|131K/16K|TJS|-/-/-|Llama 3.1 70B on Nebius|chat",
        "nebius/Meta-Llama-3.1-8B-Instruct|llama-3.1-8b|Llama 3.1 8B|C|0.0600/0.0600|131K/16K|TJS|-/-/-|Llama 3.1 8B on Nebius|chat",
        "nebius/Llama-3.3-70B-Instruct|llama-3.3-70b|Llama 3.3 70B|C|0.3500/0.4000|131K/16K|TJS|-/-/-|Llama 3.3 70B on Nebius|chat",
        "nebius/Llama-3.2-90B-Vision-Instruct|llama-3.2-90b-vision|Llama 3.2 90B Vision|C|0.3500/0.4000|131K/16K|VTJS|-/-/-|Vision model on Nebius|chat",
        "nebius/Llama-3.2-11B-Vision-Instruct|llama-3.2-11b-vision|Llama 3.2 11B Vision|C|0.0600/0.0600|131K/16K|VTJS|-/-/-|Compact vision model|chat",
        "nebius/Qwen2.5-72B-Instruct|qwen-2.5-72b|Qwen 2.5 72B|C|0.4000/0.4000|131K/16K|TJS|-/-/-|Qwen 2.5 72B on Nebius|chat",
        "nebius/QwQ-32B-Preview|qwq-32b|QwQ 32B|C|0.1500/0.1500|32K/32K|TJK|-/-/-|QwQ reasoning model|chat",
        "nebius/DeepSeek-V3|deepseek-v3|DeepSeek V3|C|0.3500/0.9000|131K/16K|TJS|-/-/-|DeepSeek V3 on Nebius|chat",
        "nebius/DeepSeek-R1|deepseek-r1|DeepSeek R1|C|0.3500/2.15|131K/16K|TJK|-/-/-|DeepSeek R1 reasoning|chat",
        "nebius/Mistral-Large-Instruct-2411|mistral-large|Mistral Large|C|0.9000/2.70|131K/16K|TJS|-/-/-|Mistral Large on Nebius|chat",
        "nebius/Mistral-Nemo-Instruct-2407|mistral-nemo|Mistral Nemo|C|0.0600/0.0600|131K/16K|TJS|-/-/-|Mistral Nemo on Nebius|chat",
        "nebius/Phi-3.5-MoE-instruct|phi-3.5-moe|Phi 3.5 MoE|C|0.1200/0.1200|131K/16K|TJS|-/-/-|Microsoft Phi 3.5 MoE|chat",
        "nebius/bge-m3|bge-m3|BGE M3|C|0.0200/-|8K/1K|E|-/-/-|BGE M3 embeddings|embed",
        "nebius/bge-multilingual-gemma2|bge-gemma2|BGE Multilingual Gemma2|C|0.0200/-|8K/1K|E|-/-/-|Multilingual embeddings|embed",

    // === NEL (1 models) ===
        "nel/luke-large-finetuned-conll-2003|luke-ner|entity-linker|Studio Ousia: LUKE NER|C|0.000000/0.000000|512/256|S|-/-/-|LUKE for entity typing and linking|chat",

    // === NER (6 models) ===
        "ner/distilbert-ner|distilbert-ner-f|DistilBERT NER|C|0.000080/0.000250|512/512|VSTJ|-/-/-|DistilBERT NER token classification|chat",
        "ner/bert-large-uncased-ner|bert-ner-f|BERT Large NER|C|0.000100/0.000300|512/512|VSTJ|-/-/-|BERT NER for entities|chat",
        "ner/bert-base-multilingual-cased-ner|bert-multilingual-ner|ner-multilingual|Hugging Face: Multilingual NER|C|0.000000/0.000000|512/256|S|-/-/-|Multilingual BERT for entity recognition|chat",
        "ner/xlm-roberta-large-finetuned-conll03-english|xlm-conll|ner-xlm|XLM-RoBERTa: CoNLL03 English|C|0.000000/0.000000|512/256|S|-/-/-|XLM for multilingual NER|chat",
        "ner/biobert-base-cased-v1.1-ner|biobert-ner|bioner|BioBERT: NER|C|0.000000/0.000000|512/256|S|-/-/-|BioBERT for biomedical entity extraction|chat",
        "ner/conllpp-ner|conllpp-ner|ner-conll|CoNLL++ NER|C|0.000000/0.000000|512/256|S|-/-/-|State-of-the-art CoNLL++ NER model|chat",

    // === NEVERSLEEP (2 models) ===
        "neversleep/llama-3.1-lumimaid-8b|llama-3.1-lumimaid-8|NeverSleep: Lumimaid v0.2 8B|C|0.000000/0.000001|32K/8K|JS|-/-/-|Lumimaid v0.2 8B is a finetune of [Llama 3.1 8B](/models/meta-llama/llama-3.1-8b|chat",
        "neversleep/noromaid-20b|noromaid-20b|Noromaid 20B|C|0.000001/0.000002|4K/1K|JS|-/-/-|A collab between IkariDev and Undi. This merge is suitable for RP, ERP, and gene|chat",

    // === NEX-AGI (1 models) ===
        "nex-agi/deepseek-v3.1-nex-n1:free|deepseek-v3.1-nex-n1|Nex AGI: DeepSeek V3.1 Nex N1 (free)|C|-/-|131K/163K|JST|-/-/-|DeepSeek V3.1 Nex-N1 is the flagship release of the Nex-N1 series - a post-train|chat",

    // === NOUSRESEARCH (7 models) ===
        "nousresearch/hermes-4-70b|hermes-4-70b|Nous: Hermes 4 70B|C|0.000000/0.000000|131K/131K|JKST|-/-/-|Hermes 4 70B is a hybrid reasoning model from Nous Research, built on Meta-Llama|chat",
        "nousresearch/hermes-4-405b|hermes-4-405b|Nous: Hermes 4 405B|C|0.000000/0.000001|131K/131K|JKST|-/-/-|Hermes 4 is a large-scale reasoning model built on Meta-Llama-3.1-405B and relea|chat",
        "nousresearch/deephermes-3-mistral-24b-preview|deephermes-3-mistral|Nous: DeepHermes 3 Mistral 24B Preview|C|0.000000/0.000000|32K/32K|JKST|-/-/-|DeepHermes 3 (Mistral 24B Preview) is an instruction-tuned language model by Nou|chat",
        "nousresearch/hermes-3-llama-3.1-70b|hermes-3-llama-3.1-7|Nous: Hermes 3 70B Instruct|C|0.000000/0.000000|65K/16K|JS|-/-/-|Hermes 3 is a generalist language model with many improvements over [Hermes 2](/|chat",
        "nousresearch/hermes-3-llama-3.1-405b:free|hermes-3-llama-3.1-4|Nous: Hermes 3 405B Instruct (free)|C|-/-|131K/32K|-|-/-/-|Hermes 3 is a generalist language model with many improvements over Hermes 2, in|chat",
        "nousresearch/hermes-3-llama-3.1-405b|hermes-3-llama-3.1-4|Nous: Hermes 3 405B Instruct|C|0.000001/0.000001|131K/16K|J|-/-/-|Hermes 3 is a generalist language model with many improvements over Hermes 2, in|chat",
        "nousresearch/hermes-2-pro-llama-3-8b|hermes-2-pro-llama-3|NousResearch: Hermes 2 Pro - Llama-3 8B|C|0.000000/0.000000|8K/2K|JS|-/-/-|Hermes 2 Pro is an upgraded, retrained version of Nous Hermes 2, consisting of a|chat",

    // === NOVITA (19 models) ===
        "novita/llama-3.3-70b-instruct|llama-3.3-70b|Llama 3.3 70B|C|0.1900/0.1900|131K/8K|TJS|-/-/-|Llama 3.3 70B on Novita|chat",
        "novita/llama-3.1-405b-instruct|llama-3.1-405b|Llama 3.1 405B|C|1.00/1.00|131K/8K|TJS|-/-/-|Llama 3.1 405B on Novita|chat",
        "novita/llama-3.1-70b-instruct|llama-3.1-70b|Llama 3.1 70B|C|0.1900/0.1900|131K/8K|TJS|-/-/-|Llama 3.1 70B on Novita|chat",
        "novita/llama-3.1-8b-instruct|llama-3.1-8b|Llama 3.1 8B|C|0.0300/0.0300|131K/8K|TJS|-/-/-|Llama 3.1 8B on Novita|chat",
        "novita/deepseek-v3|deepseek-v3|DeepSeek V3|C|0.2900/0.8700|65K/8K|TJS|-/-/-|DeepSeek V3 on Novita|chat",
        "novita/deepseek-r1|deepseek-r1|DeepSeek R1|C|0.2900/1.20|65K/8K|TJK|-/-/-|DeepSeek R1 reasoning on Novita|chat",
        "novita/deepseek-r1-distill-llama-70b|deepseek-r1-70b|DeepSeek R1 Distill 70B|C|0.1900/0.1900|65K/8K|TJK|-/-/-|DeepSeek R1 distilled|chat",
        "novita/qwen-2.5-72b-instruct|qwen-2.5-72b|Qwen 2.5 72B|C|0.2900/0.2900|131K/8K|TJS|-/-/-|Qwen 2.5 72B on Novita|chat",
        "novita/qwen-2.5-coder-32b-instruct|qwen-coder-32b|Qwen 2.5 Coder 32B|C|0.1000/0.1000|131K/8K|TJS|-/-/-|Qwen Coder on Novita|chat",
        "novita/qwq-32b-preview|qwq-32b|QwQ 32B|C|0.1000/0.1000|32K/32K|TJK|-/-/-|QwQ reasoning on Novita|chat",
        "novita/mistral-large-2411|mistral-large|Mistral Large|C|0.8000/2.40|131K/8K|TJS|-/-/-|Mistral Large on Novita|chat",
        "novita/mixtral-8x22b-instruct|mixtral-8x22b|Mixtral 8x22B|C|0.2900/0.2900|65K/8K|TJS|-/-/-|Mixtral 8x22B on Novita|chat",
        "novita/codestral-2501|codestral|Codestral|C|0.1000/0.3000|262K/8K|TJS|-/-/-|Codestral on Novita|chat",
        "novita/flux-1-schnell|flux-schnell|FLUX.1 Schnell|C|0.0030/-|77/-|I|-/-/-|Fast FLUX image generation|image",
        "novita/flux-1-dev|flux-dev|FLUX.1 Dev|C|0.0250/-|77/-|I|-/-/-|FLUX Dev image generation|image",
        "novita/sdxl|sdxl|SDXL|C|0.0020/-|77/-|I|-/-/-|SDXL on Novita|image",
        "novita/sd3-medium|sd3-medium|SD3 Medium|C|0.0300/-|77/-|I|-/-/-|SD3 Medium on Novita|image",
        "novita/all-MiniLM-L6-v2|minilm-l6|MiniLM L6 v2|C|0.0100/-|512/384|E|-/-/-|Lightweight embeddings|embed",
        "novita/bge-large-en-v1.5|bge-large|BGE Large EN|C|0.0100/-|512/1K|E|-/-/-|BGE embeddings on Novita|embed",

    // === NVIDIA (9 models) ===
        "nvidia/nemotron-3-nano-30b-a3b:free|nemotron-3-nano-30b-|NVIDIA: Nemotron 3 Nano 30B A3B (free)|C|-/-|256K/64K|KT|-/-/-|NVIDIA Nemotron 3 Nano 30B A3B is a small language MoE model with highest comput|chat",
        "nvidia/nemotron-3-nano-30b-a3b|nemotron-3-nano-30b-|NVIDIA: Nemotron 3 Nano 30B A3B|C|0.000000/0.000000|262K/262K|JKST|-/-/-|NVIDIA Nemotron 3 Nano 30B A3B is a small language MoE model with highest comput|chat",
        "nvidia/nemotron-nano-12b-v2-vl:free|nemotron-nano-12b-v2|NVIDIA: Nemotron Nano 12B 2 VL (free)|C|-/-|128K/128K|KTV|-/-/-|NVIDIA Nemotron Nano 2 VL is a 12-billion-parameter open multimodal reasoning mo|chat",
        "nvidia/nemotron-nano-12b-v2-vl|nemotron-nano-12b-v2|NVIDIA: Nemotron Nano 12B 2 VL|C|0.000000/0.000001|131K/32K|JKV|-/-/-|NVIDIA Nemotron Nano 2 VL is a 12-billion-parameter open multimodal reasoning mo|chat",
        "nvidia/llama-3.3-nemotron-super-49b-v1.5|llama-3.3-nemotron-s|NVIDIA: Llama 3.3 Nemotron Super 49B V1.5|C|0.000000/0.000000|131K/32K|JKT|-/-/-|Llama-3.3-Nemotron-Super-49B-v1.5 is a 49B-parameter, English-centric reasoning/|chat",
        "nvidia/nemotron-nano-9b-v2:free|nemotron-nano-9b-v2:|NVIDIA: Nemotron Nano 9B V2 (free)|C|-/-|128K/32K|JKST|-/-/-|NVIDIA-Nemotron-Nano-9B-v2 is a large language model (LLM) trained from scratch |chat",
        "nvidia/nemotron-nano-9b-v2|nemotron-nano-9b-v2|NVIDIA: Nemotron Nano 9B V2|C|0.000000/0.000000|131K/32K|JKT|-/-/-|NVIDIA-Nemotron-Nano-9B-v2 is a large language model (LLM) trained from scratch |chat",
        "nvidia/llama-3.1-nemotron-ultra-253b-v1|llama-3.1-nemotron-u|NVIDIA: Llama 3.1 Nemotron Ultra 253B v1|C|0.000001/0.000002|131K/32K|JKS|-/-/-|Llama-3.1-Nemotron-Ultra-253B-v1 is a large language model (LLM) optimized for a|chat",
        "nvidia/llama-3.1-nemotron-70b-instruct|llama-3.1-nemotron-7|NVIDIA: Llama 3.1 Nemotron 70B Instruct|C|0.000001/0.000001|131K/16K|JT|-/-/-|NVIDIA's Llama 3.1 Nemotron 70B is a language model designed for generating prec|chat",

    // === OLLAMA (4 models) ===
        "ollama/mistral-7b-instruct-q4|mistral-7b-q4|mistral-q4|Ollama: Mistral 7B Q4|C|0.000000/0.000000|32K/4K|JT|-/-/-|Mistral 7B quantized to 4-bit for efficient inference|chat",
        "ollama/neural-chat-7b-v3-3-q5|neural-chat-q5|neural-q5|Ollama: Neural Chat Q5|C|0.000000/0.000000|8K/2K|JT|-/-/-|Neural Chat 7B quantized to 5-bit|chat",
        "ollama/llama2-7b-chat-q4|llama2-7b-q4|llama-q4|Ollama: Llama 2 7B Q4|C|0.000000/0.000000|4K/2K|JT|-/-/-|Llama 2 7B chat quantized to 4-bit|chat",
        "ollama/openchat-3.5-q4|openchat-q4|openchat-q4|Ollama: OpenChat 3.5 Q4|C|0.000000/0.000000|8K/2K|JT|-/-/-|OpenChat 3.5 quantized to 4-bit|chat",

    // === OPEN (7 models) ===
        "open/deepseek-coder-33b|deepseek-coder-33b|DeepSeek: Coder 33B|C|0.000800/0.000800|4K/2K|VSTJK|-/-/-|DeepSeek Code model 33B|chat",
        "open/codeqwen-7b|codeqwen-7b|Alibaba: CodeQwen 7B|C|0.000150/0.000150|8K/2K|VSTJK|-/-/-|Qwen specialized for code generation|chat",
        "open/starcoder2-15b|starcoder2-15b|BigCode: StarCoder2 15B|C|0.000300/0.000300|16K/2K|VSTJK|-/-/-|Next generation code model|chat",
        "open/mathstral-7b|mathstral-7b|Mistral: Mathstral 7B|C|0.000120/0.000120|8K/2K|VSTJ|-/-/-|Mistral specialized for mathematics|chat",
        "open/lol-gpt3-175b-instruct|lol-gpt3-175b|OpenAI: LOL-GPT3 175B|C|0.0010/0.0020|2K/2K|VSTJ|-/-/-|GPT3 style model 175B|chat",
        "open/llava-1.6-34b|llava-34b|NousResearch: LLaVA 1.6 34B|C|0.000800/0.000800|4K/2K|VSTJK|-/-/-|LLaVA 1.6 34B multimodal|chat",
        "open/qwen-vl-plus|qwen-vl-plus|Alibaba: Qwen VL Plus|C|0.000350/0.000350|32K/2K|VSTJK|-/-/-|Qwen Vision-Language Plus|chat",

    // === OPENAI (70 models) ===
        "openai/gpt-5.2-chat|gpt-5.2-chat|OpenAI: GPT-5.2 Chat|C|0.000002/0.000014|128K/16K|JSTV|-/-/-|GPT-5.2 Chat (AKA Instant) is the fast, lightweight member of the 5.2 family, op|chat",
        "openai/gpt-5.2-pro|gpt-5.2-pro|OpenAI: GPT-5.2 Pro|C|0.000021/0.000168|400K/128K|JKSTV|-/-/-|GPT-5.2 Pro is OpenAI's most advanced model, offering major improvements in agen|chat",
        "openai/gpt-5.2|gpt-5.2|OpenAI: GPT-5.2|C|0.000002/0.000014|400K/128K|JKSTV|-/-/-|GPT-5.2 is the latest frontier-grade model in the GPT-5 series, offering stronge|chat",
        "openai/gpt-5.1-codex-max|gpt-5.1-codex-max|OpenAI: GPT-5.1-Codex-Max|C|0.000001/0.000010|400K/128K|JKSTV|-/-/-|GPT-5.1-Codex-Max is OpenAI's latest agentic coding model, designed for long-run|chat",
        "openai/gpt-5.1|gpt-5.1|OpenAI: GPT-5.1|C|0.000001/0.000010|400K/128K|JKSTV|-/-/-|GPT-5.1 is the latest frontier-grade model in the GPT-5 series, offering stronge|chat",
        "openai/gpt-5.1-chat|gpt-5.1-chat|OpenAI: GPT-5.1 Chat|C|0.000001/0.000010|128K/16K|JSTV|-/-/-|GPT-5.1 Chat (AKA Instant is the fast, lightweight member of the 5.1 family, opt|chat",
        "openai/gpt-5.1-codex|gpt-5.1-codex|OpenAI: GPT-5.1-Codex|C|0.000001/0.000010|400K/128K|JKSTV|-/-/-|GPT-5.1-Codex is a specialized version of GPT-5.1 optimized for software enginee|chat",
        "openai/gpt-5.1-codex-mini|gpt-5.1-codex-mini|OpenAI: GPT-5.1-Codex-Mini|C|0.000000/0.000002|400K/100K|JKSTV|-/-/-|GPT-5.1-Codex-Mini is a smaller and faster version of GPT-5.1-Codex|chat",
        "openai/gpt-oss-safeguard-20b|gpt-oss-safeguard-20|OpenAI: gpt-oss-safeguard-20b|C|0.000000/0.000000|131K/65K|JKT|-/-/-|gpt-oss-safeguard-20b is a safety reasoning model from OpenAI built upon gpt-oss|chat",
        "openai/gpt-5-image-mini|gpt-5-image-mini|OpenAI: GPT-5 Image Mini|C|0.000003/0.000002|400K/128K|JKSTV|-/-/-|GPT-5 Image Mini combines OpenAI's advanced language capabilities, powered by [G|chat",
        "openai/gpt-5-image|gpt-5-image|OpenAI: GPT-5 Image|C|0.000010/0.000010|400K/128K|JKSTV|-/-/-|[GPT-5](https://openrouter.ai/openai/gpt-5) Image combines OpenAI's GPT-5 model |chat",
        "openai/o3-deep-research|o3-deep-research|OpenAI: o3 Deep Research|C|0.000010/0.000040|200K/100K|JKSTV|-/-/-|o3-deep-research is OpenAI's advanced model for deep research, designed to tackl|chat",
        "openai/o4-mini-deep-research|o4-mini-deep-researc|OpenAI: o4 Mini Deep Research|C|0.000002/0.000008|200K/100K|JKSTV|-/-/-|o4-mini-deep-research is OpenAI's faster, more affordable deep research model-id|chat",
        "openai/gpt-5-pro|gpt-5-pro|OpenAI: GPT-5 Pro|C|0.000015/0.000120|400K/128K|JKSTV|-/-/-|GPT-5 Pro is OpenAI's most advanced model, offering major improvements in reason|chat",
        "openai/gpt-5-codex|gpt-5-codex|OpenAI: GPT-5 Codex|C|0.000001/0.000010|400K/128K|JKSTV|-/-/-|GPT-5-Codex is a specialized version of GPT-5 optimized for software engineering|chat",
        "openai/gpt-4o-audio-preview|gpt-4o-audio-preview|OpenAI: GPT-4o Audio|C|0.000003/0.000010|128K/16K|JST|-/-/-|The gpt-4o-audio-preview model adds support for audio inputs as prompts. This en|chat",
        "openai/gpt-5-chat|gpt-5-chat|OpenAI: GPT-5 Chat|C|0.000001/0.000010|128K/16K|JSV|-/-/-|GPT-5 Chat is designed for advanced, natural, multimodal, and context-aware conv|chat",
        "openai/gpt-5|gpt-5|OpenAI: GPT-5|C|0.000001/0.000010|400K/128K|JKSTV|-/-/-|GPT-5 is OpenAI's most advanced model, offering major improvements in reasoning,|chat",
        "openai/gpt-5-mini|gpt-5-mini|OpenAI: GPT-5 Mini|C|0.000000/0.000002|400K/128K|JKSTV|-/-/-|GPT-5 Mini is a compact version of GPT-5, designed to handle lighter-weight reas|chat",
        "openai/gpt-5-nano|gpt-5-nano|OpenAI: GPT-5 Nano|C|0.000000/0.000000|400K/128K|JKSTV|-/-/-|GPT-5-Nano is the smallest and fastest variant in the GPT-5 system, optimized fo|chat",
        "openai/gpt-oss-120b:free|gpt-oss-120b:free|OpenAI: gpt-oss-120b (free)|C|-/-|131K/32K|KT|-/-/-|gpt-oss-120b is an open-weight, 117B-parameter Mixture-of-Experts (MoE) language|chat",
        "openai/gpt-oss-120b|gpt-oss-120b|OpenAI: gpt-oss-120b|C|0.000000/0.000000|131K/32K|JKST|-/-/-|gpt-oss-120b is an open-weight, 117B-parameter Mixture-of-Experts (MoE) language|chat",
        "openai/gpt-oss-120b:exacto|gpt-oss-120b:exacto|OpenAI: gpt-oss-120b (exacto)|C|0.000000/0.000000|131K/32K|JKST|-/-/-|gpt-oss-120b is an open-weight, 117B-parameter Mixture-of-Experts (MoE) language|chat",
        "openai/gpt-oss-20b:free|gpt-oss-20b:free|OpenAI: gpt-oss-20b (free)|C|-/-|131K/131K|JKST|-/-/-|gpt-oss-20b is an open-weight 21B parameter model released by OpenAI under the A|chat",
        "openai/gpt-oss-20b|gpt-oss-20b|OpenAI: gpt-oss-20b|C|0.000000/0.000000|131K/32K|JKST|-/-/-|gpt-oss-20b is an open-weight 21B parameter model released by OpenAI under the A|chat",
        "openai/o3-pro|o3-pro|OpenAI: o3 Pro|C|0.000020/0.000080|200K/100K|JKSTV|-/-/-|The o-series of models are trained with reinforcement learning to think before t|chat",
        "openai/codex-mini|codex-mini|OpenAI: Codex Mini|C|0.000002/0.000006|200K/100K|JKSTV|-/-/-|codex-mini-latest is a fine-tuned version of o4-mini specifically for use in Cod|chat",
        "openai/o4-mini-high|o4-mini-high|OpenAI: o4 Mini High|C|0.000001/0.000004|200K/100K|JKSTV|-/-/-|OpenAI o4-mini-high is the same model as [o4-mini](/openai/o4-mini) with reasoni|chat",
        "openai/o3|o3|OpenAI: o3|C|0.000002/0.000008|200K/100K|JKSTV|-/-/-|o3 is a well-rounded and powerful model across domains. It sets a new standard f|chat",
        "openai/o4-mini|o4-mini|OpenAI: o4 Mini|C|0.000001/0.000004|200K/100K|JKSTV|-/-/-|OpenAI o4-mini is a compact reasoning model in the o-series, optimized for fast,|chat",
        "openai/gpt-4.1|gpt-4.1|OpenAI: GPT-4.1|C|0.000002/0.000008|1047K/32K|JSTV|-/-/-|GPT-4.1 is a flagship large language model optimized for advanced instruction fo|chat",
        "openai/gpt-4.1-mini|gpt-4.1-mini|OpenAI: GPT-4.1 Mini|C|0.000000/0.000002|1047K/32K|JSTV|-/-/-|GPT-4.1 Mini is a mid-sized model delivering performance competitive with GPT-4o|chat",
        "openai/gpt-4.1-nano|gpt-4.1-nano|OpenAI: GPT-4.1 Nano|C|0.000000/0.000000|1047K/32K|JSTV|-/-/-|For tasks that demand low latency, GPT-4.1 nano is the fastest and cheapest mode|chat",
        "openai/o1-pro|o1-pro|OpenAI: o1-pro|C|0.000150/0.000600|200K/100K|JKSV|-/-/-|The o1 series of models are trained with reinforcement learning to think before |chat",
        "openai/gpt-4o-mini-search-preview|gpt-4o-mini-search-p|OpenAI: GPT-4o-mini Search Preview|C|0.000000/0.000001|128K/16K|JS|-/-/-|GPT-4o mini Search Preview is a specialized model for web search in Chat Complet|chat",
        "openai/gpt-4o-search-preview|gpt-4o-search-previe|OpenAI: GPT-4o Search Preview|C|0.000003/0.000010|128K/16K|JS|-/-/-|GPT-4o Search Previewis a specialized model for web search in Chat Completions. |chat",
        "openai/o3-mini-high|o3-mini-high|OpenAI: o3 Mini High|C|0.000001/0.000004|200K/100K|JST|-/-/-|OpenAI o3-mini-high is the same model as [o3-mini](/openai/o3-mini) with reasoni|chat",
        "openai/o3-mini|o3-mini|OpenAI: o3 Mini|C|0.000001/0.000004|200K/100K|JST|-/-/-|OpenAI o3-mini is a cost-efficient language model optimized for STEM reasoning t|chat",
        "openai/o1|o1|OpenAI: o1|C|0.000015/0.000060|200K/100K|JSTV|-/-/-|The latest and strongest model family from OpenAI, o1 is designed to spend more |chat",
        "openai/gpt-4o-2024-11-20|gpt-4o-2024-11-20|OpenAI: GPT-4o (2024-11-20)|C|0.000003/0.000010|128K/16K|JSTV|-/-/-|The 2024-11-20 version of GPT-4o offers a leveled-up creative writing ability wi|chat",
        "openai/chatgpt-4o-latest|chatgpt-4o-latest|OpenAI: ChatGPT-4o|C|0.000005/0.000015|128K/16K|JSV|-/-/-|OpenAI ChatGPT 4o is continually updated by OpenAI to point to the current versi|chat",
        "openai/gpt-4o-2024-08-06|gpt-4o-2024-08-06|OpenAI: GPT-4o (2024-08-06)|C|0.000003/0.000010|128K/16K|JSTV|-/-/-|The 2024-08-06 version of GPT-4o offers improved performance in structured outpu|chat",
        "openai/gpt-4o-mini-2024-07-18|gpt-4o-mini-2024-07-|OpenAI: GPT-4o-mini (2024-07-18)|C|0.000000/0.000001|128K/16K|JSTV|-/-/-|GPT-4o mini is OpenAI's newest model after [GPT-4 Omni](/models/openai/gpt-4o), |chat",
        "openai/gpt-4o-mini|gpt-4o-mini|OpenAI: GPT-4o-mini|C|0.000000/0.000001|128K/16K|JSTV|-/-/-|GPT-4o mini is OpenAI's newest model after [GPT-4 Omni](/models/openai/gpt-4o), |chat",
        "openai/gpt-4o-2024-05-13|gpt-4o-2024-05-13|OpenAI: GPT-4o (2024-05-13)|C|0.000005/0.000015|128K/4K|JSTV|-/-/-|GPT-4o ("o" for "omni") is OpenAI's latest AI model, supporting both text and im|chat",
        "openai/gpt-4o|gpt-4o|OpenAI: GPT-4o|C|0.000003/0.000010|128K/16K|JSTV|-/-/-|GPT-4o ("o" for "omni") is OpenAI's latest AI model, supporting both text and im|chat",
        "openai/gpt-4o:extended|gpt-4o:extended|OpenAI: GPT-4o (extended)|C|0.000006/0.000018|128K/64K|JSTV|-/-/-|GPT-4o ("o" for "omni") is OpenAI's latest AI model, supporting both text and im|chat",
        "openai/gpt-4-turbo|gpt-4-turbo|OpenAI: GPT-4 Turbo|C|0.000010/0.000030|128K/4K|JSTV|-/-/-|The latest GPT-4 Turbo model with vision capabilities. Vision requests can now u|chat",
        "openai/gpt-3.5-turbo-0613|gpt-3.5-turbo-0613|OpenAI: GPT-3.5 Turbo (older v0613)|C|0.000001/0.000002|4K/4K|JST|-/-/-|GPT-3.5 Turbo is OpenAI's fastest model. It can understand and generate natural |chat",
        "openai/gpt-4-turbo-preview|gpt-4-turbo-preview|OpenAI: GPT-4 Turbo Preview|C|0.000010/0.000030|128K/4K|JST|-/-/-|The preview GPT-4 model with improved instruction following, JSON mode, reproduc|chat",
        "openai/gpt-4-1106-preview|gpt-4-1106-preview|OpenAI: GPT-4 Turbo (older v1106)|C|0.000010/0.000030|128K/4K|JST|-/-/-|The latest GPT-4 Turbo model with vision capabilities. Vision requests can now u|chat",
        "openai/gpt-3.5-turbo-instruct|gpt-3.5-turbo-instru|OpenAI: GPT-3.5 Turbo Instruct|C|0.000002/0.000002|4K/4K|JS|-/-/-|This model is a variant of GPT-3.5 Turbo tuned for instructional prompts and omi|chat",
        "openai/gpt-3.5-turbo-16k|gpt-3.5-turbo-16k|OpenAI: GPT-3.5 Turbo 16k|C|0.000003/0.000004|16K/4K|JST|-/-/-|This model offers four times the context length of gpt-3.5-turbo, allowing it to|chat",
        "openai/gpt-4-0314|gpt-4-0314|OpenAI: GPT-4 (older v0314)|C|0.000030/0.000060|8K/4K|JST|-/-/-|GPT-4-0314 is the first version of GPT-4 released, with a context length of 8,19|chat",
        "openai/gpt-4|gpt-4|OpenAI: GPT-4|C|0.000030/0.000060|8K/4K|JST|-/-/-|OpenAI's flagship model, GPT-4 is a large-scale multimodal language model capabl|chat",
        "openai/gpt-3.5-turbo|gpt-3.5-turbo|OpenAI: GPT-3.5 Turbo|C|0.000000/0.000002|16K/4K|JST|-/-/-|GPT-3.5 Turbo is OpenAI's fastest model. It can understand and generate natural |chat",
        "openai/o1-mini|o1-mini|o1 Mini|C|1.10/4.40/0.5500|128K/65K|VTJSK|-/-/-|Fast reasoning for STEM tasks|chat",
        "openai/text-embedding-3-large|embed-3-large|Text Embedding 3 Large|C|0.1300/-|8K/3K|E|-/-/-|Most capable embedding model, 3072 dimensions|embed",
        "openai/text-embedding-3-small|embed-3-small|Text Embedding 3 Small|C|0.0200/-|8K/1K|E|-/-/-|Efficient embedding model, 1536 dimensions|embed",
        "openai/whisper-1|whisper-1|Whisper|C|0.0060/-|-/-|A|-/-/-|Speech-to-text model, $0.006/minute|audio",
        "openai/tts-1|tts-1|TTS-1|C|15.00/-|4K/-|A|-/-/-|Text-to-speech, $15/1M characters|audio",
        "openai/tts-1-hd|tts-1-hd|TTS-1 HD|C|30.00/-|4K/-|A|-/-/-|High-quality text-to-speech, $30/1M characters|audio",
        "openai/dall-e-3|dall-e-3|DALL-E 3|C|0.0400/-|4K/-|I|-/-/-|Image generation, $0.04-0.12/image|image",
        "openai/gpt-image-1|gpt-image-1|GPT Image 1|C|5.00/-|32K/-|I|-/-/-|Advanced image generation with text input|image",
        "openai/gpt-4-turbo-finetuned-medical|gpt-4-medical|gpt-4-medical-ft|OpenAI: GPT-4 Turbo Medical FT|C|3.00/12.00|128K/4K|VSTJS|-/-/-|GPT-4 Turbo fine-tuned for medical applications|chat",
        "openai/gpt-4-turbo-finetuned-code|gpt-4-code|gpt-4-code-ft|OpenAI: GPT-4 Turbo Code FT|C|3.00/12.00|128K/4K|VSTJS|-/-/-|GPT-4 Turbo fine-tuned for advanced code tasks|chat",
        "openai/gpt-4o-finetuned-instructions|gpt-4o-inst|gpt-4o-inst-ft|OpenAI: GPT-4o Instructions FT|C|3.00/12.00|128K/16K|VSTJS|-/-/-|GPT-4o fine-tuned for instruction following|chat",
        "openai/gpt-4-vision|gpt-4-vision|OpenAI: GPT-4 Vision|L|0.000010/0.000030|128K/4K|VST|-/-/-|Legacy GPT-4 with vision capability|chat",
        "openai/gpt-4-turbo-vision|gpt-4-turbo-vision|OpenAI: GPT-4 Turbo Vision|L|0.000010/0.000030|128K/4K|VST|-/-/-|GPT-4 Turbo with vision support|chat",
        "openai/gpt-4-32k-vision|gpt-4-32k-vision|OpenAI: GPT-4 32K Vision|L|0.0100/0.0300|32K/4K|VST|-/-/-|GPT-4 with limited context for vision|chat",

    // === OPENGVLAB (1 models) ===
        "opengvlab/internvl3-78b|internvl3-78b|OpenGVLab: InternVL3 78B|C|0.000000/0.000000|32K/32K|JSV|-/-/-|The InternVL3 series is an advanced multimodal large language model (MLLM). Comp|chat",

    // === OPENROUTER (2 models) ===
        "openrouter/bodybuilder|bodybuilder|Body Builder (beta)|C|-/-|128K/32K|-|-/-/-|Transform your natural language requests into structured OpenRouter API request |chat",
        "openrouter/auto|auto|Auto Router|C|-/-|2000K/500K|-|-/-/-|Your prompt will be processed by a meta-model and routed to one of dozens of mod|chat",

    // === OPENSOURCE (10 models) ===
        "opensource/mistral-nemo|nemo-f|Mistral: Nemo|C|0.000120/0.000120|8K/2K|VSTJ|-/-/-|Mistral Nemo 12B|chat",
        "opensource/mistral-7b-v0.3|mistral-v03-f|Mistral: 7B v0.3|C|0.000080/0.000080|32K/2K|VSTJ|-/-/-|Mistral 7B v0.3 latest|chat",
        "opensource/llama-3.1-405b|llama-31-405b-f|Meta: Llama 3.1 405B|C|0.0027/0.0081|131K/4K|VSTJK|-/-/-|Llama 3.1 405B ultra|chat",
        "opensource/llama-3.1-70b|llama-31-70b-f|Meta: Llama 3.1 70B|C|0.000590/0.000790|131K/4K|VSTJK|-/-/-|Llama 3.1 70B large|chat",
        "opensource/llama-3.1-8b|llama-31-8b-f|Meta: Llama 3.1 8B|C|0.000040/0.000060|131K/4K|VSTJK|-/-/-|Llama 3.1 8B small|chat",
        "opensource/llama-3.2-90b|llama-32-90b-f|Meta: Llama 3.2 90B|C|0.000810/0.000810|8K/4K|VSTJK|-/-/-|Llama 3.2 90B multimodal|chat",
        "opensource/llama-3.2-1b|llama-32-1b-f|Meta: Llama 3.2 1B|C|0.000010/0.000010|8K/4K|VSTJK|-/-/-|Llama 3.2 1B tiny|chat",
        "opensource/nous-hermes-3-70b|hermes-3-70b-f|NousResearch: Hermes 3 70B|C|0.000590/0.000790|8K/2K|VSTJ|-/-/-|Hermes 3 70B advanced|chat",
        "opensource/nous-hermes-3-405b|hermes-3-405b-f|NousResearch: Hermes 3 405B|C|0.0030/0.0090|8K/2K|VSTJ|-/-/-|Hermes 3 405B ultra|chat",
        "opensource/solstice-7b|solstice-7b-f|Solstice: 7B|C|0.000080/0.000080|4K/2K|VSTJ|-/-/-|Solstice 7B optimized|chat",

    // === OTHER (161 models) ===
        "# TOGETHER AI ADDITIONAL MODELS|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# REPLICATE EDGE & LOCAL MODELS|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# HUGGING FACE INFERENCE API|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# NEW OPEN SOURCE RELEASES|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# MULTIMODAL OPEN SOURCE|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# INFERENCE PLATFORMS (ANYSCALE| RUNPOD| VAST AI)|None|-/-|-/-|-|-/-/-||chat",
        "# SPECIALIZED REASONING MODELS|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# RETRIEVAL AUGMENTED GENERATION (RAG)|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# LONG CONTEXT MODELS|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# INSTRUCTION TUNED VARIANTS|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# SPECIALIZED TRANSLATION MODELS|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# DOMAIN-SPECIFIC EMERGING|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# MULTIMODAL AUDIO|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# VISION SPECIALIST|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# EMERGING CHINESE MODELS|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# EMERGING JAPANESE MODELS|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# LIGHTWEIGHT EDGE MODELS|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# MULTIMODEL ENSEMBLE|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# PROPRIETARY PROVIDER MODELS|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# RESEARCH LAB MODELS|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# QUANTIZED INFERENCE|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# SPARSE MODELS|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# ADAPTER & LORA VARIANTS|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# ENTERPRISE LLM VARIANTS|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# HEALTHCARE SPECIFIC MODELS|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# FINANCIAL DOMAIN SPECIFIC|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# LEGAL DOMAIN SPECIFIC|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# SCIENTIFIC RESEARCH MODELS|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# E-COMMERCE & RETAIL|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# CUSTOMER SERVICE & SUPPORT|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# CONTENT GENERATION & MARKETING|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# SOFTWARE DEVELOPMENT|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# DATA SCIENCE & ANALYTICS|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# HUMAN RESOURCES|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# MANUFACTURING & QUALITY|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# ENERGY & UTILITIES|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# EDUCATION & LEARNING|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# REAL ESTATE|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# TELECOMMUNICATIONS|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# AUTOMOTIVE|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# HOSPITALITY & TOURISM|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# GOVERNMENT & PUBLIC SECTOR|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# ADDITIONAL PROVIDER MODELS|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# ADDITIONAL ASIAN MODELS|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# ADDITIONAL OPEN SOURCE|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# CODE-SPECIFIC MODELS|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# REASONING & MATH MODELS|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# EDGE & MOBILE MODELS|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# MULTILINGUAL MODELS|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# EMBEDDING MODELS|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# RERANKER MODELS|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# CLASSIFICATION MODELS|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# SEMANTIC SEARCH|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# DOMAIN SPECIFIC LANGUAGE|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# SUMMARIZATION MODELS|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# QUESTION ANSWERING|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# SENTIMENT ANALYSIS|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# NAMED ENTITY RECOGNITION|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# PARAPHRASE MODELS|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# SEMANTIC TEXTUAL SIMILARITY|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# ZERO-SHOT CLASSIFICATION|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# LANGUAGE DETECTION|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# TOKENIZATION MODELS|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# DATA LABELING & ANNOTATION|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# SYNTHETIC DATA GENERATION|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# REINFORCEMENT LEARNING MODELS|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# CALIBRATION & UNCERTAINTY|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# KNOWLEDGE DISTILLATION|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# MULTI-TASK LEARNING|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# CROSS-LINGUAL TRANSFER|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# PROMPT OPTIMIZATION|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# IN-CONTEXT LEARNING|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# CHAIN-OF-THOUGHT|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# TREE-OF-THOUGHTS|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# CLAUDE FINE-TUNED VARIANTS|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# GPT-4 FINE-TUNED VARIANTS|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# GEMINI FINE-TUNED VARIANTS|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# MISTRAL FINE-TUNED VARIANTS|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# LLAMA FINE-TUNED VARIANTS|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# DISTILLED VARIANTS|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# SPECIALIZED FINE-TUNED MODELS|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# VISION-SPECIALIZED FINE-TUNED|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# AUDIO-SPECIALIZED FINE-TUNED|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# RETRIEVAL-SPECIALIZED FINE-TUNED|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# INSTRUCTION-TUNED VARIANTS|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# PREFERENCE-ALIGNED VARIANTS|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# CHAT-OPTIMIZED VARIANTS|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# INSTRUCTION-FOLLOWING VARIANTS|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# FINAL 1200+ MILESTONE MODELS|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# OBJECT DETECTION & LOCALIZATION|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# SEMANTIC SEGMENTATION|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# POSE ESTIMATION|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# 3D VISION|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# OPTICAL CHARACTER RECOGNITION (OCR)|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# IMAGE CLASSIFICATION|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# MEDICAL IMAGING|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# ANOMALY DETECTION|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# FACE DETECTION & RECOGNITION|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# AUTONOMOUS DRIVING (PERCEPTION)|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# SATELLITE & AERIAL IMAGING|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# VIDEO UNDERSTANDING|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# SPEECH RECOGNITION (ASR)|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# TEXT-TO-SPEECH (TTS)|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# MUSIC UNDERSTANDING & GENERATION|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# AUDIO SOURCE SEPARATION|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# SOUND CLASSIFICATION & TAGGING|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# AUDIO FINGERPRINTING|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# SPEAKER RECOGNITION|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# EMOTION RECOGNITION (SPEECH & AUDIO)|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# MULTIMODAL INTEGRATION (AUDIO-VISUAL)|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# SPECIALIZED MULTIMODAL|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# ENTERPRISE PREMIUM MODELS|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# SPECIALIZED CODING MODELS|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# RESEARCH & ADVANCED MODELS|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# REASONING OPTIMIZED|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# VISION PREMIUM|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# MULTIMODAL PREMIUM|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# MULTILINGUAL PREMIUM|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# DOMAIN EXPERT PREMIUM|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# SPEED OPTIMIZED|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# ACCURACY FOCUSED|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# CONTEXT EXTENDED|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# FINE-TUNING OPTIMIZED|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# SAFETY CERTIFIED|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# COMPLIANCE READY|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# CHINESE REGION MODELS|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# INDIAN REGION MODELS|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# LATIN AMERICAN REGION MODELS|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# JAPANESE REGION MODELS|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# KOREAN REGION MODELS|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# SOUTHEAST ASIAN MODELS|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# MIDDLE EAST & AFRICA MODELS|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# MEDICAL DOMAIN MODELS|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# LEGAL DOMAIN MODELS|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# FINANCIAL DOMAIN MODELS|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# CODE GENERATION MODELS|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# REASONING & MATHS MODELS|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# DIALOGUE & CONVERSATIONAL MODELS|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# TRANSLATION MODELS|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# AUDIO & SPEECH MODELS|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# RETRIEVAL & EMBEDDING MODELS|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# QUESTION ANSWERING SPECIALIZED|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# SENTIMENT & EMOTION ANALYSIS|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# NER & SEQUENCE LABELING|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# ZERO-SHOT & FEW-SHOT|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# SUMMARIZATION SPECIALIZED|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# PARAPHRASE & SEMANTIC SIMILARITY|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# INFORMATION RETRIEVAL & RANKING|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# TEXT CLASSIFICATION VARIANTS|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# NAMED ENTITY LINKING|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# COREFERENCE RESOLUTION|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# DEPENDENCY PARSING|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# CONSTITUENCY PARSING|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# SEMANTIC ROLE LABELING|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# DOCUMENT UNDERSTANDING|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# GRAPH NEURAL NETWORKS|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# RECOMMENDATION SYSTEMS|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# RANKING & LEARNING-TO-RANK|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# OBJECT DETECTION|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# INSTANCE SEGMENTATION|None|None|None|-/-|-/-|-|-/-/-||chat",
        "# PANOPTIC SEGMENTATION|None|None|None|-/-|-/-|-|-/-/-||chat",

    // === PARAPHRASE (1 models) ===
        "paraphrase/paraphrase-multilingual|paraphrase-f|Paraphrase Multilingual|C|0.000100/0.000300|512/512|VSTJ|-/-/-|Multilingual paraphrasing|chat",

    // === PARSING (2 models) ===
        "parsing/uddaptan-en-ud24-en_ewt-small|ddparser-en|parse-english|UDDaptation: English Parser|C|0.000000/0.000000|512/256|S|-/-/-|Small model for dependency parsing|chat",
        "parsing/constituency-small|constituency-parse|parse-constituency|Stanford: Constituency Small|C|0.000000/0.000000|512/256|S|-/-/-|Lightweight constituency parser|chat",

    // === PERPLEXITY (7 models) ===
        "perplexity/sonar-pro-search|sonar-pro-search|Perplexity: Sonar Pro Search|C|0.000003/0.000015|200K/8K|JKSV|-/-/-|Exclusively available on the OpenRouter API, Sonar Pro's new Pro Search mode is |chat",
        "perplexity/sonar-reasoning-pro|sonar-reasoning-pro|Perplexity: Sonar Reasoning Pro|C|0.000002/0.000008|128K/32K|KV|-/-/-|Note: Sonar Pro pricing includes Perplexity search pricing. See [details here](h|chat",
        "perplexity/sonar-pro|sonar-pro|Perplexity: Sonar Pro|C|0.000003/0.000015|200K/8K|V|-/-/-|Note: Sonar Pro pricing includes Perplexity search pricing. See [details here](h|chat",
        "perplexity/sonar-deep-research|sonar-deep-research|Perplexity: Sonar Deep Research|C|0.000002/0.000008|128K/32K|K|-/-/-|Sonar Deep Research is a research-focused model designed for multi-step retrieva|chat",
        "perplexity/sonar-reasoning|sonar-reasoning|Perplexity: Sonar Reasoning|C|0.000001/0.000005|127K/31K|K|-/-/-|Sonar Reasoning is a reasoning model provided by Perplexity based on [DeepSeek R|chat",
        "perplexity/sonar|sonar|Perplexity: Sonar|C|0.000001/0.000001|127K/31K|V|-/-/-|Sonar is lightweight, affordable, fast, and simple to use - now featuring citati|chat",
        "perplexity/r1-1776|r1-1776|R1 1776|C|2.00/8.00|127K/8K|TJK|-/-/-|Post-trained reasoning model|chat",

    // === PREMIUM (43 models) ===
        "premium/claude-opus-4-enterprise-ultra|claude-opus-ultra-p|Anthropic: Claude Opus 4 Enterprise Ultra|C|10.00/50.00/1.00|200K/32K|VSTJSKC|-/-/-|Claude Opus 4 maximum tier|chat",
        "premium/gpt-4-turbo-enterprise|gpt-4-turbo-ent-p|OpenAI: GPT-4 Turbo Enterprise|C|0.0020/0.0060/0.0010|128K/16K|VSTJK|-/-/-|GPT-4 Turbo enterprise edition|chat",
        "premium/gemini-2-ultra|gemini-2-ultra-p|Google: Gemini 2 Ultra|C|0.0200/0.0800|1000K/8K|VSTJK|-/-/-|Gemini 2 ultra performance|chat",
        "premium/mistral-large-2.5|mistral-large-25-p|Mistral: Large 2.5|C|0.0020/0.0060|128K/8K|VSTJ|-/-/-|Mistral Large 2.5 advanced|chat",
        "premium/claude-opus-code|claude-code-p|Anthropic: Claude Opus Code|C|6.00/30.00/0.6000|200K/32K|VSTJKC|-/-/-|Claude optimized for coding|chat",
        "premium/gpt-4-code-interpreter|gpt-4-code-p|OpenAI: GPT-4 Code Interpreter|C|0.0030/0.0090|128K/16K|VSTJK|-/-/-|GPT-4 with code execution|chat",
        "premium/codegemini-2|codegemini-2-p|Google: Code Gemini 2|C|0.0075/0.0300|1000K/8K|VSTJK|-/-/-|Gemini 2 code optimization|chat",
        "premium/mistral-code-large|mistral-code-large-p|Mistral: Code Large|C|0.0010/0.0030|128K/8K|VSTJK|-/-/-|Mistral specialized code|chat",
        "premium/claude-haiku-research|claude-haiku-research-p|Anthropic: Claude Haiku Research|C|0.000080/0.000400|200K/4K|VSTJKC|-/-/-|Haiku research variant|chat",
        "premium/gpt-4-mini-advanced|gpt-4-mini-adv-p|OpenAI: GPT-4 Mini Advanced|C|0.000075/0.000300|128K/16K|VSTJK|-/-/-|GPT-4 Mini advanced version|chat",
        "premium/gemini-flash-advanced|gemini-flash-adv-p|Google: Gemini Flash Advanced|C|0.000750/0.0030|1000K/8K|VSTJK|-/-/-|Gemini Flash advanced|chat",
        "premium/claude-opus-reasoning|claude-reasoning-p|Anthropic: Claude Opus Reasoning|C|5.00/25.00/0.5000|200K/32K|VSTJSKC|-/-/-|Claude with enhanced reasoning|chat",
        "premium/gpt-4-reasoning|gpt-4-reasoning-p|OpenAI: GPT-4 Reasoning|C|0.0030/0.0090|128K/16K|VSTJK|-/-/-|GPT-4 reasoning enhanced|chat",
        "premium/gemini-reasoning-pro|gemini-reason-pro-p|Google: Gemini Reasoning Pro|C|0.0100/0.0400|1000K/8K|VSTJK|-/-/-|Gemini with reasoning|chat",
        "premium/gpt-4-vision-ultra|gpt-4-vision-ultra-p|OpenAI: GPT-4 Vision Ultra|C|0.0025/0.0075|128K/4K|VSTJK|-/-/-|GPT-4 Vision ultra resolution|chat",
        "premium/gemini-vision-ultra|gemini-vision-ultra-p|Google: Gemini Vision Ultra|C|0.0075/0.0300|1000K/8K|VSTJK|-/-/-|Gemini Vision maximum|chat",
        "premium/claude-opus-vision|claude-opus-vision-p|Anthropic: Claude Opus Vision|C|5.00/25.00/0.5000|200K/4K|VSTJSKC|-/-/-|Claude with vision capabilities|chat",
        "premium/multimodal-fusion-ultra|fusion-ultra-p|ModelSuite: Multimodal Fusion Ultra|C|0.0100/0.0300|128K/8K|VSTJKC|-/-/-|Ultimate multimodal integration|chat",
        "premium/vision-language-pro|vlm-pro-p|ModelSuite: Vision-Language Pro|C|0.0080/0.0240|100K/4K|VSTJK|-/-/-|Pro multimodal synthesis|chat",
        "premium/claude-opus-multilingual|claude-multi-p|Anthropic: Claude Opus Multilingual|C|5.50/27.50/0.5500|200K/32K|VSTJKC|-/-/-|Claude multilingual expert|chat",
        "premium/gpt-4-multilingual|gpt-4-multi-p|OpenAI: GPT-4 Multilingual|C|0.0020/0.0060|128K/16K|VSTJ|-/-/-|GPT-4 language expert|chat",
        "premium/gemini-multilingual-pro|gemini-multi-pro-p|Google: Gemini Multilingual Pro|C|0.0090/0.0360|1000K/8K|VSTJ|-/-/-|Gemini language specialist|chat",
        "premium/claude-legal-pro|claude-legal-pro-p|Anthropic: Claude Legal Pro|C|7.00/35.00/0.7000|200K/32K|VSTJKC|-/-/-|Claude legal expertise|chat",
        "premium/gpt-4-medical-pro|gpt-4-medical-pro-p|OpenAI: GPT-4 Medical Pro|C|0.0040/0.0120|128K/16K|VSTJK|-/-/-|GPT-4 medical specialist|chat",
        "premium/gemini-financial-pro|gemini-fin-pro-p|Google: Gemini Financial Pro|C|0.0100/0.0400|1000K/8K|VSTJ|-/-/-|Gemini finance expert|chat",
        "premium/claude-opus-turbo|claude-turbo-p|Anthropic: Claude Opus Turbo|C|6.00/30.00/0.6000|200K/32K|VSTJKC|-/-/-|Claude optimized for speed|chat",
        "premium/gpt-4-fast|gpt-4-fast-p|OpenAI: GPT-4 Fast|C|0.0015/0.0045|128K/16K|VSTJ|-/-/-|GPT-4 low latency|chat",
        "premium/gemini-express|gemini-express-p|Google: Gemini Express|C|0.0050/0.0200|1000K/8K|VSTJ|-/-/-|Gemini rapid response|chat",
        "premium/claude-opus-precise|claude-precise-p|Anthropic: Claude Opus Precise|C|7.00/35.00/0.7000|200K/32K|VSTJKC|-/-/-|Claude maximum precision|chat",
        "premium/gpt-4-accurate|gpt-4-accurate-p|OpenAI: GPT-4 Accurate|C|0.0040/0.0120|128K/16K|VSTJK|-/-/-|GPT-4 high accuracy|chat",
        "premium/gemini-precise-pro|gemini-precise-pro-p|Google: Gemini Precise Pro|C|0.0120/0.0480|1000K/8K|VSTJ|-/-/-|Gemini precision mode|chat",
        "premium/claude-opus-context-max|claude-context-max-p|Anthropic: Claude Opus Context Max|C|6.00/30.00/0.6000|400K/32K|VSTJKC|-/-/-|Claude 400K context|chat",
        "premium/gpt-4-context-ultra|gpt-4-context-ultra-p|OpenAI: GPT-4 Context Ultra|C|0.0030/0.0090|256K/16K|VSTJK|-/-/-|GPT-4 ultra long context|chat",
        "premium/gemini-context-giant|gemini-context-giant-p|Google: Gemini Context Giant|C|0.0150/0.0600|2000K/8K|VSTJ|-/-/-|Gemini 2M context|chat",
        "premium/claude-tuning-pro|claude-tuning-pro-p|Anthropic: Claude Tuning Pro|C|5.50/27.50/0.5500|200K/32K|VSTJKC|-/-/-|Claude tuning-optimized|chat",
        "premium/gpt-4-tuning-pro|gpt-4-tuning-pro-p|OpenAI: GPT-4 Tuning Pro|C|0.0025/0.0075|128K/16K|VSTJK|-/-/-|GPT-4 tuning enhanced|chat",
        "premium/gemini-tuning-elite|gemini-tuning-elite-p|Google: Gemini Tuning Elite|C|0.0085/0.0340|1000K/8K|VSTJ|-/-/-|Gemini tuning specialist|chat",
        "premium/claude-opus-safe|claude-safe-p|Anthropic: Claude Opus Safe|C|6.50/32.50/0.6500|200K/32K|VSTJKC|-/-/-|Claude with safety focus|chat",
        "premium/gpt-4-guardrails|gpt-4-guardrails-p|OpenAI: GPT-4 Guardrails|C|0.0030/0.0090|128K/16K|VSTJK|-/-/-|GPT-4 with guardrails|chat",
        "premium/gemini-safety-pro|gemini-safety-pro-p|Google: Gemini Safety Pro|C|0.0090/0.0360|1000K/8K|VSTJ|-/-/-|Gemini safety certified|chat",
        "premium/claude-opus-compliant|claude-compliant-p|Anthropic: Claude Opus Compliant|C|7.00/35.00/0.7000|200K/32K|VSTJKC|-/-/-|Claude compliance-ready|chat",
        "premium/gpt-4-regulatory|gpt-4-regulatory-p|OpenAI: GPT-4 Regulatory|C|0.0040/0.0120|128K/16K|VSTJK|-/-/-|GPT-4 regulatory compliant|chat",
        "premium/gemini-compliance-plus|gemini-compliance-plus-p|Google: Gemini Compliance Plus|C|0.0100/0.0400|1000K/8K|VSTJ|-/-/-|Gemini compliance assured|chat",

    // === PRIME-INTELLECT (1 models) ===
        "prime-intellect/intellect-3|intellect-3|Prime Intellect: INTELLECT-3|C|0.000000/0.000001|131K/131K|JKST|-/-/-|INTELLECT-3 is a 106B-parameter Mixture-of-Experts model (12B active) post-train|chat",

    // === PROMPT (1 models) ===
        "prompt/prompt-optimizer|prompt-opt-f|Prompt Optimizer|C|0.000200/0.000600|4K/1K|VSTJ|-/-/-|Prompt optimization model|chat",

    // === PROVIDER (8 models) ===
        "provider/xi-api-nova|xi-api-nova|ElevenLabs: Nova Speech|C|0.000020/0.000060|1K/1|VT|-/-/-|ElevenLabs Nova speech synthesis|chat",
        "provider/playai-tts|playai-tts|Play.ai: TTS Model|C|0.000015/0.000045|1K/1|VT|-/-/-|Play.ai ultra-realistic text-to-speech|chat",
        "provider/cohere-command-r|cohere-r-f|Cohere: Command R|C|0.000500/0.0015|4K/2K|VSTJ|-/-/-|Cohere Command R optimized model|chat",
        "provider/cohere-command-r-plus|cohere-r-plus-f|Cohere: Command R Plus|C|0.0030/0.0100|4K/2K|VSTJ|-/-/-|Cohere Command R Plus extended|chat",
        "provider/ai21-jamba|jamba-f|AI21: Jamba|C|0.000500/0.0015|8K/2K|VSTJ|-/-/-|Jamba hybrid SSM-Transformer|chat",
        "provider/ai21-jamba-instruct|jamba-instruct-f|AI21: Jamba Instruct|C|0.000500/0.0015|8K/2K|VSTJ|-/-/-|Jamba instruction tuned|chat",
        "provider/writer-palmyra|palmyra-f|Writer: Palmyra|C|0.000300/0.000900|4K/2K|VSTJ|-/-/-|Palmyra open model|chat",
        "provider/stability-stablediffusion3|stable-diffusion-3-f|Stability: Stable Diffusion 3|C|0.000150/0.000450|1K/1K|VT|-/-/-|Latent diffusion 3 generation|chat",

    // === QA (5 models) ===
        "qa/electra-large-qa|electra-qa-f|ELECTRA Large QA|C|0.000100/0.000300|512/256|VSTJ|-/-/-|ELECTRA question answering|chat",
        "qa/deberta-large-qa|deberta-qa-f|DeBERTa Large QA|C|0.000120/0.000350|512/256|VSTJ|-/-/-|DeBERTa QA extraction|chat",
        "qa/deepset-bert-base-uncased-squad2|deepset-squad|qa-bert|Deepset: BERT Base SQuAD2|C|0.000000/0.000000|512/256|S|-/-/-|BERT fine-tuned for SQuAD 2.0 QA|chat",
        "qa/mrqa-small|mrqa-small|qa-mrqa|IBM: MRQA Small|C|0.000000/0.000000|512/256|S|-/-/-|MRQA multi-dataset question answering|chat",
        "qa/electra-base-discriminator-qa|electra-qa|electra-squad|Google: ELECTRA Base QA|C|0.000000/0.000000|512/256|S|-/-/-|ELECTRA tuned for extractive QA|chat",

    // === QUANTIZED (2 models) ===
        "quantized/llama-3-70b-q4|llama-70b-q4|Meta: Llama 3 70B Q4|C|0.000400/0.000400|8K/2K|VST|-/-/-|Llama 3 70B quantized to 4-bit|chat",
        "quantized/mistral-8x22b-q5|mixtral-22b-q5|Mistral: Mixtral 8x22B Q5|C|0.000500/0.000500|32K/2K|VST|-/-/-|Mixtral 8x22B quantized to 5-bit|chat",

    // === QWEN (59 models) ===
        "qwen/qwen3-vl-32b-instruct|qwen3-vl-32b-instruc|Qwen: Qwen3 VL 32B Instruct|C|0.000000/0.000002|262K/65K|JSV|-/-/-|Qwen3-VL-32B-Instruct is a large-scale multimodal vision-language model designed|chat",
        "qwen/qwen3-vl-8b-thinking|qwen3-vl-8b-thinking|Qwen: Qwen3 VL 8B Thinking|C|0.000000/0.000002|256K/32K|JKSTV|-/-/-|Qwen3-VL-8B-Thinking is the reasoning-optimized variant of the Qwen3-VL-8B multi|chat",
        "qwen/qwen3-vl-8b-instruct|qwen3-vl-8b-instruct|Qwen: Qwen3 VL 8B Instruct|C|0.000000/0.000000|131K/32K|JSTV|-/-/-|Qwen3-VL-8B-Instruct is a multimodal vision-language model from the Qwen3-VL ser|chat",
        "qwen/qwen3-vl-30b-a3b-thinking|qwen3-vl-30b-a3b-thi|Qwen: Qwen3 VL 30B A3B Thinking|C|0.000000/0.000001|131K/32K|JKSTV|-/-/-|Qwen3-VL-30B-A3B-Thinking is a multimodal model that unifies strong text generat|chat",
        "qwen/qwen3-vl-30b-a3b-instruct|qwen3-vl-30b-a3b-ins|Qwen: Qwen3 VL 30B A3B Instruct|C|0.000000/0.000001|262K/65K|JSTV|-/-/-|Qwen3-VL-30B-A3B-Instruct is a multimodal model that unifies strong text generat|chat",
        "qwen/qwen3-vl-235b-a22b-thinking|qwen3-vl-235b-a22b-t|Qwen: Qwen3 VL 235B A22B Thinking|C|0.000000/0.000001|262K/262K|JKSTV|-/-/-|Qwen3-VL-235B-A22B Thinking is a multimodal model that unifies strong text gener|chat",
        "qwen/qwen3-vl-235b-a22b-instruct|qwen3-vl-235b-a22b-i|Qwen: Qwen3 VL 235B A22B Instruct|C|0.000000/0.000001|262K/65K|JSTV|-/-/-|Qwen3-VL-235B-A22B Instruct is an open-weight multimodal model that unifies stro|chat",
        "qwen/qwen3-max|qwen3-max|Qwen: Qwen3 Max|C|0.000001/0.000006|256K/32K|JT|-/-/-|Qwen3-Max is an updated release built on the Qwen3 series, offering major improv|chat",
        "qwen/qwen3-coder-plus|qwen3-coder-plus|Qwen: Qwen3 Coder Plus|C|0.000001/0.000005|128K/65K|JST|-/-/-|Qwen3 Coder Plus is Alibaba's proprietary version of the Open Source Qwen3 Coder|chat",
        "qwen/qwen3-coder-flash|qwen3-coder-flash|Qwen: Qwen3 Coder Flash|C|0.000000/0.000002|128K/65K|JT|-/-/-|Qwen3 Coder Flash is Alibaba's fast and cost efficient version of their propriet|chat",
        "qwen/qwen3-next-80b-a3b-thinking|qwen3-next-80b-a3b-t|Qwen: Qwen3 Next 80B A3B Thinking|C|0.000000/0.000001|262K/262K|JKST|-/-/-|Qwen3-Next-80B-A3B-Thinking is a reasoning-first chat model in the Qwen3-Next li|chat",
        "qwen/qwen3-next-80b-a3b-instruct|qwen3-next-80b-a3b-i|Qwen: Qwen3 Next 80B A3B Instruct|C|0.000000/0.000001|262K/65K|JST|-/-/-|Qwen3-Next-80B-A3B-Instruct is an instruction-tuned chat model in the Qwen3-Next|chat",
        "qwen/qwen-plus-2025-07-28|qwen-plus-2025-07-28|Qwen: Qwen Plus 0728|C|0.000000/0.000001|1000K/32K|JST|-/-/-|Qwen Plus 0728, based on the Qwen3 foundation model, is a 1 million context hybr|chat",
        "qwen/qwen-plus-2025-07-28:thinking|qwen-plus-2025-07-28|Qwen: Qwen Plus 0728 (thinking)|C|0.000000/0.000004|1000K/32K|JKST|-/-/-|Qwen Plus 0728, based on the Qwen3 foundation model, is a 1 million context hybr|chat",
        "qwen/qwen3-30b-a3b-thinking-2507|qwen3-30b-a3b-thinki|Qwen: Qwen3 30B A3B Thinking 2507|C|0.000000/0.000000|32K/8K|JKST|-/-/-|Qwen3-30B-A3B-Thinking-2507 is a 30B parameter Mixture-of-Experts reasoning mode|chat",
        "qwen/qwen3-coder-30b-a3b-instruct|qwen3-coder-30b-a3b-|Qwen: Qwen3 Coder 30B A3B Instruct|C|0.000000/0.000000|160K/32K|JST|-/-/-|Qwen3-Coder-30B-A3B-Instruct is a 30.5B parameter Mixture-of-Experts (MoE) model|chat",
        "qwen/qwen3-30b-a3b-instruct-2507|qwen3-30b-a3b-instru|Qwen: Qwen3 30B A3B Instruct 2507|C|0.000000/0.000000|262K/262K|JST|-/-/-|Qwen3-30B-A3B-Instruct-2507 is a 30.5B-parameter mixture-of-experts language mod|chat",
        "qwen/qwen3-235b-a22b-thinking-2507|qwen3-235b-a22b-thin|Qwen: Qwen3 235B A22B Thinking 2507|C|0.000000/0.000001|262K/262K|JKST|-/-/-|Qwen3-235B-A22B-Thinking-2507 is a high-performance, open-weight Mixture-of-Expe|chat",
        "qwen/qwen3-coder:free|qwen3-coder:free|Qwen: Qwen3 Coder 480B A35B (free)|C|-/-|262K/262K|T|-/-/-|Qwen3-Coder-480B-A35B-Instruct is a Mixture-of-Experts (MoE) code generation mod|chat",
        "qwen/qwen3-coder|qwen3-coder|Qwen: Qwen3 Coder 480B A35B|C|0.000000/0.000001|262K/262K|JKST|-/-/-|Qwen3-Coder-480B-A35B-Instruct is a Mixture-of-Experts (MoE) code generation mod|chat",
        "qwen/qwen3-coder:exacto|qwen3-coder:exacto|Qwen: Qwen3 Coder 480B A35B (exacto)|C|0.000000/0.000002|262K/65K|JKST|-/-/-|Qwen3-Coder-480B-A35B-Instruct is a Mixture-of-Experts (MoE) code generation mod|chat",
        "qwen/qwen3-235b-a22b-2507|qwen3-235b-a22b-2507|Qwen: Qwen3 235B A22B Instruct 2507|C|0.000000/0.000000|262K/65K|JKST|-/-/-|Qwen3-235B-A22B-Instruct-2507 is a multilingual, instruction-tuned mixture-of-ex|chat",
        "qwen/qwen3-4b:free|qwen3-4b:free|Qwen: Qwen3 4B (free)|C|-/-|40K/10K|JKST|-/-/-|Qwen3-4B is a 4 billion parameter dense language model from the Qwen3 series, de|chat",
        "qwen/qwen3-30b-a3b|qwen3-30b-a3b|Qwen: Qwen3 30B A3B|C|0.000000/0.000000|40K/40K|JKST|-/-/-|Qwen3, the latest generation in the Qwen large language model series, features b|chat",
        "qwen/qwen3-8b|qwen3-8b|Qwen: Qwen3 8B|C|0.000000/0.000000|128K/20K|JKST|-/-/-|Qwen3-8B is a dense 8.2B parameter causal language model from the Qwen3 series, |chat",
        "qwen/qwen3-14b|qwen3-14b|Qwen: Qwen3 14B|C|0.000000/0.000000|40K/40K|JKST|-/-/-|Qwen3-14B is a dense 14.8B parameter causal language model from the Qwen3 series|chat",
        "qwen/qwen3-32b|qwen3-32b|Qwen: Qwen3 32B|C|0.000000/0.000000|40K/40K|JKST|-/-/-|Qwen3-32B is a dense 32.8B parameter causal language model from the Qwen3 series|chat",
        "qwen/qwen3-235b-a22b|qwen3-235b-a22b|Qwen: Qwen3 235B A22B|C|0.000000/0.000001|40K/40K|JKST|-/-/-|Qwen3-235B-A22B is a 235B parameter mixture-of-experts (MoE) model developed by |chat",
        "qwen/qwen2.5-coder-7b-instruct|qwen2.5-coder-7b-ins|Qwen: Qwen2.5 Coder 7B Instruct|C|0.000000/0.000000|32K/8K|JS|-/-/-|Qwen2.5-Coder-7B-Instruct is a 7B parameter instruction-tuned language model opt|chat",
        "qwen/qwen2.5-vl-32b-instruct|qwen2.5-vl-32b-instr|Qwen: Qwen2.5 VL 32B Instruct|C|0.000000/0.000000|16K/16K|JSV|-/-/-|Qwen2.5-VL-32B is a multimodal vision-language model fine-tuned through reinforc|chat",
        "qwen/qwq-32b|qwq-32b|Qwen: QwQ 32B|C|0.000000/0.000000|32K/8K|JKST|-/-/-|QwQ is the reasoning model of the Qwen series. Compared with conventional instru|chat",
        "qwen/qwen-vl-plus|qwen-vl-plus|Qwen: Qwen VL Plus|C|0.000000/0.000001|7K/1K|JV|-/-/-|Qwen's Enhanced Large Visual Language Model. Significantly upgraded for detailed|chat",
        "qwen/qwen-vl-max|qwen-vl-max|Qwen: Qwen VL Max|C|0.000001/0.000003|131K/8K|JTV|-/-/-|Qwen VL Max is a visual understanding model with 7500 tokens context length. It |chat",
        "qwen/qwen-turbo|qwen-turbo|Qwen: Qwen-Turbo|C|0.000000/0.000000|1000K/8K|JT|-/-/-|Qwen-Turbo, based on Qwen2.5, is a 1M context model that provides fast speed and|chat",
        "qwen/qwen2.5-vl-72b-instruct|qwen2.5-vl-72b-instr|Qwen: Qwen2.5 VL 72B Instruct|C|0.000000/0.000000|32K/32K|JSV|-/-/-|Qwen2.5-VL is proficient in recognizing common objects such as flowers, birds, f|chat",
        "qwen/qwen-plus|qwen-plus|Qwen: Qwen-Plus|C|0.000000/0.000001|131K/8K|JT|-/-/-|Qwen-Plus, based on the Qwen2.5 foundation model, is a 131K context model with a|chat",
        "qwen/qwen-max|qwen-max|Qwen: Qwen-Max|C|0.000002/0.000006|32K/8K|JT|-/-/-|Qwen-Max, based on Qwen2.5, provides the best inference performance among [Qwen |chat",
        "qwen/qwen-2.5-coder-32b-instruct|qwen-2.5-coder-32b-i|Qwen2.5 Coder 32B Instruct|C|0.000000/0.000000|32K/32K|JS|-/-/-|Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (|chat",
        "qwen/qwen-2.5-7b-instruct|qwen-2.5-7b-instruct|Qwen: Qwen2.5 7B Instruct|C|0.000000/0.000000|32K/8K|-|-/-/-|Qwen2.5 7B is the latest series of Qwen large language models. Qwen2.5 brings th|chat",
        "qwen/qwen-2.5-72b-instruct|qwen-2.5-72b-instruc|Qwen2.5 72B Instruct|C|0.000000/0.000000|32K/16K|JST|-/-/-|Qwen2.5 72B is the latest series of Qwen large language models. Qwen2.5 brings t|chat",
        "qwen/qwen-2.5-vl-7b-instruct:free|qwen-2.5-vl-7b-instr|Qwen: Qwen2.5-VL 7B Instruct (free)|C|-/-|32K/8K|V|-/-/-|Qwen2.5 VL 7B is a multimodal LLM from the Qwen Team with the following key enha|chat",
        "qwen/qwen-2.5-vl-7b-instruct|qwen-2.5-vl-7b-instr|Qwen: Qwen2.5-VL 7B Instruct|C|0.000000/0.000000|32K/8K|V|-/-/-|Qwen2.5 VL 7B is a multimodal LLM from the Qwen Team with the following key enha|chat",
        "qwen/qwen2.5-72b-instruct|qwen2.5-72b|Qwen 2.5 72B|C|0.5500/1.65|131K/8K|VTJS|-/-/-|Flagship 72B instruct model|chat",
        "qwen/qwen2.5-32b-instruct|qwen2.5-32b|Qwen 2.5 32B|C|0.2800/0.5500|131K/8K|VTJS|-/-/-|Balanced 32B model|chat",
        "qwen/qwen2.5-14b-instruct|qwen2.5-14b|Qwen 2.5 14B|C|0.0550/0.1100|131K/8K|TJS|-/-/-|Efficient 14B model|chat",
        "qwen/qwen2.5-7b-instruct|qwen2.5-7b|Qwen 2.5 7B|C|0.0140/0.0410|131K/8K|TJS|-/-/-|Compact 7B model|chat",
        "qwen/qwen2.5-3b-instruct|qwen2.5-3b|Qwen 2.5 3B|C|0.0042/0.0083|32K/8K|TJ|-/-/-|Tiny 3B model|chat",
        "qwen/qwen2.5-coder-32b-instruct|qwen-coder-32b|Qwen 2.5 Coder 32B|C|0.2800/0.5500|131K/8K|TJS|-/-/-|Code-specialized 32B model|chat",
        "qwen/qwen2.5-coder-14b-instruct|qwen-coder-14b|Qwen 2.5 Coder 14B|C|0.0550/0.1100|131K/8K|TJS|-/-/-|Code-specialized 14B model|chat",
        "qwen/qwen2.5-math-72b-instruct|qwen-math-72b|Qwen 2.5 Math 72B|C|0.5500/1.65|4K/4K|TJS|-/-/-|Math-specialized 72B model|chat",
        "qwen/qwen-vl-max-0809|qwen-vl-max|Qwen VL Max|C|0.2800/0.8200|32K/8K|VTJS|-/-/-|Best vision-language model|chat",
        "qwen/qwen-vl-plus-0809|qwen-vl-plus|Qwen VL Plus|C|0.1100/0.3300|32K/8K|VTJS|-/-/-|Enhanced vision-language model|chat",
        "qwen/qwen2-vl-72b-instruct|qwen2-vl-72b|Qwen2 VL 72B|C|0.5500/1.65|131K/8K|VTJS|-/-/-|Large vision-language model|chat",
        "qwen/qwen2-vl-7b-instruct|qwen2-vl-7b|Qwen2 VL 7B|C|0.0140/0.0410|32K/8K|VTJS|-/-/-|Compact vision-language model|chat",
        "qwen/qwen-long|qwen-long|Qwen Long|C|0.000700/0.0028|10000K/8K|TJS|-/-/-|10M context for ultra-long documents|chat",
        "qwen/qwq-32b-preview|qwq-32b|QwQ 32B|C|0.2800/0.8200|32K/32K|TJK|-/-/-|Chain-of-thought reasoning model|chat",
        "qwen/qwen2-audio-instruct|qwen2-audio|Qwen2 Audio|C|0.0280/0.0820|32K/8K|ATJ|-/-/-|Audio understanding model|audio",
        "qwen/text-embedding-v3|qwen-embed-v3|Text Embedding V3|C|0.000100/-|8K/1K|E|-/-/-|Text embeddings|embed",
        "qwen/qwen2.5-vl-7b-instruct|qwen2.5-vl-7b|Alibaba: Qwen2.5-VL 7B|C|0.000000/0.000000|8K/2K|VST|-/-/-|Qwen 2.5 Vision Language 7B model with OCR capabilities|chat",

    // === RAG (2 models) ===
        "rag/e5-large|e5-large|Alibaba: E5-Large|C|0.000010/0.000020|512/512|VSTJ|-/-/-|E5-Large for dense passage retrieval|chat",
        "rag/bge-large-en|bge-large|BAAI: BGE-Large English|C|0.000010/0.000020|512/512|VSTJ|-/-/-|BGE-Large for semantic search|chat",

    // === RAIFLE (1 models) ===
        "raifle/sorcererlm-8x22b|sorcererlm-8x22b|SorcererLM 8x22B|C|0.000005/0.000005|16K/4K|-|-/-/-|SorcererLM is an advanced RP and storytelling model, built as a Low-rank 16-bit |chat",

    // === RANKING (2 models) ===
        "ranking/lambdamart|lambdamart|rank-ltr|LambdaMART|C|0.000000/0.000000|512/256|S|-/-/-|Learning-to-rank with LambdaMART|chat",
        "ranking/listwise-ranker|listwise|rank-listwise|Listwise Ranker|C|0.000000/0.000000|512/256|S|-/-/-|Listwise learning-to-rank model|chat",

    // === REALESTATE (2 models) ===
        "realestate/propertygpt|propertygpt|reale-property|PropertyGPT: Valuation|C|0.000001/0.000002|8K/2K|VSTJ|-/-/-|Property valuation and market analysis|chat",
        "realestate/investgpt|investgpt|reale-invest|InvestGPT: Real Estate|C|0.000001/0.000003|8K/2K|VSTJ|-/-/-|Real estate investment analysis and portfolio optimization|chat",

    // === REASONING (10 models) ===
        "reasoning/llama-3-reasoning-70b|llama-3-reasoning|Meta: Llama 3 Reasoning 70B|C|0.0010/0.0010|8K/2K|VSTJK|-/-/-|Llama 3 with enhanced reasoning|chat",
        "reasoning/phi-3-reasoning|phi-3-reasoning|Microsoft: Phi-3 Reasoning|C|0.000200/0.000200|131K/2K|VSTJK|-/-/-|Phi-3 with reasoning capabilities|chat",
        "reasoning/phi-3.5-mini|phi-35-mini-f|Microsoft: Phi-3.5 Mini|C|0.000100/0.000100|131K/1K|VSTJK|-/-/-|Phi-3.5 Mini with reasoning|chat",
        "reasoning/phi-3.5-moe|phi-35-moe-f|Microsoft: Phi-3.5 MoE|C|0.000300/0.000300|131K/1K|VSTJK|-/-/-|Phi-3.5 Mixture of Experts|chat",
        "reasoning/internlm2-math-20b|internlm-math-f|Shanghai AI: InternLM2 Math|C|0.000500/0.000500|4K/2K|VSTJK|-/-/-|InternLM2 specialized for math|chat",
        "reasoning/mixture-of-agents|moa-reasoning-f|MoA: Mixture of Agents|C|0.0015/0.0045|8K/2K|VSTJK|-/-/-|Ensemble of specialized agents|chat",
        "reasoning/llemma-7b|llemma-7b|llemma|Meta: Llemma 7B|C|0.000000/0.000000|4K/2K|JT|-/-/-|Llemma 7B specialized for mathematical reasoning|chat",
        "reasoning/llemma-34b|llemma-34b|llemma-large|Meta: Llemma 34B|C|0.000000/0.000001|4K/4K|JT|-/-/-|Llemma 34B for advanced mathematical proofs|chat",
        "reasoning/mathcoder-7b|mathcoder-7b|mathcoder|MathCoder: 7B|C|0.000000/0.000000|4K/2K|JT|-/-/-|MathCoder 7B for solving math problems|chat",
        "reasoning/mathcoder-34b|mathcoder-34b|mathcoder-large|MathCoder: 34B|C|0.000000/0.000001|4K/4K|JT|-/-/-|MathCoder 34B for complex mathematics|chat",

    // === RECOMMENDATION (2 models) ===
        "recommendation/collaborative-filtering-embedding|collab-embed|recommend-collab|Collaborative Filtering Embedding|C|0.000000/0.000000|512/256|S|-/-/-|Collaborative filtering with embeddings|chat",
        "recommendation/neural-collaborative-filtering|ncf|recommend-neural|Neural Collaborative Filtering|C|0.000000/0.000000|512/256|S|-/-/-|Neural network for recommendations|chat",

    // === RELACE (2 models) ===
        "relace/relace-search|relace-search|Relace: Relace Search|C|0.000001/0.000003|256K/128K|T|-/-/-|The relace-search model uses 4-12 'view_file' and 'grep' tools in parallel to ex|chat",
        "relace/relace-apply-3|relace-apply-3|Relace: Relace Apply 3|C|0.000001/0.000001|256K/128K|-|-/-/-|Relace Apply 3 is a specialized code-patching LLM that merges AI-suggested edits|chat",

    // === REPLICATE (3 models) ===
        "replicate/flan-t5-xl|flan-t5-xl-r|Google: FLAN-T5 XL (Replicate)|C|0.000050/0.000150|512/512|VTJ|-/-/-|FLAN-T5 XL on Replicate|chat",
        "replicate/openhermes-2.5|openhermes-r|NousResearch: OpenHermes 2.5|C|0.000100/0.000300|4K/2K|VTJ|-/-/-|OpenHermes 2.5 via Replicate|chat",
        "replicate/orca-mini-8b|orca-mini-r|Microsoft: Orca Mini 8B (Replicate)|C|0.000100/0.000300|8K/2K|VTJ|-/-/-|Orca Mini 8B on Replicate|chat",

    // === RERANKER (3 models) ===
        "reranker/bge-reranker-large|bge-reranker-f|BAAI: BGE Reranker Large|C|0.000150/0.000450|512/256|VSTJ|-/-/-|BGE semantic reranking|chat",
        "reranker/jina-reranker-v1|jina-reranker-f|Jina: Reranker v1|C|0.000120/0.000350|8K/256|VSTJ|-/-/-|Jina reranking model|chat",
        "reranker/rankgpt|rankgpt-f|RankGPT|C|0.000500/0.0015|4K/256|VSTJ|-/-/-|GPT-based reranking|chat",

    // === RESEARCH (2 models) ===
        "research/allenai-olmo-7b|olmo-7b-r|AllenAI: OLMo 7B|C|0.000100/0.000300|2K/2K|VSTJ|-/-/-|Open Language Model from AllenAI|chat",
        "research/stability-stablelm-2|stablelm-2-r|Stability AI: StableLM 2|C|0.000080/0.000080|4K/2K|VSTJ|-/-/-|StableLM 2 foundation model|chat",

    // === RETAIL (3 models) ===
        "retail/shopgpt|shopgpt|retail-shop|ShopGPT: E-commerce|C|0.000001/0.000002|8K/2K|VSTJ|-/-/-|E-commerce model for product recommendations|chat",
        "retail/fashionai-vision|fashionai|retail-fashion|FashionAI: Vision|C|0.000001/0.000003|8K/2K|VSTJ|-/-/-|AI for fashion and apparel analysis|chat",
        "retail/pricegpt|pricegpt|retail-price|PriceGPT: Dynamics|C|0.000001/0.000002|8K/2K|VSTJ|-/-/-|Dynamic pricing and revenue optimization|chat",

    // === RETRIEVAL (2 models) ===
        "retrieval/bge-base-finetuned-uat-retrieval|bge-retrieval|bge-ret-ft|BAAI: BGE Retrieval FT|C|0.000000/0.000000|512/256|VS|-/-/-|BGE fine-tuned for UAT and legal document retrieval|chat",
        "retrieval/e5-large-finetuned-domain-specific|e5-domain|e5-domain-ft|Hugging Face: E5 Domain-Specific FT|C|0.000000/0.000000|512/512|VS|-/-/-|E5 Large fine-tuned for domain-specific semantic search|chat",

    // === RL (3 models) ===
        "rl/trl-ppo|trl-ppo-f|TRL PPO Model|C|0.000400/0.0012|4K/2K|VSTJ|-/-/-|PPO reinforcement learning|chat",
        "rl/gpt2-medium-policy|gpt2-policy|policy-model|OpenAI: GPT-2 Medium Policy|C|0.000000/0.000000|1K/512|T|-/-/-|GPT-2 Medium for policy learning|chat",
        "rl/t5-base-policy|t5-policy|seq2seq-policy|Google: T5 Base Policy|C|0.000000/0.000000|512/512|T|-/-/-|T5 Base for sequence-to-sequence tasks|chat",

    // === RUNPOD (1 models) ===
        "runpod/mistral-7b-instruct|mistral-7b-rp|Mistral 7B Instruct (RunPod)|C|0.000080/0.000100|32K/2K|VSTJ|-/-/-|Mistral 7B on RunPod serverless|chat",

    // === SAMBANOVA (20 models) ===
        "sambanova/Meta-Llama-3.3-70B-Instruct|llama-3.3-70b|Llama 3.3 70B Instruct|C|0.4000/0.8000|4K/4K|TJS|-/-/-|Meta Llama 3.3 70B on SambaNova RDU|chat",
        "sambanova/Meta-Llama-3.1-405B-Instruct|llama-3.1-405b|Llama 3.1 405B Instruct|C|5.00/10.00|4K/4K|TJS|-/-/-|Largest Llama model, 405B parameters|chat",
        "sambanova/Meta-Llama-3.1-70B-Instruct|llama-3.1-70b|Llama 3.1 70B Instruct|C|0.4000/0.8000|4K/4K|TJS|-/-/-|Meta Llama 3.1 70B on SambaNova|chat",
        "sambanova/Meta-Llama-3.1-8B-Instruct|llama-3.1-8b|Llama 3.1 8B Instruct|C|0.1000/0.2000|4K/4K|TJS|-/-/-|Meta Llama 3.1 8B on SambaNova|chat",
        "sambanova/Llama-3.2-90B-Vision-Instruct|llama-3.2-90b-vision|Llama 3.2 90B Vision|C|0.6000/1.20|4K/4K|VTJS|-/-/-|Llama 3.2 90B with vision capabilities|chat",
        "sambanova/Llama-3.2-11B-Vision-Instruct|llama-3.2-11b-vision|Llama 3.2 11B Vision|C|0.1500/0.3000|4K/4K|VTJS|-/-/-|Compact Llama 3.2 with vision|chat",
        "sambanova/Qwen2.5-72B-Instruct|qwen-2.5-72b|Qwen 2.5 72B Instruct|C|0.4000/0.8000|4K/4K|TJS|-/-/-|Alibaba Qwen 2.5 72B on SambaNova|chat",
        "sambanova/Qwen2.5-Coder-32B-Instruct|qwen-coder-32b|Qwen 2.5 Coder 32B|C|0.2000/0.4000|4K/4K|TJS|-/-/-|Qwen 2.5 specialized for code|chat",
        "sambanova/QwQ-32B|qwq-32b|QwQ 32B|C|0.2000/0.4000|4K/4K|TJK|-/-/-|Alibaba QwQ reasoning model|chat",
        "sambanova/DeepSeek-R1|deepseek-r1|DeepSeek R1|C|0.6000/1.20|4K/4K|TJK|-/-/-|DeepSeek R1 reasoning model|chat",
        "sambanova/DeepSeek-R1-Distill-Llama-70B|deepseek-r1-70b|DeepSeek R1 Distill 70B|C|0.4000/0.8000|4K/4K|TJK|-/-/-|DeepSeek R1 distilled to Llama 70B|chat",
        "sambanova/Llama-4-Maverick-17B-128E-Instruct|llama-4-maverick|Llama 4 Maverick 17B|C|0.2000/0.6000|131K/16K|VTJS|-/-/-|Meta Llama 4 Maverick 400B MoE|chat",
        "sambanova/Llama-4-Scout-17B-16E-Instruct|llama-4-scout|Llama 4 Scout 17B|C|0.1500/0.4000|131K/16K|VTJS|-/-/-|Meta Llama 4 Scout 109B MoE|chat",
        "sambanova/gpt-oss-120b|gpt-oss-120b|GPT-OSS 120B|C|0.5900/0.7900|131K/16K|TJS|-/-/-|OpenAI open-weight 120B MoE|chat",
        "sambanova/DeepSeek-V3-0324|deepseek-v3|DeepSeek V3 0324|C|0.4000/0.8000|65K/16K|TJS|-/-/-|DeepSeek V3 with function calling|chat",
        "sambanova/Qwen3-32B|qwen3-32b|Qwen3 32B|C|0.2000/0.4000|131K/16K|TJS|-/-/-|Alibaba Qwen3 32B multilingual|chat",
        "sambanova/Llama-3.2-1B-Instruct|llama-3.2-1b|Llama 3.2 1B|C|0.0500/0.1000|131K/4K|TJ|-/-/-|Smallest Llama 3.2 model|chat",
        "sambanova/Llama-3.2-3B-Instruct|llama-3.2-3b|Llama 3.2 3B|C|0.0800/0.1600|131K/4K|TJ|-/-/-|Compact Llama 3.2 model|chat",
        "sambanova/ALLaM-7B-Instruct-preview|allam-7b|ALLaM 7B Arabic|C|0.1000/0.2000|4K/4K|TJ|-/-/-|Arabic language model preview|chat",
        "sambanova/E5-Mistral-7B-Instruct|e5-mistral-7b|E5 Mistral 7B Embed|C|0.1000/-|4K/4K|E|-/-/-|Embedding model|embed",

    // === SAO10K (5 models) ===
        "sao10k/l3.1-70b-hanami-x1|l3.1-70b-hanami-x1|Sao10K: Llama 3.1 70B Hanami x1|C|0.000003/0.000003|16K/4K|-|-/-/-|This is [Sao10K](/sao10k)'s experiment over [Euryale v2.2](/sao10k/l3.1-euryale-|chat",
        "sao10k/l3.3-euryale-70b|l3.3-euryale-70b|Sao10K: Llama 3.3 Euryale 70B|C|0.000001/0.000001|131K/16K|JS|-/-/-|Euryale L3.3 70B is a model focused on creative roleplay from [Sao10k](https://k|chat",
        "sao10k/l3.1-euryale-70b|l3.1-euryale-70b|Sao10K: Llama 3.1 Euryale 70B v2.2|C|0.000001/0.000001|32K/8K|JST|-/-/-|Euryale L3.1 70B v2.2 is a model focused on creative roleplay from [Sao10k](http|chat",
        "sao10k/l3-lunaris-8b|l3-lunaris-8b|Sao10K: Llama 3 8B Lunaris|C|0.000000/0.000000|8K/2K|JS|-/-/-|Lunaris 8B is a versatile generalist and roleplaying model based on Llama 3. It'|chat",
        "sao10k/l3-euryale-70b|l3-euryale-70b|Sao10k: Llama 3 Euryale 70B v2.1|C|0.000001/0.000001|8K/8K|T|-/-/-|Euryale 70B v2.1 is a model focused on creative roleplay from [Sao10k](https://k|chat",

    // === SCIENCE (4 models) ===
        "science/scibertagpt|scibertagpt|science-bert|SciGPT: SciPDF|C|0.000001/0.000004|8K/2K|VSTJ|-/-/-|Model for scientific paper understanding|chat",
        "science/mathandgpt|mathandgpt|science-math|MathAndGPT: Theorem|C|0.000002/0.000005|8K/2K|VSTJK|-/-/-|Specialized for mathematical theorem proving|chat",
        "science/cheminformatics-bert|cheminformatics|science-chem|Cheminformatics BERT|C|0.000001/0.000003|4K/2K|VSTJ|-/-/-|Model for chemical structure and property prediction|chat",
        "science/bioinformatics-llama|bioinformatics|science-bio|BioInformatics Llama|C|0.000001/0.000003|8K/2K|VSTJ|-/-/-|Llama for genomics and biological sequence analysis|chat",

    // === SEA (3 models) ===
        "sea/xlm-roberta-large|xlm-roberta|xlm-sea|Facebook: XLM-RoBERTa Large|C|0.000000/0.000000|512/256|S|-/-/-|XLM-RoBERTa for Southeast Asian languages|chat",
        "sea/multilingual-t5-large|mT5-large|mT5-sea|Google: mT5 Large|C|0.000000/0.000000|512/512|T|-/-/-|Multilingual T5 Large for SE Asian tasks|chat",
        "sea/ph-llama-13b|ph-llama|manila-llama|Philippine: Llama 13B|C|0.000000/0.000001|4K/2K|T|-/-/-|Llama 13B optimized for Philippine English and Tagalog|chat",

    // === SEARCH (3 models) ===
        "search/voyage-large-2|voyage-large-f|Voyage: Large 2|C|0.000150/0.000150|16K/1K|VSTJ|-/-/-|Voyage semantic search large|chat",
        "search/voyage-code-2|voyage-code-f|Voyage: Code 2|C|0.000100/0.000100|16K/1K|VSTJK|-/-/-|Voyage code search|chat",
        "search/nomic-embed-text|nomic-embed-f|Nomic: Embed Text|C|0.000080/0.000100|2K/768|VSTJ|-/-/-|Nomic embeddings 768D|chat",

    // === SEMANTIC (3 models) ===
        "semantic/paraphrase-MiniLM-L6-v2|paraphrase-minilm|semantic-small|Sentence Transformers: Paraphrase MiniLM|C|0.000000/0.000000|512/256|VS|-/-/-|Paraphrase detection with MiniLM|chat",
        "semantic/paraphrase-multilingual-MiniLM-L12-v2|paraphrase-multilingual|semantic-multilingual|Sentence Transformers: Paraphrase Multilingual|C|0.000000/0.000000|512/256|VS|-/-/-|Multilingual paraphrase detection|chat",
        "semantic/semantic-search-qa-msmarco-distilbert-base-v4|semantic-search-qa|semantic-qa|Sentence Transformers: Semantic Search QA|C|0.000000/0.000000|512/256|VS|-/-/-|Semantic search for question answering|chat",

    // === SENTIMENT (6 models) ===
        "sentiment/distilbert-base-uncased-finetuned|sentiment-f|DistilBERT Sentiment|C|0.000050/0.000150|512/256|VSTJ|-/-/-|Fine-tuned sentiment analysis|chat",
        "sentiment/bert-large-uncased-finetuned|sentiment-bert-f|BERT Large Sentiment|C|0.000100/0.000300|512/256|VSTJ|-/-/-|BERT large sentiment|chat",
        "sentiment/distilbert-base-uncased-finetuned-sst-2|distilbert-sst2|sentiment-small|Hugging Face: DistilBERT SST-2|C|0.000000/0.000000|512/256|S|-/-/-|DistilBERT fine-tuned for sentiment|chat",
        "sentiment/bert-base-uncased-finetuned-emotion|bert-emotion|emotion-analysis|Hugging Face: BERT Emotion|C|0.000000/0.000000|512/256|S|-/-/-|BERT trained for emotion detection|chat",
        "sentiment/xlnet-base-cased-finetuned-imdb|xlnet-imdb|sentiment-xlnet|XLNet: IMDB Sentiment|C|0.000000/0.000000|512/256|S|-/-/-|XLNet fine-tuned on IMDB reviews|chat",
        "sentiment/roberta-base-openai-detector|roberta-openai|text-classifier|RoBERTa: OpenAI Detector|C|0.000000/0.000000|512/256|S|-/-/-|RoBERTa for synthetic text detection|chat",

    // === SPARK (10 models) ===
        "spark/spark-4.0-ultra|spark-4-ultra|Spark 4.0 Ultra|C|0.6900/0.6900|131K/8K|VTJS|-/-/-|Flagship 128K context model|chat",
        "spark/spark-4.0-max-32k|spark-4-max|Spark 4.0 Max 32K|C|0.4200/0.4200|32K/8K|VTJS|-/-/-|32K context model|chat",
        "spark/spark-3.5-max|spark-3.5-max|Spark 3.5 Max|C|0.4200/0.4200|131K/8K|TJS|-/-/-|Previous gen 128K model|chat",
        "spark/spark-3.5-pro|spark-3.5-pro|Spark 3.5 Pro|C|0.1400/0.1400|32K/8K|TJS|-/-/-|Balanced 32K model|chat",
        "spark/spark-lite|spark-lite|Spark Lite|C|0/0|8K/4K|TJ|-/-/-|Free lightweight model|chat",
        "spark/spark-vision-4.0|spark-vision-4|Spark Vision 4.0|C|0.4200/0.4200|32K/8K|VTJ|-/-/-|Vision-language model|chat",
        "spark/spark-code|spark-code|Spark Code|C|0.2100/0.2100|32K/8K|TJS|-/-/-|Code generation model|chat",
        "spark/spark-asr-v2|spark-asr|Spark ASR v2|C|0.0300/-|-/-|A|-/-/-|Speech recognition|audio",
        "spark/spark-tts-v2|spark-tts|Spark TTS v2|C|0.0100/-|-/-|A|-/-/-|Text-to-speech|audio",
        "spark/spark-embedding-v1|spark-embed|Spark Embedding v1|C|0.000300/-|512/768|E|-/-/-|Text embeddings|embed",

    // === SPARSE (2 models) ===
        "sparse/llm-pruned-70b|llm-pruned-70b-s|ModelSuite: Pruned LLM 70B|C|0.000500/0.000500|8K/2K|VSTJ|-/-/-|Sparsely pruned 70B model|chat",
        "sparse/vision-pruned-large|vision-pruned-l-s|ModelSuite: Pruned Vision|C|0.000080/0.000200|2K/2K|VT|-/-/-|Sparsely pruned vision model|chat",

    // === SRL (1 models) ===
        "srl/diegogarciar-electra-base-srl-english|electra-srl|semantic-roles|ELECTRA: Semantic Role Labeling|C|0.000000/0.000000|512/256|S|-/-/-|ELECTRA for semantic role labeling|chat",

    // === STABILITY (16 models) ===
        "stability/stable-image-ultra|stable-ultra|Stable Image Ultra|C|0.0800/-|10K/-|I|-/-/-|Highest quality image generation, $0.08/image|image",
        "stability/sd3.5-large|sd3.5-large|SD 3.5 Large|C|0.0650/-|10K/-|I|-/-/-|SD 3.5 Large, $0.065/image|image",
        "stability/sd3.5-large-turbo|sd3.5-turbo|SD 3.5 Large Turbo|C|0.0400/-|10K/-|I|-/-/-|Fast SD 3.5, $0.04/image|image",
        "stability/sd3.5-medium|sd3.5-medium|SD 3.5 Medium|C|0.0350/-|10K/-|I|-/-/-|SD 3.5 Medium, $0.035/image|image",
        "stability/sd3-large|sd3-large|SD 3 Large|C|0.0650/-|10K/-|I|-/-/-|SD 3 Large, $0.065/image|image",
        "stability/sd3-large-turbo|sd3-turbo|SD 3 Large Turbo|C|0.0400/-|10K/-|I|-/-/-|Fast SD 3, $0.04/image|image",
        "stability/sd3-medium|sd3-medium|SD 3 Medium|C|0.0350/-|10K/-|I|-/-/-|SD 3 Medium, $0.035/image|image",
        "stability/stable-image-core|stable-core|Stable Image Core|C|0.0300/-|10K/-|I|-/-/-|Efficient core model, $0.03/image|image",
        "stability/stable-diffusion-xl-1024-v1-0|sdxl-1.0|SDXL 1.0|C|0.0020/-|77/-|I|-/-/-|SDXL, $0.002-0.006 depending on steps|image",
        "stability/stable-diffusion-xl-1024-v0-9|sdxl-0.9|SDXL 0.9|C|0.0020/-|77/-|I|-/-/-|SDXL 0.9 beta|image",
        "stability/stable-diffusion-v1-6|sd-1.6|SD 1.6|C|0.0020/-|77/-|I|-/-/-|Legacy SD 1.6|image",
        "stability/stable-diffusion-inpaint|sd-inpaint|SD Inpaint|C|0.0020/-|77/-|I|-/-/-|Image inpainting|image",
        "stability/stable-image-control|stable-control|Stable Image Control|C|0.0400/-|10K/-|I|-/-/-|Controlled image generation|image",
        "stability/stable-image-upscale|stable-upscale|Stable Image Upscale|C|0.0200/-|-/-|I|-/-/-|Image upscaling, $0.02/image|image",
        "stability/stable-fast-upscale|fast-upscale|Fast Upscale|C|0.0100/-|-/-|I|-/-/-|Fast upscaling, $0.01/image|image",
        "stability/stable-video-diffusion|svd|Stable Video Diffusion|C|0.2000/-|-/-|D|-/-/-|Video generation|video",

    // === STEPFUN-AI (1 models) ===
        "stepfun-ai/step3|step3|StepFun: Step3|C|0.000001/0.000001|65K/65K|JKSTV|-/-/-|Step3 is a cutting-edge multimodal reasoning model-built on a Mixture-of-Experts|chat",

    // === STRUCTURED (2 models) ===
        "structured/ta-bert|tabert|tabular-bert|IBM: TabBERT|C|0.000000/0.000000|512/256|S|-/-/-|TabBERT for tabular data understanding|chat",
        "structured/ditto|ditto|data-matching|University of Wisconsin: DITTO|C|0.000000/0.000000|512/256|S|-/-/-|Deep learning for entity matching|chat",

    // === STS (1 models) ===
        "sts/sentence-transformers-msmarco|sts-msmarco-f|STS MSMARCO|C|0.000080/0.000250|512/512|VSTJ|-/-/-|MSMARCO semantic similarity|chat",

    // === SUMMARIZATION (3 models) ===
        "summarization/pegasus-cnn-dailymail|pegasus-cnn|summarize-news|Google: Pegasus CNN/DailyMail|C|0.000000/0.000000|1K/512|T|-/-/-|PEGASUS fine-tuned for news summarization|chat",
        "summarization/bart-large-cnn|bart-cnn|summarize-bart|Facebook: BART CNN|C|0.000000/0.000000|1K/512|T|-/-/-|BART large for abstractive summarization|chat",
        "summarization/pegasus-pubmed|pegasus-pubmed|summarize-medical|Google: Pegasus PubMed|C|0.000000/0.000000|1K/512|T|-/-/-|PEGASUS for medical paper summarization|chat",

    // === SUMMARY (3 models) ===
        "summary/pegasus-xsum|pegasus-xsum-f|PEGASUS XSum|C|0.000100/0.000300|1K/512|VSTJ|-/-/-|PEGASUS extreme summarization|chat",
        "summary/bart-large-cnn|bart-cnn-f|BART Large CNN|C|0.000100/0.000300|1K/512|VSTJ|-/-/-|BART CNN news summarization|chat",
        "summary/t5-large|t5-large-f|T5 Large|C|0.000120/0.000350|512/512|VSTJ|-/-/-|T5 text-to-text large|chat",

    // === SUPPORT (3 models) ===
        "support/supportgpt|supportgpt|support-bot|SupportGPT: Ticketing|C|0.000001/0.000002|8K/2K|VSTJ|-/-/-|Customer support automation and ticket routing|chat",
        "support/intentgpt|intentgpt|support-intent|IntentGPT: Understanding|C|0.000001/0.000002|4K/2K|VSTJ|-/-/-|Intent classification for support chatbots|chat",
        "support/multilingual-support|multisupp|support-multi|MultilingualSupport LLM|C|0.000001/0.000003|8K/2K|VSTJ|-/-/-|Multilingual customer support automation|chat",

    // === SWITCHPOINT (1 models) ===
        "switchpoint/router|router|Switchpoint Router|C|0.000001/0.000003|131K/32K|K|-/-/-|Switchpoint AI's router instantly analyzes your request and directs it to the op|chat",

    // === SYNTHETIC (1 models) ===
        "synthetic/llm-synthetic|synthetic-llm-f|Synthetic Data LLM|C|0.000300/0.000900|4K/2K|VSTJ|-/-/-|Synthetic data generation|chat",

    // === TELECOM (2 models) ===
        "telecom/networkgpt|networkgpt|telecom-network|NetworkGPT: Optimization|C|0.000001/0.000004|8K/2K|VSTJ|-/-/-|Network optimization and performance management|chat",
        "telecom/customergpt|customergpt|telecom-customer|CustomerGPT: Telecom|C|0.000001/0.000003|8K/2K|VSTJ|-/-/-|Telecom customer experience optimization|chat",

    // === TENCENT (1 models) ===
        "tencent/hunyuan-a13b-instruct|hunyuan-a13b-instruc|Tencent: Hunyuan A13B Instruct|C|0.000000/0.000001|131K/131K|JKS|-/-/-|Hunyuan-A13B is a 13B active parameter Mixture-of-Experts (MoE) language model d|chat",

    // === THEDRUMMER (4 models) ===
        "thedrummer/cydonia-24b-v4.1|cydonia-24b-v4.1|TheDrummer: Cydonia 24B V4.1|C|0.000000/0.000000|131K/131K|JS|-/-/-|Uncensored and creative writing model based on Mistral Small 3.2 24B with good r|chat",
        "thedrummer/skyfall-36b-v2|skyfall-36b-v2|TheDrummer: Skyfall 36B V2|C|0.000001/0.000001|32K/32K|-|-/-/-|Skyfall 36B v2 is an enhanced iteration of Mistral Small 2501, specifically fine|chat",
        "thedrummer/unslopnemo-12b|unslopnemo-12b|TheDrummer: UnslopNemo 12B|C|0.000000/0.000000|32K/8K|JST|-/-/-|UnslopNemo v4.1 is the latest addition from the creator of Rocinante, designed f|chat",
        "thedrummer/rocinante-12b|rocinante-12b|TheDrummer: Rocinante 12B|C|0.000000/0.000000|32K/8K|JST|-/-/-|Rocinante 12B is designed for engaging storytelling and rich prose.

Early teste|chat",

    // === THUDM (1 models) ===
        "thudm/glm-4.1v-9b-thinking|glm-4.1v-9b-thinking|THUDM: GLM 4.1V 9B Thinking|C|0.000000/0.000000|65K/8K|KV|-/-/-|GLM-4.1V-9B-Thinking is a 9B parameter vision-language model developed by THUDM,|chat",

    // === TIMESERIES (2 models) ===
        "timeseries/n-beats|nbeats|forecast-nbeats|Element AI: N-BEATS|C|0.000000/0.000000|512/256|S|-/-/-|Neural basis expansion for forecasting|chat",
        "timeseries/temporal-transformer|temporal-trans|forecast-temporal|Google: Temporal Transformer|C|0.000000/0.000000|512/256|S|-/-/-|Transformer for temporal sequence modeling|chat",

    // === TNGTECH (6 models) ===
        "tngtech/tng-r1t-chimera:free|tng-r1t-chimera:free|TNG: R1T Chimera (free)|C|-/-|163K/163K|JKST|-/-/-|TNG-R1T-Chimera is an experimental LLM with a faible for creative storytelling a|chat",
        "tngtech/tng-r1t-chimera|tng-r1t-chimera|TNG: R1T Chimera|C|0.000000/0.000001|163K/65K|JKST|-/-/-|TNG-R1T-Chimera is an experimental LLM with a faible for creative storytelling a|chat",
        "tngtech/deepseek-r1t2-chimera:free|deepseek-r1t2-chimer|TNG: DeepSeek R1T2 Chimera (free)|C|-/-|163K/40K|K|-/-/-|DeepSeek-TNG-R1T2-Chimera is the second-generation Chimera model from TNG Tech. |chat",
        "tngtech/deepseek-r1t2-chimera|deepseek-r1t2-chimer|TNG: DeepSeek R1T2 Chimera|C|0.000000/0.000001|163K/163K|JKST|-/-/-|DeepSeek-TNG-R1T2-Chimera is the second-generation Chimera model from TNG Tech. |chat",
        "tngtech/deepseek-r1t-chimera:free|deepseek-r1t-chimera|TNG: DeepSeek R1T Chimera (free)|C|-/-|163K/40K|K|-/-/-|DeepSeek-R1T-Chimera is created by merging DeepSeek-R1 and DeepSeek-V3 (0324), c|chat",
        "tngtech/deepseek-r1t-chimera|deepseek-r1t-chimera|TNG: DeepSeek R1T Chimera|C|0.000000/0.000001|163K/163K|JKS|-/-/-|DeepSeek-R1T-Chimera is created by merging DeepSeek-R1 and DeepSeek-V3 (0324), c|chat",

    // === TOGETHER (139 models) ===
        "together/cartesia/sonic|sonic|Cartesia Sonic|C|65.00/-|-/-|A|-/-/-|Cartesia Sonic on Together AI|audio",
        "together/cartesia/sonic-2|sonic-2|Cartesia Sonic 2|C|65.00/-|-/-|A|-/-/-|Cartesia Sonic 2 on Together AI|audio",
        "together/Meta-Llama/Llama-Guard-7b|llama-guard-7b|Llama Guard (7B)|C|0.2000/0.2000|4K/1K|M|-/-/-|Llama Guard (7B) on Together AI|chat",
        "together/togethercomputer/MoA-1|moa-1|Together AI MoA-1|C|-/-|32K/4K|T|-/-/-|Together AI MoA-1 on Together AI|chat",
        "together/meta-llama/Llama-3.3-70B-Instruct-Turbo|llama-3.3-70b-instruct-turbo|Meta Llama 3.3 70B Instruct Turbo|C|0.8800/0.8800|131K/4K|TJS|-/-/-|Meta Llama 3.3 70B Instruct Turbo on Together AI|chat",
        "together/kwaivgI/kling-1.6-standard|kling-1.6-standard|Kling 1.6 Standard|C|-/-|4K/1K|D|-/-/-|Kling 1.6 Standard on Together AI|video",
        "together/black-forest-labs/FLUX.2-flex|flux.2-flex|FLUX.2 [flex]|C|-/-|4K/1K|I|-/-/-|FLUX.2 [flex] on Together AI|image",
        "together/deepseek-ai/DeepSeek-R1|deepseek-r1|DeepSeek R1-0528|C|3.00/7.00|163K/4K|TJK|-/-/-|DeepSeek R1-0528 on Together AI|chat",
        "together/meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo|meta-llama-3.1-70b-instruct-turbo|Meta Llama 3.1 70B Instruct Turbo|C|0.8800/0.8800|131K/4K|TJS|-/-/-|Meta Llama 3.1 70B Instruct Turbo on Together AI|chat",
        "together/black-forest-labs/FLUX.1-dev-lora|flux.1-dev-lora|FLUX.1 [dev] LoRA|C|-/-|4K/1K|I|-/-/-|FLUX.1 [dev] LoRA on Together AI|image",
        "together/HiDream-ai/HiDream-I1-Full|hidream-i1-full|HiDream-I1-Full|C|-/-|4K/1K|I|-/-/-|HiDream-I1-Full on Together AI|image",
        "together/black-forest-labs/FLUX.1-krea-dev|flux.1-krea-dev|FLUX.1 Krea [dev]|C|-/-|4K/1K|I|-/-/-|FLUX.1 Krea [dev] on Together AI|image",
        "together/togethercomputer/MoA-1-Turbo|moa-1-turbo|Together AI MoA-1-Turbo|C|-/-|32K/4K|T|-/-/-|Together AI MoA-1-Turbo on Together AI|chat",
        "together/Lykon/DreamShaper|dreamshaper|Dreamshaper|C|-/-|4K/1K|I|-/-/-|Dreamshaper on Together AI|image",
        "together/HiDream-ai/HiDream-I1-Dev|hidream-i1-dev|HiDream-I1-Dev|C|-/-|4K/1K|I|-/-/-|HiDream-I1-Dev on Together AI|image",
        "together/Qwen/Qwen-Image|qwen-image|Qwen Image|C|-/-|4K/1K|I|-/-/-|Qwen Image on Together AI|image",
        "together/RunDiffusion/Juggernaut-pro-flux|juggernaut-pro-flux|Juggernaut Pro Flux by RunDiffusion 1.0.0|C|-/-|4K/1K|I|-/-/-|Juggernaut Pro Flux by RunDiffusion 1.0.0 on Together AI|image",
        "together/google/imagen-4.0-preview|imagen-4.0-preview|Google Imagen 4.0 Preview|C|-/-|4K/1K|I|-/-/-|Google Imagen 4.0 Preview on Together AI|image",
        "together/google/imagen-4.0-ultra|imagen-4.0-ultra|Google Imagen 4.0 Ultra|C|-/-|4K/1K|I|-/-/-|Google Imagen 4.0 Ultra on Together AI|image",
        "together/google/veo-3.0|veo-3.0|Google Veo 3.0|C|-/-|4K/1K|D|-/-/-|Google Veo 3.0 on Together AI|video",
        "together/minimax/hailuo-02|hailuo-02|MiniMax Hailuo 02|C|-/-|4K/1K|D|-/-/-|MiniMax Hailuo 02 on Together AI|video",
        "together/stabilityai/stable-diffusion-3-medium|stable-diffusion-3-medium|Stable Diffusion 3|C|-/-|4K/1K|I|-/-/-|Stable Diffusion 3 on Together AI|image",
        "together/minimax/video-01-director|video-01-director|MiniMax 01 Director|C|-/-|4K/1K|D|-/-/-|MiniMax 01 Director on Together AI|video",
        "together/deepseek-ai/DeepSeek-R1-0528-tput|deepseek-r1-0528-tput|DeepSeek R1 0528 Throughput|C|0.5500/2.19|163K/4K|TJK|-/-/-|DeepSeek R1 0528 Throughput on Together AI|chat",
        "together/google/flash-image-2.5|flash-image-2.5|Gemini Flash Image 2.5 (Nano Banana)|C|-/-|4K/1K|I|-/-/-|Gemini Flash Image 2.5 (Nano Banana) on Together AI|image",
        "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8|llama-4-maverick-17b-128e-instruct-fp8|Llama 4 Maverick Instruct (17Bx128E)|C|0.2700/0.8500|1048K/4K|TJ|-/-/-|Llama 4 Maverick Instruct (17Bx128E) on Together AI|chat",
        "together/ServiceNow-AI/Apriel-1.5-15b-Thinker|apriel-1.5-15b-thinker|Apriel 1.5 15B Thinker|C|-/-|131K/4K|T|-/-/-|Apriel 1.5 15B Thinker on Together AI|chat",
        "together/scb10x/scb10x-typhoon-2-1-gemma3-12b|scb10x-typhoon-2-1-gemma3-12b|Typhoon 2.1 12B|C|0.2000/0.2000|131K/4K|T|-/-/-|Typhoon 2.1 12B on Together AI|chat",
        "together/meta-llama/Llama-Guard-4-12B|llama-guard-4-12b|Llama Guard 4 12B|C|0.2000/0.2000|1048K/4K|M|-/-/-|Llama Guard 4 12B on Together AI|chat",
        "together/meta-llama/LlamaGuard-2-8b|llamaguard-2-8b|Meta Llama Guard 2 8B|C|0.2000/0.2000|8K/2K|M|-/-/-|Meta Llama Guard 2 8B on Together AI|chat",
        "together/HiDream-ai/HiDream-I1-Fast|hidream-i1-fast|HiDream-I1-Fast|C|-/-|4K/1K|I|-/-/-|HiDream-I1-Fast on Together AI|image",
        "together/Wan-AI/Wan2.2-T2V-A14B|wan2.2-t2v-a14b|Wan 2.2 T2V|C|-/-|4K/1K|D|-/-/-|Wan 2.2 T2V on Together AI|video",
        "together/ByteDance/Seedance-1.0-pro|seedance-1.0-pro|ByteDance Seedance 1.0 Pro|C|-/-|4K/1K|D|-/-/-|ByteDance Seedance 1.0 Pro on Together AI|video",
        "together/google/veo-3.0-fast-audio|veo-3.0-fast-audio|Google Veo 3.0 Fast + Audio|C|-/-|4K/1K|D|-/-/-|Google Veo 3.0 Fast + Audio on Together AI|video",
        "together/vidu/vidu-q1|vidu-q1|Vidu Q1|C|-/-|4K/1K|D|-/-/-|Vidu Q1 on Together AI|video",
        "together/intfloat/multilingual-e5-large-instruct|multilingual-e5-large-instruct|Multilingual E5 Large Instruct|C|0.0200/0.0200|514/128|E|-/-/-|Multilingual E5 Large Instruct on Together AI|embed",
        "together/meta-llama/Llama-4-Scout-17B-16E-Instruct|llama-4-scout-17b-16e-instruct|Llama 4 Scout Instruct (17Bx16E)|C|0.1800/0.5900|1048K/4K|TJ|-/-/-|Llama 4 Scout Instruct (17Bx16E) on Together AI|chat",
        "together/meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo|meta-llama-3.1-8b-instruct-turbo|Meta Llama 3.1 8B Instruct Turbo|C|0.1800/0.1800|131K/4K|TJS|-/-/-|Meta Llama 3.1 8B Instruct Turbo on Together AI|chat",
        "together/ByteDance-Seed/Seedream-3.0|seedream-3.0|ByteDance Seedream 3.0|C|-/-|4K/1K|I|-/-/-|ByteDance Seedream 3.0 on Together AI|image",
        "together/ByteDance-Seed/Seedream-4.0|seedream-4.0|ByteDance Seedream 4.0|C|-/-|4K/1K|I|-/-/-|ByteDance Seedream 4.0 on Together AI|image",
        "together/Qwen/Qwen3-Next-80B-A3B-Thinking|qwen3-next-80b-a3b-thinking|Qwen3 Next 80B A3b Thinking|C|0.1500/1.50|262K/4K|TJ|-/-/-|Qwen3 Next 80B A3b Thinking on Together AI|chat",
        "together/arcee-ai/trinity-mini|trinity-mini|Trinity Mini|C|0.0450/0.1500|128K/4K|T|-/-/-|Trinity Mini on Together AI|chat",
        "together/deepcogito/cogito-v2-1-671b|cogito-v2-1-671b|Cogito v2.1 671B|C|1.25/1.25|163K/4K|T|-/-/-|Cogito v2.1 671B on Together AI|chat",
        "together/mixedbread-ai/Mxbai-Rerank-Large-V2|mxbai-rerank-large-v2|Mxbai Rerank Large V2|C|0.1000/0.1000|32K/4K|R|-/-/-|Mxbai Rerank Large V2 on Together AI|rerank",
        "together/Alibaba-NLP/gte-modernbert-base|gte-modernbert-base|Gte Modernbert Base|C|0.0800/0.0800|8K/2K|E|-/-/-|Gte Modernbert Base on Together AI|embed",
        "together/deepcogito/cogito-v2-preview-llama-70B|cogito-v2-preview-llama-70b|Deepcogito Cogito V2 Preview Llama 70B|C|0.8800/0.8800|32K/4K|T|-/-/-|Deepcogito Cogito V2 Preview Llama 70B on Together AI|chat",
        "together/Qwen/Qwen2.5-VL-72B-Instruct|qwen2.5-vl-72b-instruct|Qwen2.5-VL (72B) Instruct|C|1.95/8.00|32K/4K|VTJS|-/-/-|Qwen2.5-VL (72B) Instruct on Together AI|chat",
        "together/marin-community/marin-8b-instruct|marin-8b-instruct|Marin 8B Instruct|C|0.1800/0.1800|4K/1K|TJ|-/-/-|Marin 8B Instruct on Together AI|chat",
        "together/black-forest-labs/FLUX.1-kontext-dev|flux.1-kontext-dev|FLUX.1 Kontext [dev]|C|-/-|4K/1K|I|-/-/-|FLUX.1 Kontext [dev] on Together AI|image",
        "together/zai-org/GLM-4.6|glm-4.6|Glm 4.6 Fp8|C|0.6000/2.20|202K/4K|T|-/-/-|Glm 4.6 Fp8 on Together AI|chat",
        "together/meta-llama/Meta-Llama-3.1-405B-Instruct-Lite-Pro|meta-llama-3.1-405b-instruct-lite-pro|Meta Llama 3.1 405B Instruct Turbo|C|-/-|4K/1K|TJS|-/-/-|Meta Llama 3.1 405B Instruct Turbo on Together AI|chat",
        "together/black-forest-labs/FLUX.1-kontext-max|flux.1-kontext-max|FLUX.1 Kontext [max]|C|-/-|-/-|I|-/-/-|FLUX.1 Kontext [max] on Together AI|image",
        "together/ideogram/ideogram-3.0|ideogram-3.0|Ideogram 3.0|C|-/-|4K/1K|I|-/-/-|Ideogram 3.0 on Together AI|image",
        "together/black-forest-labs/FLUX.1-pro|flux.1-pro|FLUX.1 [pro]|C|-/-|4K/1K|I|-/-/-|FLUX.1 [pro] on Together AI|image",
        "together/black-forest-labs/FLUX.1.1-pro|flux.1.1-pro|FLUX1.1 [pro]|C|-/-|4K/1K|I|-/-/-|FLUX1.1 [pro] on Together AI|image",
        "together/Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8|qwen3-coder-480b-a35b-instruct-fp8|Qwen3 Coder 480B A35B Instruct Fp8|C|2.00/2.00|262K/4K|TJ|-/-/-|Qwen3 Coder 480B A35B Instruct Fp8 on Together AI|chat",
        "together/black-forest-labs/FLUX.1-kontext-pro|flux.1-kontext-pro|FLUX.1 Kontext [pro]|C|-/-|-/-|I|-/-/-|FLUX.1 Kontext [pro] on Together AI|image",
        "together/openai/sora-2|sora-2|Sora 2|C|-/-|4K/1K|D|-/-/-|Sora 2 on Together AI|video",
        "together/kwaivgI/kling-2.1-standard|kling-2.1-standard|Kling 2.1 Standard|C|-/-|4K/1K|D|-/-/-|Kling 2.1 Standard on Together AI|video",
        "together/google/imagen-4.0-fast|imagen-4.0-fast|Google Imagen 4.0 Fast|C|-/-|4K/1K|I|-/-/-|Google Imagen 4.0 Fast on Together AI|image",
        "together/kwaivgI/kling-2.1-master|kling-2.1-master|Kling 2.1 Master|C|-/-|4K/1K|D|-/-/-|Kling 2.1 Master on Together AI|video",
        "together/google/veo-3.0-audio|veo-3.0-audio|Google Veo 3.0 + Audio|C|-/-|4K/1K|D|-/-/-|Google Veo 3.0 + Audio on Together AI|video",
        "together/Rundiffusion/Juggernaut-Lightning-Flux|juggernaut-lightning-flux|Juggernaut Lightning Flux by RunDiffusion|C|-/-|4K/1K|I|-/-/-|Juggernaut Lightning Flux by RunDiffusion on Together AI|image",
        "together/Wan-AI/Wan2.2-I2V-A14B|wan2.2-i2v-a14b|Wan 2.2 I2V|C|-/-|4K/1K|D|-/-/-|Wan 2.2 I2V on Together AI|video",
        "together/google/veo-2.0|veo-2.0|Google Veo 2.0|C|-/-|4K/1K|D|-/-/-|Google Veo 2.0 on Together AI|video",
        "together/google/veo-3.0-fast|veo-3.0-fast|Google Veo 3.0 Fast|C|-/-|4K/1K|D|-/-/-|Google Veo 3.0 Fast on Together AI|video",
        "together/kwaivgI/kling-2.0-master|kling-2.0-master|Kling 2.0 Master|C|-/-|4K/1K|D|-/-/-|Kling 2.0 Master on Together AI|video",
        "together/pixverse/pixverse-v5|pixverse-v5|PixVerse v5|C|-/-|4K/1K|D|-/-/-|PixVerse v5 on Together AI|video",
        "together/stabilityai/stable-diffusion-xl-base-1.0|stable-diffusion-xl-base-1.0|SD XL|C|-/-|4K/1K|I|-/-/-|SD XL on Together AI|image",
        "together/openai/sora-2-pro|sora-2-pro|Sora 2 Pro|C|-/-|4K/1K|D|-/-/-|Sora 2 Pro on Together AI|video",
        "together/ByteDance/Seedance-1.0-lite|seedance-1.0-lite|ByteDance Seedance 1.0 Lite|C|-/-|4K/1K|D|-/-/-|ByteDance Seedance 1.0 Lite on Together AI|video",
        "together/kwaivgI/kling-1.6-pro|kling-1.6-pro|Kling 1.6 Pro|C|-/-|4K/1K|D|-/-/-|Kling 1.6 Pro on Together AI|video",
        "together/vidu/vidu-2.0|vidu-2.0|Vidu 2.0|C|-/-|4K/1K|D|-/-/-|Vidu 2.0 on Together AI|video",
        "together/kwaivgI/kling-2.1-pro|kling-2.1-pro|Kling 2.1 Pro|C|-/-|4K/1K|D|-/-/-|Kling 2.1 Pro on Together AI|video",
        "together/Virtue-AI/VirtueGuard-Text-Lite|virtueguard-text-lite|Virtueguard Text Lite|C|0.2000/0.2000|32K/4K|M|-/-/-|Virtueguard Text Lite on Together AI|chat",
        "together/arize-ai/qwen-2-1.5b-instruct|qwen-2-1.5b-instruct|Arize AI Qwen 2 1.5B Instruct|C|0.1000/0.1000|32K/4K|TJ|-/-/-|Arize AI Qwen 2 1.5B Instruct on Together AI|chat",
        "together/black-forest-labs/FLUX.1-dev|flux.1-dev|FLUX.1 [dev]|C|-/-|4K/1K|I|-/-/-|FLUX.1 [dev] on Together AI|image",
        "together/mistralai/Ministral-3-14B-Instruct-2512|ministral-3-14b-instruct-2512|Ministral 3 14B Instruct 2512|C|0.2000/0.2000|262K/4K|TJ|-/-/-|Ministral 3 14B Instruct 2512 on Together AI|chat",
        "together/meta-llama/Llama-Guard-3-11B-Vision-Turbo|llama-guard-3-11b-vision-turbo|Meta Llama Guard 3 11B Vision Turbo|C|0.1800/0.1800|131K/4K|M|-/-/-|Meta Llama Guard 3 11B Vision Turbo on Together AI|chat",
        "together/Qwen/Qwen2.5-72B-Instruct-Turbo|qwen2.5-72b-instruct-turbo|Qwen2.5 72B Instruct Turbo|C|1.20/1.20|131K/4K|TJS|-/-/-|Qwen2.5 72B Instruct Turbo on Together AI|chat",
        "together/Qwen/Qwen3-235B-A22B-fp8-tput|qwen3-235b-a22b-fp8-tput|Qwen3 235B A22B FP8 Throughput|C|0.2000/0.6000|40K/4K|TJ|-/-/-|Qwen3 235B A22B FP8 Throughput on Together AI|chat",
        "together/meta-llama/Meta-Llama-3-8B-Instruct-Lite|meta-llama-3-8b-instruct-lite|Meta Llama 3 8B Instruct Lite|C|0.1000/0.1000|8K/2K|TJ|-/-/-|Meta Llama 3 8B Instruct Lite on Together AI|chat",
        "together/moonshotai/Kimi-K2-Thinking|kimi-k2-thinking|Kimi K2 Thinking|C|1.20/4.00|262K/4K|T|-/-/-|Kimi K2 Thinking on Together AI|chat",
        "together/black-forest-labs/FLUX.1-schnell|flux.1-schnell|FLUX.1 Schnell|C|-/-|4K/1K|I|-/-/-|FLUX.1 Schnell on Together AI|image",
        "together/deepcogito/cogito-v2-preview-llama-405B|cogito-v2-preview-llama-405b|Deepcogito Cogito V2 Preview Llama 405B|C|3.50/3.50|32K/4K|T|-/-/-|Deepcogito Cogito V2 Preview Llama 405B on Together AI|chat",
        "together/zai-org/GLM-4.5-Air-FP8|glm-4.5-air-fp8|Glm 4.5 Air Fp8|C|0.2000/1.10|131K/4K|T|-/-/-|Glm 4.5 Air Fp8 on Together AI|chat",
        "together/togethercomputer/Refuel-Llm-V2|refuel-llm-v2|Refuel LLM V2|C|0.6000/0.6000|16K/4K|T|-/-/-|Refuel LLM V2 on Together AI|chat",
        "together/meta-llama/Llama-3-70b-chat-hf|llama-3-70b-chat-hf|Meta Llama 3 70B Instruct Reference|C|0.8800/0.8800|8K/2K|TJ|-/-/-|Meta Llama 3 70B Instruct Reference on Together AI|chat",
        "together/deepcogito/cogito-v2-preview-llama-109B-MoE|cogito-v2-preview-llama-109b-moe|Cogito V2 Preview Llama 109B MoE|C|0.1800/0.5900|32K/4K|T|-/-/-|Cogito V2 Preview Llama 109B MoE on Together AI|chat",
        "together/hexgrad/Kokoro-82M|kokoro-82m|Kokoro 82M|C|4.00/-|4K/1K|A|-/-/-|Kokoro 82M on Together AI|audio",
        "together/google/gemini-3-pro-image|gemini-3-pro-image|Gemini 3 (Nano Banana 2 Pro)|C|-/-|4K/1K|I|-/-/-|Gemini 3 (Nano Banana 2 Pro) on Together AI|image",
        "together/deepseek-ai/DeepSeek-V3|deepseek-v3|DeepSeek V3-0324|C|1.25/1.25|131K/4K|TJS|-/-/-|DeepSeek V3-0324 on Together AI|chat",
        "together/google/gemma-3n-E4B-it|gemma-3n-e4b-it|Gemma 3N E4B Instruct|C|0.0200/0.0400|32K/4K|T|-/-/-|Gemma 3N E4B Instruct on Together AI|chat",
        "together/essentialai/rnj-1-instruct|rnj-1-instruct|EssentialAI Rnj-1 Instruct|C|0.1500/0.1500|32K/4K|TJ|-/-/-|EssentialAI Rnj-1 Instruct on Together AI|chat",
        "together/Qwen/Qwen3-VL-32B-Instruct|qwen3-vl-32b-instruct|Qwen3-VL-32B-Instruct|C|0.5000/1.50|262K/4K|VTJ|-/-/-|Qwen3-VL-32B-Instruct on Together AI|chat",
        "together/togethercomputer/Refuel-Llm-V2-Small|refuel-llm-v2-small|Refuel LLM V2 Small|C|0.2000/0.2000|8K/2K|T|-/-/-|Refuel LLM V2 Small on Together AI|chat",
        "together/Qwen/Qwen3-235B-A22B-Instruct-2507-tput|qwen3-235b-a22b-instruct-2507-tput|Qwen3 235B A22B Instruct 2507 FP8 Throughput|C|0.2000/0.6000|262K/4K|TJ|-/-/-|Qwen3 235B A22B Instruct 2507 FP8 Throughput on Together AI|chat",
        "together/meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo|meta-llama-3.1-405b-instruct-turbo|Meta Llama 3.1 405B Instruct Turbo|C|3.50/3.50|10K/2K|TJS|-/-/-|Meta Llama 3.1 405B Instruct Turbo on Together AI|chat",
        "together/black-forest-labs/FLUX.2-pro|flux.2-pro|FLUX.2 [pro]|C|-/-|4K/1K|I|-/-/-|FLUX.2 [pro] on Together AI|image",
        "together/black-forest-labs/FLUX.2-dev|flux.2-dev|FLUX.2 [dev]|C|-/-|4K/1K|I|-/-/-|FLUX.2 [dev] on Together AI|image",
        "together/mistralai/Mistral-Small-24B-Instruct-2501|mistral-small-24b-instruct-2501|Mistral Small (24B) Instruct 25.01|C|0.1000/0.3000|32K/4K|TJ|-/-/-|Mistral Small (24B) Instruct 25.01 on Together AI|chat",
        "together/mistralai/Mixtral-8x7B-Instruct-v0.1|mixtral-8x7b-instruct-v0.1|Mixtral-8x7B Instruct v0.1|C|0.6000/0.6000|32K/4K|TJ|-/-/-|Mixtral-8x7B Instruct v0.1 on Together AI|chat",
        "together/nvidia/NVIDIA-Nemotron-Nano-9B-v2|nvidia-nemotron-nano-9b-v2|Nvidia Nemotron Nano 9B V2|C|0.0600/0.2500|131K/4K|T|-/-/-|Nvidia Nemotron Nano 9B V2 on Together AI|chat",
        "together/Qwen/Qwen2.5-7B-Instruct-Turbo|qwen2.5-7b-instruct-turbo|Qwen2.5 7B Instruct Turbo|C|0.3000/0.3000|32K/4K|TJS|-/-/-|Qwen2.5 7B Instruct Turbo on Together AI|chat",
        "together/Salesforce/Llama-Rank-V1|llama-rank-v1|Salesforce Llama Rank V1 (8B)|C|0.1000/0.1000|8K/2K|R|-/-/-|Salesforce Llama Rank V1 (8B) on Together AI|rerank",
        "together/canopylabs/orpheus-3b-0.1-ft|orpheus-3b-0.1-ft|Orpheus 3B 0.1 FT|C|0.2700/0.8500|4K/1K|A|-/-/-|Orpheus 3B 0.1 FT on Together AI|audio",
        "together/Qwen/Qwen3-VL-8B-Instruct|qwen3-vl-8b-instruct|Qwen3-VL-8B-Instruct|C|0.1800/0.6800|262K/4K|VTJ|-/-/-|Qwen3-VL-8B-Instruct on Together AI|chat",
        "together/Qwen/Qwen3-Next-80B-A3B-Instruct|qwen3-next-80b-a3b-instruct|Qwen3 Next 80B A3b Instruct|C|0.1500/1.50|262K/4K|TJ|-/-/-|Qwen3 Next 80B A3b Instruct on Together AI|chat",
        "together/ServiceNow-AI/Apriel-1.6-15b-Thinker|apriel-1.6-15b-thinker|Apriel 1.6 15B Thinker|C|-/-|131K/4K|T|-/-/-|Apriel 1.6 15B Thinker on Together AI|chat",
        "together/mistralai/Mistral-7B-Instruct-v0.3|mistral-7b-instruct-v0.3|Mistral (7B) Instruct v0.3|C|0.2000/0.2000|32K/4K|TJ|-/-/-|Mistral (7B) Instruct v0.3 on Together AI|chat",
        "together/meta-llama/Llama-3.2-3B-Instruct-Turbo|llama-3.2-3b-instruct-turbo|Meta Llama 3.2 3B Instruct Turbo|C|0.0600/0.0600|131K/4K|TJS|-/-/-|Meta Llama 3.2 3B Instruct Turbo on Together AI|chat",
        "together/moonshotai/Kimi-K2-Instruct-0905|kimi-k2-instruct-0905|Kimi K2-Instruct 0905|C|1.00/3.00|262K/4K|TJ|-/-/-|Kimi K2-Instruct 0905 on Together AI|chat",
        "together/togethercomputer/m2-bert-80M-32k-retrieval|m2-bert-80m-32k-retrieval|M2-BERT-Retrieval-32k|C|8000.00/8000.00|32K/4K|E|-/-/-|M2-BERT-Retrieval-32k on Together AI|embed",
        "together/openai/whisper-large-v3|whisper-large-v3|Whisper large-v3|C|0.2700/0.8500|4K/1K|A|-/-/-|Whisper large-v3 on Together AI|audio",
        "together/meta-llama/Llama-3.1-405B-Instruct|llama-3.1-405b-instruct|Meta Llama 3.1 405B Instruct|C|3.50/3.50|4K/1K|TJS|-/-/-|Meta Llama 3.1 405B Instruct on Together AI|chat",
        "together/meta-llama/Llama-3-70b-hf|llama-3-70b-hf|Meta Llama 3 70B HF|C|0.9000/0.9000|8K/2K|T|-/-/-|Meta Llama 3 70B HF on Together AI|chat",
        "together/Qwen/Qwen3-235B-A22B-Thinking-2507|qwen3-235b-a22b-thinking-2507|Qwen3 235B A22B Thinking 2507 FP8|C|0.6500/3.00|262K/4K|TJ|-/-/-|Qwen3 235B A22B Thinking 2507 FP8 on Together AI|chat",
        "together/meta-llama/Meta-Llama-3.1-70B-Instruct-Reference|meta-llama-3.1-70b-instruct-reference|Meta Llama 3.1 70B Instruct|C|0.9000/0.9000|8K/2K|TJS|-/-/-|Meta Llama 3.1 70B Instruct on Together AI|chat",
        "together/BAAI/bge-large-en-v1.5|bge-large-en-v1.5|BAAI-Bge-Large-1.5|C|0.0160/0.0160|4K/1K|E|-/-/-|BAAI-Bge-Large-1.5 on Together AI|embed",
        "together/meta-llama/Llama-3.2-1B-Instruct|llama-3.2-1b-instruct|Meta Llama 3.2 1B Instruct|C|0.0600/0.0600|131K/4K|TJS|-/-/-|Meta Llama 3.2 1B Instruct on Together AI|chat",
        "together/Qwen/Qwen2.5-14B-Instruct|qwen2.5-14b-instruct|Qwen 2.5 14B Instruct|C|0.8000/0.8000|32K/4K|TJS|-/-/-|Qwen 2.5 14B Instruct on Together AI|chat",
        "together/meta-llama/Meta-Llama-3.1-8B-Instruct-Reference|meta-llama-3.1-8b-instruct-reference|Meta Llama 3.1 8B Instruct|C|0.2000/0.2000|16K/4K|TJS|-/-/-|Meta Llama 3.1 8B Instruct on Together AI|chat",
        "together/meta-llama/Meta-Llama-3-8B-Instruct|meta-llama-3-8b-instruct|Meta Llama 3 8B Instruct|C|0.2000/0.2000|8K/2K|TJ|-/-/-|Meta Llama 3 8B Instruct on Together AI|chat",
        "together/BAAI/bge-base-en-v1.5|bge-base-en-v1.5|BAAI-Bge-Base-1.5|C|8000.00/8000.00|512/128|E|-/-/-|BAAI-Bge-Base-1.5 on Together AI|embed",
        "together/mistralai/Mistral-7B-Instruct-v0.2|mistral-7b-instruct-v0.2|Mistral (7B) Instruct v0.2|C|0.2000/0.2000|32K/4K|TJ|-/-/-|Mistral (7B) Instruct v0.2 on Together AI|chat",
        "together/openai/gpt-oss-20b|gpt-oss-20b|OpenAI GPT-OSS 20B|C|0.0500/0.2000|131K/4K|T|-/-/-|OpenAI GPT-OSS 20B on Together AI|chat",
        "together/deepseek-ai/DeepSeek-V3.1|deepseek-v3.1|Deepseek V3.1|C|0.6000/1.70|131K/4K|TJS|-/-/-|Deepseek V3.1 on Together AI|chat",
        "together/Qwen/Qwen2.5-72B-Instruct|qwen2.5-72b-instruct|Qwen2.5 72B Instruct|C|1.20/1.20|32K/4K|TJS|-/-/-|Qwen2.5 72B Instruct on Together AI|chat",
        "together/openai/gpt-oss-120b|gpt-oss-120b|OpenAI GPT-OSS 120B|C|0.1500/0.6000|131K/4K|T|-/-/-|OpenAI GPT-OSS 120B on Together AI|chat",
        "together/deepseek-ai/DeepSeek-R1-Distill-Llama-70B|deepseek-r1-distill-llama-70b|DeepSeek R1 Distill Llama 70B|C|2.00/2.00|131K/4K|TJK|-/-/-|DeepSeek R1 Distill Llama 70B on Together AI|chat",
        "together/llama-3-70b-instruct|llama-3-70b-t|Meta: Llama 3 70B (Together)|C|0.000900/0.000900|8K/2K|VSTJ|-/-/-|Llama 3 70B via Together AI|chat",
        "together/mixtral-8x22b-instruct|mixtral-8x22b-t|Mistral: Mixtral 8x22B (Together)|C|0.000900/0.000900|65K/2K|VSTJ|-/-/-|Mixtral 8x22B sparse mixture via Together|chat",
        "together/phi-3-mini-instruct|phi-3-mini-t|Microsoft: Phi-3 Mini (Together)|C|0.000080/0.000080|131K/2K|VSTJ|-/-/-|Lightweight Phi-3 Mini via Together|chat",
        "together/dbrx-instruct|dbrx-instruct-t|Databricks: DBRX (Together)|C|0.000600/0.000600|32K/2K|VSTJ|-/-/-|Databricks DBRX via Together|chat",
        "together/yi-large-turbo|yi-large-turbo-t|01.AI: Yi Large Turbo|C|0.000900/0.000900|200K/2K|VSTJ|-/-/-|Yi Large Turbo via Together|chat",
        "together/llava-1.6-34b|llava-1.6-34b|Together: LLaVA 1.6 34B|C|0.000001/0.000008|4K/2K|VST|-/-/-|Open-source LLaVA vision model with 34B parameters|chat",
        "together/llava-1.6-13b|llava-1.6-13b|Together: LLaVA 1.6 13B|C|0.000000/0.000003|4K/2K|VST|-/-/-|Efficient LLaVA vision model with 13B parameters|chat",
        "together/llava-onevision-7b|llava-onevision-7b|Together: LLaVA OneVision 7B|C|0.000000/0.000001|4K/1K|VST|-/-/-|Latest LLaVA OneVision compact model|chat",
        "together/llava-onevision-72b|llava-onevision-72b|Together: LLaVA OneVision 72B|C|0.000002/0.000015|4K/4K|VST|-/-/-|Large-scale LLaVA OneVision model|chat",

    // === TOKENIZE (2 models) ===
        "tokenize/gpt2-tokenizer|gpt2-tokenizer-f|GPT-2 Tokenizer|C|0.000010/0.000010|8K/256|VST|-/-/-|BPE tokenization|chat",
        "tokenize/sentencepiece|sentencepiece-f|SentencePiece|C|0.000010/0.000010|8K/256|VST|-/-/-|Subword tokenization|chat",

    // === TOT (1 models) ===
        "tot/tot-planner|tot-f|Tree-of-Thoughts Planner|C|0.000500/0.0015|4K/2K|VSTJK|-/-/-|Tree-of-thoughts planning|chat",

    // === TRANSFER (1 models) ===
        "transfer/xlm-cross-lingual|xlm-transfer-f|XLM Cross-Lingual|C|0.000120/0.000350|512/512|VSTJ|-/-/-|Cross-lingual transfer|chat",

    // === TRANSLATION (7 models) ===
        "translation/nllb-200|nllb-200-t|Meta: NLLB-200|C|0.000100/0.000300|1K/512|VTJ|-/-/-|Meta's No Language Left Behind 200|chat",
        "translation/m2m-100|m2m-100-t|Facebook: M2M-100|C|0.000080/0.000200|512/512|VTJ|-/-/-|Many-to-Many translation model|chat",
        "translation/nllb-200-distilled|nllb-200|nllb-dist|Meta: NLLB 200M|C|0.000000/0.000000|512/256|T|-/-/-|NLLB 200M distilled for 200 languages|chat",
        "translation/nllb-200-1.3b|nllb-1.3b|nllb-small|Meta: NLLB 1.3B|C|0.000000/0.000000|512/256|T|-/-/-|NLLB 1.3B for multilingual translation|chat",
        "translation/nllb-200-3.3b|nllb-3.3b|nllb-medium|Meta: NLLB 3.3B|C|0.000000/0.000000|512/512|T|-/-/-|NLLB 3.3B medium multilingual model|chat",
        "translation/m2m-100-418m|m2m-418m|m2m-small|Facebook: M2M 418M|C|0.000000/0.000000|512/256|T|-/-/-|M2M 100 418M many-to-many translation|chat",
        "translation/m2m-100-1.2b|m2m-1.2b|m2m-medium|Facebook: M2M 1.2B|C|0.000000/0.000000|512/256|T|-/-/-|M2M 100 1.2B enhanced translation|chat",

    // === UNDI95 (1 models) ===
        "undi95/remm-slerp-l2-13b|remm-slerp-l2-13b|ReMM SLERP 13B|C|0.000000/0.000001|6K/1K|JS|-/-/-|A recreation trial of the original MythoMax-L2-B13 but with updated models. #mer|chat",

    // === VASTAI (1 models) ===
        "vastai/neural-chat-7b|neural-chat-7b-v|Intel: Neural Chat 7B (Vast)|C|0.000100/0.000200|8K/2K|VSTJ|-/-/-|Intel Neural Chat via Vast AI|chat",

    // === VERTEX (2 models) ===
        "vertex/claude-3.5-sonnet-vision|vertex-claude-vision|GCP: Claude 3.5 Sonnet Vision|C|0.0030/0.0150|200K/4K|VSTJKC|-/-/-|Claude 3.5 Sonnet via Google Cloud Vertex AI|chat",
        "vertex/gemini-2.5-vision-exp|vertex-gemini-vision-exp|GCP: Gemini 2.5 Vision Exp|C|0.000003/0.000008|1000K/8K|VSTJK|-/-/-|Experimental Gemini 2.5 vision via Vertex|chat",

    // === VIDEO (5 models) ===
        "video/timesformer|timesformer-v|TimeSformer Video|C|0.000250/0.000250|8/8|VT|-/-/-|Transformer for video understanding|chat",
        "video/slowfusion|slowfusion-v|SlowFusion 3D CNN|C|0.000200/0.000200|16/16|VT|-/-/-|Spatiotemporal 3D CNN|chat",
        "video/actionclip|actionclip-v|ActionCLIP Video|C|0.000150/0.000150|8/8|VT|-/-/-|Contrastive learning for action|chat",
        "video/timesformer-base-finetuned-kinetics-400|timesformer|video-action|Meta: TimeSformer Kinetics|C|0.000000/0.000000|8/2K|V|-/-/-|Vision transformer for video action recognition|chat",
        "video/slowfast-101-r101|slowfusion|video-slowfast|SlowFusion 101|C|0.000000/0.000004|8/2K|V|-/-/-|SlowFast network for video understanding|chat",

    // === VISION (51 models) ===
        "vision/dinov2-large|dinov2-large-v|Meta: DINOv2 Large|C|0.000050/0.000100|2K/2K|VT|-/-/-|Vision backbone without labels|chat",
        "vision/blip-2|blip-2-v|Salesforce: BLIP-2|C|0.000100/0.000300|4K/2K|VT|-/-/-|Multimodal foundation model|chat",
        "vision/clip-vit-base-finetuned-multilingual|clip-multilingual|clip-ft|OpenAI: CLIP ViT Multilingual FT|C|0.000000/0.000000|512/256|VS|-/-/-|CLIP fine-tuned for multilingual image-text understanding|chat",
        "vision/dinov2-base-finetuned-object-detection|dinov2-det|dinov2-ft|Meta: DINOv2 Object Detection FT|C|0.000000/0.000000|512/256|VS|-/-/-|DINOv2 fine-tuned for object detection|chat",
        "vision/resnet50-finetuned-medical-imaging|resnet50-medical|resnet-medical-ft|ResNet50 Medical FT|C|0.000000/0.000000|224/256|VS|-/-/-|ResNet50 fine-tuned for medical imaging classification|chat",
        "vision/yolov8n|yolov8n-v|Ultralytics: YOLOv8 Nano|C|0.000010/0.000010|640/640|VT|-/-/-|Real-time object detection nano model|chat",
        "vision/yolov8s|yolov8s-v|Ultralytics: YOLOv8 Small|C|0.000020/0.000020|640/640|VT|-/-/-|Small object detection model|chat",
        "vision/yolov8m|yolov8m-v|Ultralytics: YOLOv8 Medium|C|0.000050/0.000050|640/640|VT|-/-/-|Medium object detection model|chat",
        "vision/yolov9-large|yolov9-large-v|Ultralytics: YOLOv9 Large|C|0.000100/0.000100|640/640|VT|-/-/-|Large YOLOv9 with attention|chat",
        "vision/faster-rcnn|faster-rcnn-v|Faster R-CNN ResNet50|C|0.000080/0.000080|800/800|VT|-/-/-|Regional CNN with FPN|chat",
        "vision/detr|detr-v|Facebook: DETR|C|0.000120/0.000120|800/800|VT|-/-/-|Detection Transformer architecture|chat",
        "vision/deeplab-v3+|deeplab-v3plus-v|DeepLab v3+ ResNet101|C|0.000100/0.000100|1K/1K|VT|-/-/-|Semantic segmentation with atrous convolution|chat",
        "vision/segformer|segformer-v|NVIDIA: SegFormer|C|0.000080/0.000080|1K/1K|VT|-/-/-|Efficient semantic segmentation|chat",
        "vision/mask-rcnn|mask-rcnn-v|Mask R-CNN FPN|C|0.000150/0.000150|1K/1K|VT|-/-/-|Instance segmentation with masks|chat",
        "vision/panoptic-fpn|panoptic-fpn-v|Panoptic FPN|C|0.000120/0.000120|1K/1K|VT|-/-/-|Panoptic segmentation FPN|chat",
        "vision/openpose|openpose-v|CMU: OpenPose|C|0.000080/0.000080|640/480|VT|-/-/-|Multi-person pose estimation|chat",
        "vision/yolov8-pose|yolov8-pose-v|Ultralytics: YOLOv8 Pose|C|0.000100/0.000100|640/640|VT|-/-/-|Real-time pose detection|chat",
        "vision/hrnet|hrnet-v|HRNet Pose|C|0.000100/0.000100|256/256|VT|-/-/-|High resolution pose network|chat",
        "vision/3dmax|3dmax-v|Kaolin: 3D Max Pool|C|0.000150/0.000150|1K/1K|VT|-/-/-|3D object detection|chat",
        "vision/nerf|nerf-v|NeRF Multi-View|C|0.000200/0.000200|512/512|VT|-/-/-|Neural radiance fields|chat",
        "vision/voxel-rcnn|voxel-rcnn-v|Voxel R-CNN|C|0.000150/0.000150|1K/1K|VT|-/-/-|3D point cloud detection|chat",
        "vision/paddleocr|paddleocr-v|PaddleOCR v4|C|0.000050/0.000150|2K/2K|VT|-/-/-|Multilingual OCR system|chat",
        "vision/tesseract-5|tesseract-5-v|Tesseract 5.0|C|0.000030/0.000080|2K/2K|VT|-/-/-|Open-source OCR engine|chat",
        "vision/easyocr|easyocr-v|EasyOCR|C|0.000060/0.000200|2K/2K|VT|-/-/-|Easy-to-use OCR toolkit|chat",
        "vision/surya-ocr|surya-ocr-v|Surya OCR|C|0.000100/0.000300|4K/4K|VT|-/-/-|Multilingual reading comprehension OCR|chat",
        "vision/efficientnet-b7|efficientnet-b7-v|EfficientNet-B7|C|0.000080/0.000080|600/600|VT|-/-/-|Efficient image classification|chat",
        "vision/convnext-large|convnext-large-v|ConvNeXt Large|C|0.000100/0.000100|224/224|VT|-/-/-|Vision backbone modernization|chat",
        "vision/vit-large-21k|vit-large-21k-v|Vision Transformer Large|C|0.000120/0.000120|224/224|VT|-/-/-|Large ViT pre-trained on 21K|chat",
        "vision/swin-large|swin-large-v|Swin Transformer Large|C|0.000100/0.000100|224/224|VT|-/-/-|Shifted window transformer|chat",
        "vision/medsam|medsam-v|Meta: Medical SAM|C|0.000200/0.000200|1K/1K|VT|-/-/-|Segment Anything for medical|chat",
        "vision/monai-segmentation|monai-seg-v|MONAI Segmentation|C|0.000180/0.000180|1K/1K|VT|-/-/-|Medical image segmentation|chat",
        "vision/radiomics|radiomics-v|Radiomics Feature Extractor|C|0.000150/0.000150|512/512|VT|-/-/-|Quantitative imaging analysis|chat",
        "vision/padim|padim-v|PaDiM Anomaly Detection|C|0.000080/0.000100|256/256|VT|-/-/-|Patch-level anomaly detection|chat",
        "vision/cutpaste|cutpaste-v|CutPaste Anomaly|C|0.000100/0.000150|256/256|VT|-/-/-|Self-supervised anomaly detection|chat",
        "vision/industrial-anomaly-detection|industrial-ad-v|Industrial Anomaly Detector|C|0.000120/0.000200|512/512|VT|-/-/-|Manufacturing defect detection|chat",
        "vision/retinaface|retinaface-v|RetinaFace|C|0.000080/0.000100|640/640|VT|-/-/-|Multi-task face detection|chat",
        "vision/arcface|arcface-v|ArcFace Recognition|C|0.000100/0.000150|512/512|VT|-/-/-|Large margin cosine loss|chat",
        "vision/vggface2|vggface2-v|VGGFace2|C|0.000080/0.000100|224/224|VT|-/-/-|Large-scale face recognition|chat",
        "vision/yolov8-traffic|yolov8-traffic-v|YOLOv8 Traffic|C|0.000200/0.000200|640/640|VT|-/-/-|Traffic sign and light detection|chat",
        "vision/bev-perception|bev-perception-v|BEV Perception|C|0.000250/0.000250|1K/1K|VT|-/-/-|Bird's eye view perception|chat",
        "vision/lidar-3d|lidar-3d-v|LiDAR 3D Detection|C|0.000300/0.000300|2K/2K|VT|-/-/-|LiDAR-based 3D object detection|chat",
        "vision/sentinel-classification|sentinel-class-v|Sentinel 2 Classification|C|0.000100/0.000150|512/512|VT|-/-/-|Land cover classification|chat",
        "vision/aerial-detection|aerial-detection-v|Aerial Object Detection|C|0.000120/0.000200|640/640|VT|-/-/-|High-resolution aerial detection|chat",
        "vision/change-detection|change-detection-v|Multi-temporal Change|C|0.000150/0.000250|512/512|VT|-/-/-|SAR change detection|chat",
        "vision/yolov5-small|yolov5-small|object-detect-small|YOLOv5 Small|C|0.000000/0.000000|640/512|V|-/-/-|YOLOv5 Small for object detection|chat",
        "vision/yolov5-medium|yolov5-medium|object-detect-med|YOLOv5 Medium|C|0.000000/0.000000|640/512|V|-/-/-|YOLOv5 Medium balanced detector|chat",
        "vision/faster-rcnn-resnet101-fpn|faster-rcnn|object-detect-rcnn|Faster R-CNN ResNet101 FPN|C|0.000000/0.000000|800/512|V|-/-/-|Faster R-CNN with FPN backbone|chat",
        "vision/deeplabv3-resnet50|deeplabv3|segment-deeplab|DeepLabV3 ResNet50|C|0.000000/0.000000|512/512|V|-/-/-|DeepLabV3 for semantic segmentation|chat",
        "vision/segformer-b0|segformer-small|segment-efficient|NVIDIA: SegFormer-B0|C|0.000000/0.000000|512/512|V|-/-/-|Efficient SegFormer for segmentation|chat",
        "vision/mask-rcnn-resnet50-fpn|mask-rcnn|segment-instance|Mask R-CNN ResNet50 FPN|C|0.000000/0.000000|800/512|V|-/-/-|Mask R-CNN for instance segmentation|chat",
        "vision/panoptic-fpn-resnet101|panoptic-fpn|segment-panoptic|Panoptic FPN ResNet101|C|0.000000/0.000000|800/512|V|-/-/-|Panoptic FPN combining semantic and instance|chat",

    // === VISION3D (2 models) ===
        "vision3d/pointnet-classification|pointnet|3d-classify|PointNet Classification|C|0.000000/0.000000|1K/256|V|-/-/-|PointNet for 3D point cloud classification|chat",
        "vision3d/pointnet++-segmentation|pointnet++|3d-segment|PointNet++ Segmentation|C|0.000000/0.000003|1K/256|V|-/-/-|PointNet++ for 3D segmentation|chat",

    // === VOLCENGINE (13 models) ===
        "volcengine/doubao-pro-256k|doubao-pro-256k|Doubao Pro 256K|C|0.6900/1.24|262K/8K|VTJS|-/-/-|Flagship 256K context model|chat",
        "volcengine/doubao-pro-128k|doubao-pro-128k|Doubao Pro 128K|C|0.6900/1.24|131K/8K|VTJS|-/-/-|High-performance 128K model|chat",
        "volcengine/doubao-pro-32k|doubao-pro-32k|Doubao Pro 32K|C|0.1100/0.2400|32K/8K|VTJS|-/-/-|Balanced 32K model|chat",
        "volcengine/doubao-pro-4k|doubao-pro-4k|Doubao Pro 4K|C|0.1100/0.2400|4K/4K|VTJS|-/-/-|Fast 4K model|chat",
        "volcengine/doubao-lite-128k|doubao-lite-128k|Doubao Lite 128K|C|0.0410/0.1240|131K/8K|TJS|-/-/-|Lightweight 128K model|chat",
        "volcengine/doubao-lite-32k|doubao-lite-32k|Doubao Lite 32K|C|0.0410/0.1240|32K/8K|TJS|-/-/-|Lightweight 32K model|chat",
        "volcengine/doubao-lite-4k|doubao-lite-4k|Doubao Lite 4K|C|0.0410/0.1240|4K/4K|TJS|-/-/-|Lightweight 4K model|chat",
        "volcengine/doubao-vision-pro-32k|doubao-vision-pro|Doubao Vision Pro 32K|C|0.2700/0.2700|32K/8K|VTJS|-/-/-|Vision-language model|chat",
        "volcengine/doubao-vision-lite-32k|doubao-vision-lite|Doubao Vision Lite 32K|C|0.1100/0.1100|32K/8K|VTJ|-/-/-|Lightweight vision model|chat",
        "volcengine/doubao-1.5-thinking-pro|doubao-thinking-pro|Doubao 1.5 Thinking Pro|C|0.5500/2.19|131K/16K|VTJSK|-/-/-|Chain-of-thought reasoning model|chat",
        "volcengine/doubao-1.5-thinking-pro-m|doubao-thinking-m|Doubao 1.5 Thinking Pro M|C|0.2800/0.8200|131K/16K|VTJSK|-/-/-|Medium thinking model|chat",
        "volcengine/doubao-embedding|doubao-embed|Doubao Embedding|C|0.000700/-|4K/2K|E|-/-/-|Text embeddings|embed",
        "volcengine/doubao-embedding-large|doubao-embed-large|Doubao Embedding Large|C|0.000700/-|4K/2K|E|-/-/-|Large text embeddings|embed",

    // === VOYAGE (12 models) ===
        "voyage/voyage-3.5|voyage-3.5|Voyage 3.5|C|0.0600/-|32K/1K|E|-/-/-|Latest embedding model, 1024 dimensions|embed",
        "voyage/voyage-3|voyage-3|Voyage 3|C|0.0600/-|32K/1K|E|-/-/-|General purpose embeddings|embed",
        "voyage/voyage-3-lite|voyage-3-lite|Voyage 3 Lite|C|0.0200/-|32K/512|E|-/-/-|Lightweight embeddings, 512 dimensions|embed",
        "voyage/voyage-code-3|voyage-code-3|Voyage Code 3|C|0.1800/-|32K/1K|E|-/-/-|Code-optimized embeddings|embed",
        "voyage/voyage-finance-2|voyage-finance|Voyage Finance 2|C|0.1200/-|32K/1K|E|-/-/-|Finance-optimized embeddings|embed",
        "voyage/voyage-law-2|voyage-law|Voyage Law 2|C|0.1200/-|16K/1K|E|-/-/-|Legal document embeddings|embed",
        "voyage/voyage-multilingual-2|voyage-multilingual|Voyage Multilingual 2|C|0.1200/-|32K/1K|E|-/-/-|Multilingual embeddings|embed",
        "voyage/voyage-02|voyage-2|Voyage 2|C|0.1000/-|16K/1K|E|-/-/-|Previous generation embeddings|embed",
        "voyage/voyage-large-2|voyage-large-2|Voyage Large 2|C|0.1200/-|16K/1K|E|-/-/-|Large embeddings, 1536 dimensions|embed",
        "voyage/voyage-large-2-instruct|voyage-large-instruct|Voyage Large 2 Instruct|C|0.1200/-|16K/1K|E|-/-/-|Instruction-tuned embeddings|embed",
        "voyage/rerank-2|rerank-2|Rerank 2|C|0.0500/-|32K/-|R|-/-/-|Document reranking, $0.05/1M tokens|rerank",
        "voyage/rerank-2-lite|rerank-2-lite|Rerank 2 Lite|C|0.0200/-|32K/-|R|-/-/-|Lightweight reranking, $0.02/1M tokens|rerank",

    // === X-AI (8 models) ===
        "x-ai/grok-4.1-fast|grok-4.1-fast|xAI: Grok 4.1 Fast|C|0.000000/0.000000|2000K/30K|JKSTV|-/-/-|Grok 4.1 Fast is xAI's best agentic tool calling model that shines in real-world|chat",
        "x-ai/grok-4-fast|grok-4-fast|xAI: Grok 4 Fast|C|0.000000/0.000000|2000K/30K|JKSTV|-/-/-|Grok 4 Fast is xAI's latest multimodal model with SOTA cost-efficiency and a 2M |chat",
        "x-ai/grok-code-fast-1|grok-code-fast-1|xAI: Grok Code Fast 1|C|0.000000/0.000002|256K/10K|JKST|-/-/-|Grok Code Fast 1 is a speedy and economical reasoning model that excels at agent|chat",
        "x-ai/grok-4|grok-4|xAI: Grok 4|C|0.000003/0.000015|256K/64K|JKSTV|-/-/-|Grok 4 is xAI's latest reasoning model with a 256k context window. It supports p|chat",
        "x-ai/grok-3-mini|grok-3-mini|xAI: Grok 3 Mini|C|0.000000/0.000000|131K/32K|JKST|-/-/-|A lightweight model that thinks before responding. Fast, smart, and great for lo|chat",
        "x-ai/grok-3|grok-3|xAI: Grok 3|C|0.000003/0.000015|131K/32K|JST|-/-/-|Grok 3 is the latest model from xAI. It's their flagship model that excels at en|chat",
        "x-ai/grok-3-mini-beta|grok-3-mini-beta|xAI: Grok 3 Mini Beta|C|0.000000/0.000000|131K/32K|JKT|-/-/-|Grok 3 Mini is a lightweight, smaller thinking model. Unlike traditional models |chat",
        "x-ai/grok-3-beta|grok-3-beta|xAI: Grok 3 Beta|C|0.000003/0.000015|131K/32K|JT|-/-/-|Grok 3 is the latest model from xAI. It's their flagship model that excels at en|chat",

    // === XAI (1 models) ===
        "xai/grok-vision|grok-vision|xAI: Grok Vision|C|0.000005/0.000015|128K/8K|VST|-/-/-|xAI Grok multimodal model with real-time data|chat",

    // === XIAOMI (1 models) ===
        "xiaomi/mimo-v2-flash:free|mimo-v2-flash:free|Xiaomi: MiMo-V2-Flash (free)|C|-/-|262K/65K|JKT|-/-/-|MiMo-V2-Flash is an open-source foundation language model developed by Xiaomi. I|chat",

    // === YI (10 models) ===
        "yi/yi-lightning|yi-lightning|Yi Lightning|C|0.1400/0.1400|16K/16K|TJS|-/-/-|Fast and affordable model|chat",
        "yi/yi-large|yi-large|Yi Large|C|0.4200/0.4200|32K/8K|TJS|-/-/-|Flagship large model|chat",
        "yi/yi-large-rag|yi-large-rag|Yi Large RAG|C|0.3500/0.3500|16K/8K|TJS|-/-/-|RAG-optimized model|chat",
        "yi/yi-large-turbo|yi-large-turbo|Yi Large Turbo|C|0.1700/0.1700|16K/8K|TJS|-/-/-|Fast large model|chat",
        "yi/yi-large-fc|yi-large-fc|Yi Large FC|C|0.4200/0.4200|32K/8K|TJS|-/-/-|Function calling model|chat",
        "yi/yi-medium|yi-medium|Yi Medium|C|0.0350/0.0350|16K/8K|TJ|-/-/-|Balanced medium model|chat",
        "yi/yi-medium-200k|yi-medium-200k|Yi Medium 200K|C|0.1700/0.1700|204K/8K|TJ|-/-/-|200K context model|chat",
        "yi/yi-spark|yi-spark|Yi Spark|C|0.0014/0.0014|16K/8K|TJ|-/-/-|Lightweight affordable model|chat",
        "yi/yi-vision|yi-vision|Yi Vision|C|0.0800/0.0800|16K/8K|VTJ|-/-/-|Vision-language model|chat",
        "yi/yi-vl-34b|yi-vl-34b|01.AI: Yi Vision 34B|C|0.000001/0.000010|8K/2K|VST|-/-/-|Yi Vision Language model with strong OCR capabilities|chat",

    // === Z-AI (9 models) ===
        "z-ai/glm-4.7|glm-4.7|Z.AI: GLM 4.7|C|0.000000/0.000002|202K/65K|JKST|-/-/-|GLM-4.7 is Z.AI's latest flagship model, featuring upgrades in two key areas: en|chat",
        "z-ai/glm-4.6v|glm-4.6v|Z.AI: GLM 4.6V|C|0.000000/0.000001|131K/24K|JKSTV|-/-/-|GLM-4.6V is a large multimodal model designed for high-fidelity visual understan|chat",
        "z-ai/glm-4.6|glm-4.6|Z.AI: GLM 4.6|C|0.000000/0.000002|202K/65K|JKST|-/-/-|Compared with GLM-4.5, this generation brings several key improvements:

Longer |chat",
        "z-ai/glm-4.6:exacto|glm-4.6:exacto|Z.AI: GLM 4.6 (exacto)|C|0.000000/0.000002|204K/131K|JKST|-/-/-|Compared with GLM-4.5, this generation brings several key improvements:

Longer |chat",
        "z-ai/glm-4.5v|glm-4.5v|Z.AI: GLM 4.5V|C|0.000001/0.000002|65K/16K|JKSTV|-/-/-|GLM-4.5V is a vision-language foundation model for multimodal agent applications|chat",
        "z-ai/glm-4.5|glm-4.5|Z.AI: GLM 4.5|C|0.000000/0.000002|131K/65K|JKST|-/-/-|GLM-4.5 is our latest flagship foundation model, purpose-built for agent-based a|chat",
        "z-ai/glm-4.5-air:free|glm-4.5-air:free|Z.AI: GLM 4.5 Air (free)|C|-/-|131K/131K|JKST|-/-/-|GLM-4.5-Air is the lightweight variant of our latest flagship model family, also|chat",
        "z-ai/glm-4.5-air|glm-4.5-air|Z.AI: GLM 4.5 Air|C|0.000000/0.000001|131K/98K|JKST|-/-/-|GLM-4.5-Air is the lightweight variant of our latest flagship model family, also|chat",
        "z-ai/glm-4-32b|glm-4-32b|Z.AI: GLM 4 32B|C|0.000000/0.000000|128K/32K|T|-/-/-|GLM 4 32B is a cost-effective foundation language model.

It can efficiently per|chat",

    // === ZEROSHOT (3 models) ===
        "zeroshot/bart-large-mnli|bart-mnli-f|BART Large MNLI|C|0.000150/0.000450|1K/256|VSTJ|-/-/-|BART zero-shot classification|chat",
        "zeroshot/mDeBERTa-large-zero-shot|mdeberta-zs|zeroshot-large|Microsoft: mDeBERTa Zero-Shot|C|0.000000/0.000000|512/256|S|-/-/-|Multilingual DeBERTa for zero-shot|chat",
        "zeroshot/distiluse-base-multilingual-cased-v2|distiluse-zs|zeroshot-sentence|Sentence Transformers: DistilUSE|C|0.000000/0.000000|512/256|VS|-/-/-|DistilUSE for zero-shot classification|chat",

    // === ZHIPU (17 models) ===
        "zhipu/glm-4-plus|glm-4-plus|GLM-4 Plus|C|0.6900/0.6900|131K/8K|VTJS|-/-/-|Enhanced GLM-4 for complex tasks|chat",
        "zhipu/glm-4-0520|glm-4|GLM-4|C|0.1400/0.1400|131K/8K|VTJS|-/-/-|Standard GLM-4 model|chat",
        "zhipu/glm-4-air|glm-4-air|GLM-4 Air|C|0.0014/0.0014|131K/8K|TJS|-/-/-|Lightweight GLM-4 variant|chat",
        "zhipu/glm-4-airx|glm-4-airx|GLM-4 AirX|C|0.0140/0.0140|8K/8K|TJS|-/-/-|Fast inference GLM-4|chat",
        "zhipu/glm-4-flash|glm-4-flash|GLM-4 Flash|C|0/0|131K/8K|TJS|-/-/-|Free tier GLM-4|chat",
        "zhipu/glm-4-flashx|glm-4-flashx|GLM-4 FlashX|C|0/0|131K/8K|TJS|-/-/-|Free tier fast GLM-4|chat",
        "zhipu/glm-4-long|glm-4-long|GLM-4 Long|C|0.0014/0.0014|1000K/8K|TJS|-/-/-|1M context for ultra-long documents|chat",
        "zhipu/glm-4v-plus|glm-4v-plus|GLM-4V Plus|C|0.1400/0.1400|8K/4K|VTJS|-/-/-|Enhanced vision model|chat",
        "zhipu/glm-4v|glm-4v|GLM-4V|C|0.0690/0.0690|8K/4K|VTJ|-/-/-|Standard vision model|chat",
        "zhipu/glm-4v-flash|glm-4v-flash|GLM-4V Flash|C|0/0|8K/4K|VTJ|-/-/-|Free tier vision model|chat",
        "zhipu/codegeex-4|codegeex-4|CodeGeeX 4|C|0.0014/0.0014|131K/8K|TJS|-/-/-|Specialized code generation|chat",
        "zhipu/glm-z1-preview|glm-z1|GLM-Z1 Preview|C|0.6900/0.6900|16K/16K|TJK|-/-/-|Reasoning model with thinking|chat",
        "zhipu/glm-z1-air-preview|glm-z1-air|GLM-Z1 Air Preview|C|0.0690/0.0690|16K/16K|TJK|-/-/-|Lightweight reasoning model|chat",
        "zhipu/glm-z1-flash-preview|glm-z1-flash|GLM-Z1 Flash Preview|C|0/0|16K/16K|TJK|-/-/-|Free tier reasoning model|chat",
        "zhipu/embedding-3|embedding-3|Embedding 3|C|0.000700/-|8K/2K|E|-/-/-|Text embeddings|embed",
        "zhipu/glm-4|glm-4|glm-4-turbo|Zhipu: GLM-4|C|0.000000/0.000000|8K/2K|VSTJ|-/-/-|Zhipu GLM-4 with multimodal capabilities|chat",
        "zhipu/glm-3.5-turbo|glm-3.5|glm-turbo|Zhipu: GLM-3.5 Turbo|C|0.000000/0.000000|8K/2K|VT|-/-/-|Fast GLM-3.5 Turbo for real-time applications|chat",

];