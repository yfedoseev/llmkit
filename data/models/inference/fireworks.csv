id,alias,name,status,input_price,output_price,cache_input_price,context_window,max_output,capabilities,quality,source,updated,description,mmlu_score,humaneval_score,math_score
fireworks/chronos-hermes-13b-v2,chronos-hermes-13b-v2,Chronos Hermes 13B v2,C,0.200000,0.200000,-,4096,4096,J,api,fireworks,2026-01-04,(chronos-13b-v2 + Nous-Hermes-Llama2-13b) 75/25 merge. This offers the imaginative writing style of ,-,-,-
fireworks/codegemma-2b,codegemma-2b,CodeGemma 2B,C,0.200000,0.200000,-,8192,8192,J,api,fireworks,2026-01-04,CodeGemma is a collection of lightweight open code models built on top of Gemma. CodeGemma models ar,-,-,-
fireworks/codegemma-7b,codegemma-7b,CodeGemma 7B,C,0.200000,0.200000,-,8192,8192,J,api,fireworks,2026-01-04,CodeGemma is a collection of lightweight open code models built on top of Gemma. CodeGemma models ar,-,-,-
fireworks/code-llama-13b,code-llama-13b,Code Llama 13B,C,0.200000,0.200000,-,16384,16384,J,api,fireworks,2026-01-04,Code Llama is a collection of pretrained and fine-tuned Large Language Models ranging in scale from ,-,-,-
fireworks/code-llama-13b-instruct,code-llama-13b,Code Llama 13B Instruct,C,0.200000,0.200000,-,16384,16384,J,api,fireworks,2026-01-04,Code Llama is a collection of pretrained and fine-tuned Large Language Models ranging in scale from ,-,-,-
fireworks/code-llama-13b-python,code-llama-13b-python,Code Llama 13B Python,C,0.200000,0.200000,-,16384,16384,J,api,fireworks,2026-01-04,Code Llama is a collection of pretrained and fine-tuned Large Language Models ranging in scale from ,-,-,-
fireworks/code-llama-34b,code-llama-34b,Code Llama 34B,C,0.100000,0.100000,-,16384,16384,J,api,fireworks,2026-01-04,Code Llama is a collection of pretrained and fine-tuned Large Language Models ranging in scale from ,-,-,-
fireworks/code-llama-34b-instruct,code-llama-34b,Code Llama 34B Instruct,C,0.100000,0.100000,-,16384,16384,J,api,fireworks,2026-01-04,Code Llama is a collection of pretrained and fine-tuned Large Language Models ranging in scale from ,-,-,-
fireworks/code-llama-34b-python,code-llama-34b-python,Code Llama 34B Python,C,0.100000,0.100000,-,16384,16384,J,api,fireworks,2026-01-04,Code Llama is a collection of pretrained and fine-tuned Large Language Models ranging in scale from ,-,-,-
fireworks/code-llama-70b,code-llama-70b,Code Llama 70B,C,0.900000,0.900000,-,4096,4096,J,api,fireworks,2026-01-04,Code Llama is a collection of pretrained and fine-tuned Large Language Models ranging in scale from ,-,-,-
fireworks/code-llama-70b-instruct,code-llama-70b,Code Llama 70B Instruct,C,0.900000,0.900000,-,4096,4096,J,api,fireworks,2026-01-04,Code Llama is a collection of pretrained and fine-tuned Large Language Models ranging in scale from ,-,-,-
fireworks/code-llama-70b-python,code-llama-70b-python,Code Llama 70B Python,C,0.900000,0.900000,-,4096,4096,J,api,fireworks,2026-01-04,Code Llama is a collection of pretrained and fine-tuned Large Language Models ranging in scale from ,-,-,-
fireworks/code-llama-7b,code-llama-7b,Code Llama 7B,C,0.200000,0.200000,-,16384,16384,J,api,fireworks,2026-01-04,Code Llama is a collection of pretrained and fine-tuned Large Language Models ranging in scale from ,-,-,-
fireworks/code-llama-7b-instruct,code-llama-7b,Code Llama 7B Instruct,C,0.200000,0.200000,-,16384,16384,J,api,fireworks,2026-01-04,Code Llama is a collection of pretrained and fine-tuned Large Language Models ranging in scale from ,-,-,-
fireworks/code-llama-7b-python,code-llama-7b-python,Code Llama 7B Python,C,0.200000,0.200000,-,16384,16384,J,api,fireworks,2026-01-04,Code Llama is a collection of pretrained and fine-tuned Large Language Models ranging in scale from ,-,-,-
fireworks/code-qwen-1p5-7b,code-qwen-1p5-7b,CodeQwen 1.5 7B,C,0.200000,0.200000,-,65536,16384,J,api,fireworks,2026-01-04,"CodeQwen1.5 is based on Qwen1.5, a language model series including decoder language models of differ",-,-,-
fireworks/cogito-v1-preview-llama-3b,cogito-v1-preview-llama-3b,Cogito v1 Preview Llama 3B,C,0.100000,0.100000,-,131072,16384,TJ,api,fireworks,2026-01-04,The Cogito LLMs are instruction tuned generative models that are also hybrid reasoning models,-,-,-
fireworks/cogito-v1-preview-llama-70b,cogito-v1-preview-llama-70b,Cogito v1 Preview Llama 70B,C,0.900000,0.900000,-,131072,16384,TJ,api,fireworks,2026-01-04,The Cogito LLMs are instruction tuned generative models that are also hybrid reasoning models,-,-,-
fireworks/cogito-v1-preview-llama-8b,cogito-v1-preview-llama-8b,Cogito v1 Preview Llama 8B,C,0.200000,0.200000,-,131072,16384,TJ,api,fireworks,2026-01-04,The Cogito LLMs are instruction tuned generative models that are also hybrid reasoning models,-,-,-
fireworks/cogito-v1-preview-qwen-14b,cogito-v1-preview-qwen-14b,Cogito v1 Preview Qwen 14B,C,0.100000,0.100000,-,131072,16384,TJ,api,fireworks,2026-01-04,The Cogito LLMs are instruction tuned generative models that are also hybrid reasoning models,-,-,-
fireworks/cogito-v1-preview-qwen-32b,cogito-v1-preview-qwen-32b,Cogito v1 Preview Qwen 32B,C,0.900000,0.900000,-,131072,16384,TJ,api,fireworks,2026-01-04,The Cogito LLMs are instruction tuned generative models that are also hybrid reasoning models,-,-,-
fireworks/deepseek-coder-1b-base,deepseek-coder-1b-base,DeepSeek Coder 1.3B Base,C,0.900000,0.900000,-,16384,16384,J,api,fireworks,2026-01-04,"DeepSeek Coder is composed of a series of code language models, each trained from scratch on 2T toke",-,-,-
fireworks/deepseek-coder-33b-instruct,deepseek-coder-33b,DeepSeek Coder 33B Instruct,C,0.100000,0.100000,-,16384,16384,J,api,fireworks,2026-01-04,"Deepseek Coder is composed of a series of code language models, each trained from scratch on 2T toke",-,-,-
fireworks/deepseek-coder-7b-base,deepseek-coder-7b-base,DeepSeek Coder 7B Base,C,0.200000,0.200000,-,4096,4096,J,api,fireworks,2026-01-04,"Deepseek Coder is composed of a series of code language models, each trained from scratch on 2T toke",-,-,-
fireworks/deepseek-coder-7b-base-v1p5,deepseek-coder-7b-base-v1p5,DeepSeek Coder 7B Base v1.5,C,0.200000,0.200000,-,4096,4096,J,api,fireworks,2026-01-04,The Deepseek Coder 7B Base v1.5 LLM is pre-trained from Deepseek 7B on 2T tokens by employing a wind,-,-,-
fireworks/deepseek-coder-7b-instruct-v1p5,deepseek-coder-7b-v1p5,DeepSeek Coder 7B Instruct v1.5,C,0.200000,0.200000,-,4096,4096,J,api,fireworks,2026-01-04,Deepseek-Coder-7B-Instruct-v1.5 is pre-trained from Deepseek-LLM 7B on 2T tokens by employing a wind,-,-,-
fireworks/deepseek-coder-v2-instruct,deepseek-coder-v2,DeepSeek Coder V2 Instruct,C,0.900000,0.900000,-,32768,16384,J,api,fireworks,2026-01-04,DeepSeek Coder V2 Instruct is a 236-billion-parameter open-source Mixture-of-Experts (MoE) code lang,-,-,-
fireworks/deepseek-coder-v2-lite-base,deepseek-coder-v2-lite-base,DeepSeek Coder V2 Lite Base,C,0.900000,0.900000,-,163840,16384,J,api,fireworks,2026-01-04,DeepSeek-Coder-V2 is an open-source Mixture-of-Experts (MoE) code language model that achieves perfo,-,-,-
fireworks/deepseek-coder-v2-lite-instruct,deepseek-coder-v2-lite,DeepSeek Coder V2 Lite Instruct,C,0.900000,0.900000,-,163840,16384,J,api,fireworks,2026-01-04,DeepSeek Coder V2 Lite Instruct is a 16-billion-parameter open-source Mixture-of-Experts (MoE) code ,-,-,-
fireworks/deepseek-prover-v2,deepseek-prover-v2,DeepSeek Prover V2,C,0.900000,0.900000,-,163840,16384,J,api,fireworks,2026-01-04,"DeepSeek-Prover-V2, an open-source large language model designed for formal theorem proving in Lean ",-,-,-
fireworks/deepseek-r1,deepseek-r1,DeepSeek R1 (Fast),C,3.000000,8.000000,-,163840,16384,J,api,fireworks,2026-01-04,DeepSeek R1 (Fast) is the speed-optimized serverless deployment of DeepSeek-R1. Compared to the Deep,-,-,-
fireworks/deepseek-r1-0528,deepseek-r1-0528,Deepseek R1 05/28,C,3.000000,8.000000,-,163840,16384,TJ,api,fireworks,2026-01-04,05/28 updated checkpoint of Deepseek R1. Its overall performance is now approaching that of leading ,-,-,-
fireworks/deepseek-r1-0528-distill-qwen3-8b,deepseek-r1-0528-distill-qwen-3-8b,DeepSeek R1 0528 Distill Qwen3 8B,C,0.200000,0.200000,-,131072,16384,TJ,api,fireworks,2026-01-04,"We distilled the chain-of-thought from DeepSeek-R1-0528 to post-train Qwen3 8B Base, obtaining DeepS",-,-,-
fireworks/deepseek-r1-basic,deepseek-r1,DeepSeek R1 (Basic),C,3.000000,8.000000,-,163840,16384,J,api,fireworks,2026-01-04,DeepSeek R1 (Basic) is the cost-optimized serverless deployment of DeepSeek-R1. Compared to the Deep,-,-,-
fireworks/deepseek-r1-distill-llama-70b,deepseek-r1-distill-llama-70b,DeepSeek R1 Distill Llama 70B,C,0.900000,0.900000,-,131072,16384,J,api,fireworks,2026-01-04,Llama 70B distilled with reasoning from Deepseek R1,-,-,-
fireworks/deepseek-r1-distill-llama-8b,deepseek-r1-distill-llama-8b,DeepSeek R1 Distill Llama 8B,C,0.200000,0.200000,-,131072,16384,J,api,fireworks,2026-01-04,Llama 8B distilled with reasoning from Deepseek R1,-,-,-
fireworks/deepseek-r1-distill-qwen-14b,deepseek-r1-distill-qwen-14b,DeepSeek R1 Distill Qwen 14B,C,0.100000,0.100000,-,131072,16384,J,api,fireworks,2026-01-04,Qwen 14B distilled with reasoning from Deepseek R1,-,-,-
fireworks/deepseek-r1-distill-qwen-1p5b,deepseek-r1-distill-qwen-1p5b,DeepSeek R1 Distill Qwen 1.5B,C,3.000000,8.000000,-,131072,16384,J,api,fireworks,2026-01-04,Qwen 1.5B distilled with reasoning from Deepseek R1,-,-,-
fireworks/deepseek-r1-distill-qwen-32b,deepseek-r1-distill-qwen-32b,DeepSeek R1 Distill Qwen 32B,C,0.900000,0.900000,-,131072,16384,J,api,fireworks,2026-01-04,Qwen 32B distilled with reasoning from Deepseek R1,-,-,-
fireworks/deepseek-r1-distill-qwen-7b,deepseek-r1-distill-qwen-7b,DeepSeek R1 Distill Qwen 7B,C,0.200000,0.200000,-,131072,16384,J,api,fireworks,2026-01-04,Qwen 7B distilled with reasoning from Deepseek R1,-,-,-
fireworks/deepseek-v2-lite-chat,deepseek-v2-lite,DeepSeek V2 Lite Chat,C,0.900000,0.900000,-,163840,16384,J,api,fireworks,2026-01-04,"DeepSeek-V2, a strong Mixture-of-Experts (MoE) language model characterized by economical training a",-,-,-
fireworks/deepseek-v2p5,deepseek-v2p5,DeepSeek V2.5,C,0.900000,0.900000,-,32768,16384,J,api,fireworks,2026-01-04,DeepSeek-V2.5 is an upgraded version that combines DeepSeek-V2-Chat and DeepSeek-Coder-V2-Instruct.,-,-,-
fireworks/deepseek-v3,deepseek-v3,DeepSeek V3,C,0.900000,0.900000,-,131072,16384,TJ,api,fireworks,2026-01-04,A a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for,-,-,-
fireworks/deepseek-v3-0324,deepseek-v3-0324,Deepseek V3 03-24,C,0.900000,0.900000,-,163840,16384,TJ,api,fireworks,2026-01-04,A strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for e,-,-,-
fireworks/deepseek-v3p1,deepseek-v3p1,DeepSeek V3.1,C,0.900000,0.900000,-,163840,16384,TJ,api,fireworks,2026-01-04,"DeepSeek-V3.1 is post-trained on the top of DeepSeek-V3.1-Base, which is built upon the original V3 ",-,-,-
fireworks/deepseek-v3p1-terminus,deepseek-v3p1-terminus,DeepSeek V3.1 Terminus,C,0.900000,0.900000,-,163840,16384,TJ,api,fireworks,2026-01-04,"DeepSeek-V3.1-Terminus is an updated version of DeepSeek-V3.1 with enhanced language consistency, re",-,-,-
fireworks/deepseek-v3p2,deepseek-v3p2,Deepseek v3.2,C,0.900000,0.900000,-,163840,16384,TJ,api,fireworks,2026-01-04,Model from Deepseek that harmonizes high computational efficiency with superior reasoning and agent ,-,-,-
fireworks/devstral-small-2-24b-instruct-2512,devstral-small-2-24b-2512,Devstral Small 2 24B Instruct 2512,C,0.100000,0.100000,-,4096,4096,VT,api,fireworks,2026-01-04,Devstral is an agentic LLM for software engineering tasks. Devstral Small 2 excels at using tools to,-,-,-
fireworks/devstral-small-2505,devstral-small-2505,Devstral-Small-2505,C,0.200000,0.200000,-,131072,16384,J,api,fireworks,2026-01-04,Devstral is an agentic LLM for software engineering tasks built under a collaboration between Mistra,-,-,-
fireworks/dolphin-2-9-2-qwen2-72b,dolphin-2-9-2-qwen2-72b,Dolphin 2.9.2 Qwen2 72B,C,0.900000,0.900000,-,131072,16384,J,api,fireworks,2026-01-04,Dolphin 2.9.2 Qwen2 72B is a fine-tuned version of the Qwen2 72B Large Language Model with a variety,-,-,-
fireworks/dolphin-2p6-mixtral-8x7b,dolphin-2p6-mixtral-8x7b,Dolphin 2.6 Mixtral 8x7b,C,0.200000,0.200000,-,32768,16384,J,api,fireworks,2026-01-04,Dolphin 2.6 Mixtral 8x7b is a fine-tuned version of the Mixtral-8x7b Large Language Model specializi,-,-,-
fireworks/eagle1-kimi-k2-instruct-0905-v0,eagle1-kimi-k2-0905-v0,EAGLE1 kimi-k2-instruct-0905 v0,C,0.350000,1.400000,-,4096,4096,-,api,fireworks,2026-01-04,EAGLE1 for kimi-k2-instruct-0905 v0,-,-,-
fireworks/eagle3-kimi-k2-instruct-0905-v0,eagle3-kimi-k2-0905-v0,EAGLE3 kimi-k2-instruct-0905 v0,C,0.350000,1.400000,-,4096,4096,-,api,fireworks,2026-01-04,EAGLE3 for kimi-k2-instruct-0905 v0,-,-,-
fireworks/eagle-llama-v3-8b-instruct-v1,eagle-llama-v3-8b-v1,EAGLE Llama 3 8B V1,C,0.200000,0.200000,-,4096,4096,-,api,fireworks,2026-01-04,EAGLE draft model for Llama 3.x 8B instruct models,-,-,-
fireworks/eagle-llama-v3-8b-instruct-v2,eagle-llama-v3-8b-v2,EAGLE Llama 3 8B V2,C,0.200000,0.200000,-,4096,4096,-,api,fireworks,2026-01-04,EAGLE draft model for Llama 3.x 8B instruct models,-,-,-
fireworks/eagle-qwen-v2p5-3b-instruct-v2,eagle-qwen-v2p5-3b-v2,EAGLE Qwen 2.5 3B Instruct V2,C,0.100000,0.100000,-,4096,4096,-,api,fireworks,2026-01-04,EAGLE draft model for Qwen 2.5 3B instruct models,-,-,-
fireworks/ernie-4p5-21b-a3b-pt,ernie-4p5-21b-a3b-pt,ERNIE-4.5-21B-A3B-PT,C,0.100000,0.100000,-,131072,16384,J,api,fireworks,2026-01-04,"ERNIE-4.5-21B-A3B is a text MoE Post-trained model, with 21B total parameters and 3B activated param",-,-,-
fireworks/ernie-4p5-300b-a47b-pt,ernie-4p5-300b-a47b-pt,ERNIE-4.5-300B-A47B-PT,C,0.200000,0.200000,-,131072,16384,J,api,fireworks,2026-01-04,"ERNIE-4.5-300B-A47B is a text MoE Post-trained model, with 300B total parameters and 47B activated p",-,-,-
fireworks/fare-20b,fare-20b,FARE-20B,C,0.200000,0.200000,-,131072,16384,J,api,fireworks,2026-01-04,"FARE-20B is a multi-task evaluator fine-tuned from gpt-oss-20B. It’s trained on a broad, multi-domai",-,-,-
fireworks/firefunction-v1,firefunction-v1,FireFunction V1,C,0.200000,0.200000,-,32768,16384,T,api,fireworks,2026-01-04,Fireworks' open-source function calling model.,-,-,-
fireworks/firefunction-v2,firefunction-v2,FireFunction V2,C,0.200000,0.200000,-,4096,4096,T,api,fireworks,2026-01-04,Fireworks' latest and most performant function-calling model. Firefunction-v2 is based on Llama-3 an,-,-,-
fireworks/firesearch-ocr-v6,firesearch-ocr-v6,Firesearch OCR V6,C,0.200000,0.200000,-,8192,8192,VJ,api,fireworks,2026-01-04,OCR model provided by Fireworks,-,-,-
fireworks/flux-1-dev-controlnet-union,flux-1-dev-controlnet-union,FLUX.1 [dev] ControlNet,C,0.200000,0.200000,-,4096,4096,-,api,fireworks,2026-01-04,Unified ControlNet for FLUX.1-dev model jointly released by researchers from InstantX Team and Shakk,-,-,-
fireworks/flux-1-dev-fp8,flux-1-dev-fp8,FLUX.1 [dev] FP8,C,0.025000,-,-,4096,0,I,api,fireworks,2026-01-04,FLUX.1 [dev] is a 12 billion parameter rectified flow transformer capable of generating images from ,-,-,-
fireworks/flux-1-schnell,flux-1-schnell,FLUX.1 [schnell],C,0.025000,-,-,4096,0,I,api,fireworks,2026-01-04,FLUX.1 [schnell] is a 12 billion parameter rectified flow transformer capable of generating images f,-,-,-
fireworks/flux-1-schnell-fp8,flux-1-schnell-fp8,FLUX.1 [schnell] FP8,C,0.025000,-,-,4096,0,I,api,fireworks,2026-01-04,FLUX.1 [schnell] is a 12 billion parameter rectified flow transformer capable of generating images f,-,-,-
fireworks/flux-kontext-max,flux-kontext-max,Flux Kontext Max,C,0.050000,-,-,4096,0,I,api,fireworks,2026-01-04,FLUX Kontext Max is Black Forest Labs' new premium model that brings maximum performance across all ,-,-,-
fireworks/flux-kontext-pro,flux-kontext-pro,Flux Kontext Pro,C,0.050000,-,-,4096,0,I,api,fireworks,2026-01-04,FLUX Kontext Pro is a specialized model for generating contextually-aware images from text descripti,-,-,-
fireworks/full-llama-v3p1-8b-instruct-8b-fp8,full-llama-1-8b-8b-fp8,Llama 3.1 8B Instruct FP8 [Full],C,0.200000,0.200000,-,4096,4096,-,api,fireworks,2026-01-04,The Meta Llama 3.1 collection of multilingual large language models (LLMs) is a collection of pretra,-,-,-
fireworks/full-llama-v3p1-8b-instruct-8b-fp8-amd,full-llama-1-8b-8b-fp8-amd,Llama 3.1 8B Instruct FP8 AMD [Full],C,0.200000,0.200000,-,4096,4096,-,api,fireworks,2026-01-04,The Meta Llama 3.1 collection of multilingual large language models (LLMs) is a collection of pretra,-,-,-
fireworks/gemma2-9b-it,gemma2-9b-it,Gemma 2 9B Instruct,C,0.200000,0.200000,-,8192,8192,J,api,fireworks,2026-01-04,"Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same rese",-,-,-
fireworks/gemma-2b-it,gemma-2b-it,Gemma 2B Instruct,C,0.200000,0.200000,-,8192,8192,J,api,fireworks,2026-01-04,"Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same rese",-,-,-
fireworks/gemma-3-12b-it,gemma-3-12b-it,Gemma 3 12B Instruct,C,0.200000,0.200000,-,131072,16384,J,api,fireworks,2026-01-04,"Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same rese",-,-,-
fireworks/gemma-3-27b-it,gemma-3-27b-it,Gemma 3 27B Instruct,C,0.200000,0.200000,-,131072,16384,J,api,fireworks,2026-01-04,Gemma 3 27B Instruct,-,-,-
fireworks/gemma-3-4b-it,gemma-3-4b-it,Gemma 3 4B Instruct,C,0.100000,0.100000,-,131072,16384,J,api,fireworks,2026-01-04,"Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same rese",-,-,-
fireworks/gemma-7b,gemma-7b,Gemma 7B,C,0.200000,0.200000,-,8192,8192,J,api,fireworks,2026-01-04,"Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same rese",-,-,-
fireworks/gemma-7b-it,gemma-7b-it,Gemma 7B Instruct,C,0.200000,0.200000,-,8192,8192,J,api,fireworks,2026-01-04,"Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same rese",-,-,-
fireworks/glm-4p5,glm-4p5,GLM-4.5,C,0.350000,0.350000,-,131072,16384,TJ,api,fireworks,2026-01-04,The GLM-4.5 series models are foundation models designed for intelligent agents. GLM-4.5 has 355 bil,-,-,-
fireworks/glm-4p5-air,glm-4p5-air,GLM-4.5-Air,C,0.350000,0.350000,-,131072,16384,TJ,api,fireworks,2026-01-04,The GLM-4.5 series models are foundation models designed for intelligent agents. GLM-4.5 has 355 bil,-,-,-
fireworks/glm-4p5v,glm-4p5v,GLM-4.5V,C,0.350000,0.350000,-,131072,16384,VTJ,api,fireworks,2026-01-04,GLM-4.5V is based on ZhipuAI’s next-generation flagship text foundation model GLM-4.5-Air (106B para,-,-,-
fireworks/glm-4p6,glm-4p6,GLM-4.6,C,0.350000,0.350000,-,202752,16384,TJ,api,fireworks,2026-01-04,"As the latest iteration in the GLM series, GLM-4.6 achieves comprehensive enhancements across multip",-,-,-
fireworks/glm-4p7,glm-4p7,GLM-4.7,C,0.350000,0.350000,-,202752,16384,TJ,api,fireworks,2026-01-04,"GLM-4.7 is a next-generation general-purpose model optimized for coding, reasoning, and agentic work",-,-,-
fireworks/gpt-oss-120b,gpt-oss-120b,OpenAI gpt-oss-120b,C,0.500000,0.500000,-,131072,16384,TJ,api,fireworks,2026-01-04,"Welcome to the gpt-oss series, OpenAI's open-weight models designed for powerful reasoning, agentic ",-,-,-
fireworks/gpt-oss-20b,gpt-oss-20b,OpenAI gpt-oss-20b,C,0.500000,0.500000,-,131072,16384,J,api,fireworks,2026-01-04,"Welcome to the gpt-oss series, OpenAI's open-weight models designed for powerful reasoning, agentic ",-,-,-
fireworks/gpt-oss-20b-eagle3-v1,gpt-oss-20b-eagle3-v1,gpt-oss-20b-drafter,C,0.500000,0.500000,-,4096,4096,-,api,fireworks,2026-01-04,gpt-oss-20b drafter,-,-,-
fireworks/gpt-oss-safeguard-120b,gpt-oss-safeguard-120b,OpenAI gpt-oss-safeguard-120b,C,0.500000,0.500000,-,131072,16384,TJ,api,fireworks,2026-01-04,gpt-oss-safeguard-120b is a safety-focused language model with 117B total parameters and 5.1B active,-,-,-
fireworks/gpt-oss-safeguard-20b,gpt-oss-safeguard-20b,OpenAI gpt-oss-safeguard-20b,C,0.500000,0.500000,-,131072,16384,TJ,api,fireworks,2026-01-04,gpt-oss-safeguard-20b is a safety-focused language model with 21B total parameters and 3.6B active p,-,-,-
fireworks/hermes-2-pro-mistral-7b,hermes-2-pro-mistral-7b,Hermes 2 Pro Mistral 7B,C,0.200000,0.200000,-,32768,16384,TJ,api,fireworks,2026-01-04,"Latest version of Nous Research's Hermes series of models, using an updated and cleaned version of t",-,-,-
fireworks/internvl3-38b,internvl3-38b,InternVL3 38B,C,0.200000,0.200000,-,16384,16384,VJ,api,fireworks,2026-01-04,The InternVL3 collection of models are advanced multimodal large language models that combine superi,-,-,-
fireworks/internvl3-78b,internvl3-78b,InternVL3 78B,C,0.200000,0.200000,-,16384,16384,VJ,api,fireworks,2026-01-04,The InternVL3 collection of models are advanced multimodal large language models that combine superi,-,-,-
fireworks/internvl3-8b,internvl3-8b,InternVL3 8B,C,0.200000,0.200000,-,16384,16384,VJ,api,fireworks,2026-01-04,The InternVL3 collection of models are advanced multimodal large language models that combine superi,-,-,-
fireworks/kat-coder,kat-coder,KAT Coder,C,0.200000,0.200000,-,262144,16384,J,api,fireworks,2026-01-04,KAT-Coder-Pro V1 is KwaiKAT's most advanced agentic coding model in the KwaiKAT series. Designed spe,-,-,-
fireworks/kat-dev-32b,kat-dev-32b,KAT Dev 32B,C,0.900000,0.900000,-,131072,16384,J,api,fireworks,2026-01-04,KAT-Dev-32B is an open-source 32B-parameter model for software engineering tasks. It is optimized vi,-,-,-
fireworks/kat-dev-72b-exp,kat-dev-72b-exp,KAT Dev 72B Exp,C,0.900000,0.900000,-,131072,16384,TJ,api,fireworks,2026-01-04,KAT-Dev-72B-Exp is an open-source 72B-parameter model for software engineering tasks. KAT-Dev-72B-Ex,-,-,-
fireworks/kimi-k2-instruct,kimi-k2,Kimi K2 Instruct,C,0.350000,1.400000,-,131072,16384,TJ,api,fireworks,2026-01-04,Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated para,-,-,-
fireworks/kimi-k2-instruct-0905,kimi-k2-0905,Kimi K2 Instruct 0905,C,0.350000,1.400000,-,262144,16384,TJ,api,fireworks,2026-01-04,"Kimi K2 0905 is an updated version of Kimi K2, a state-of-the-art mixture-of-experts (MoE) language ",-,-,-
fireworks/kimi-k2-thinking,kimi-k2-thinking,Kimi K2 Thinking,C,0.350000,1.400000,-,4096,4096,TK,api,fireworks,2026-01-04,"Kimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Ki",-,-,-
fireworks/llama4-maverick-instruct-basic,llama4-maverick,Llama 4 Maverick Instruct (Basic),C,0.220000,0.880000,-,1048576,16384,VTJ,api,fireworks,2026-01-04,The Llama 4 collection of models are natively multimodal AI models that enable text and multimodal e,-,-,-
fireworks/llama4-scout-instruct-basic,llama4-scout,Llama 4 Scout Instruct (Basic),C,0.150000,0.600000,-,1048576,16384,VTJ,api,fireworks,2026-01-04,The Llama 4 collection of models are natively multimodal AI models that enable text and multimodal e,-,-,-
fireworks/llama-guard-2-8b,llama-guard-2-8b,Llama Guard v2 8B,C,0.200000,0.200000,-,8192,8192,J,api,fireworks,2026-01-04,"Meta Llama Guard 2 is an 8B parameter Llama 3-based LLM safeguard model. Similar to Llama Guard, it ",-,-,-
fireworks/llama-guard-3-1b,llama-guard-3-1b,Llama Guard v3 1B,C,0.200000,0.200000,-,131072,16384,J,api,fireworks,2026-01-04,Llama Guard 3-1B is a fine-tuned Llama-3.2-1B pretrained model for content safety classification. Si,-,-,-
fireworks/llama-guard-3-8b,llama-guard-3-8b,Llama Guard 3 8B,C,0.200000,0.200000,-,131072,16384,J,api,fireworks,2026-01-04,"Llama Guard 3 is a Llama-3.1-8B pretrained model, fine-tuned for content safety classification. Simi",-,-,-
fireworks/llamaguard-7b,llamaguard-7b,Llama Guard 7B,C,0.200000,0.200000,-,4096,4096,J,api,fireworks,2026-01-04,Llama-Guard is a 7B parameter Llama 2-based input-output safeguard model. It can be used for classif,-,-,-
fireworks/llama-v2-13b,llama-v2-13b,Llama 2 13B,C,0.200000,0.200000,-,4096,4096,J,api,fireworks,2026-01-04,Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 ,-,-,-
fireworks/llama-v2-13b-chat,llama-v2-13b,Llama 2 13B Chat,C,0.200000,0.200000,-,4096,4096,J,api,fireworks,2026-01-04,Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 ,-,-,-
fireworks/llama-v2-70b,llama-v2-70b,Llama 2 70B,C,0.900000,0.900000,-,4096,4096,J,api,fireworks,2026-01-04,"Meta developed and publicly released the Llama 2 family of large language models (LLMs), a collectio",-,-,-
fireworks/llama-v2-7b,llama-v2-7b,Llama 2 7B,C,0.200000,0.200000,-,4096,4096,J,api,fireworks,2026-01-04,Meta's Llama 2 model family is a collection of pretrained and fine-tuned generative text models trai,-,-,-
fireworks/llama-v2-7b-chat,llama-v2-7b,Llama 2 7B Chat,C,0.200000,0.200000,-,4096,4096,J,api,fireworks,2026-01-04,Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 ,-,-,-
fireworks/llama-v3-70b-instruct,llama-v3-70b,Llama 3 70B Instruct,C,0.900000,0.900000,-,8192,8192,J,api,fireworks,2026-01-04,"Meta developed and released the Meta Llama 3 family of large language models (LLMs), a collection of",-,-,-
fireworks/llama-v3-70b-instruct-hf,llama-v3-70b-hf,Llama 3 70B Instruct (HF version),C,0.900000,0.900000,-,8192,8192,J,api,fireworks,2026-01-04,Meta’s Llama 3 instruction tuned models are optimized for dialogue use cases and outperform many of ,-,-,-
fireworks/llama-v3-70b-instruct-v2,llama-v3-70b-v2,Llama v3 70B Instruct V2 Draft Model,C,0.900000,0.900000,-,4096,4096,-,api,fireworks,2026-01-04,"Meta developed and released the Meta Llama 3 family of large language models (LLMs), a collection of",-,-,-
fireworks/llama-v3-8b,llama-v3-8b,Llama 3 8B,C,0.200000,0.200000,-,8192,8192,J,api,fireworks,2026-01-04,Llama 3 is an auto-regressive language model that uses an optimized transformer architecture. The tu,-,-,-
fireworks/llama-v3-8b-instruct,llama-v3-8b,Llama 3 8B Instruct,C,0.200000,0.200000,-,8192,8192,J,api,fireworks,2026-01-04,"Meta developed and released the Meta Llama 3 family of large language models (LLMs), a collection of",-,-,-
fireworks/llama-v3-8b-instruct-hf,llama-v3-8b-hf,Llama 3 8B Instruct (HF version),C,0.200000,0.200000,-,8192,8192,J,api,fireworks,2026-01-04,Meta's Llama 3 instruction tuned models are optimized for dialogue use cases and outperform many of ,-,-,-
fireworks/llama-v3-8b-instruct-v0,llama-v3-8b-v0,Llama v3 8B Instruct V0 Draft Model,C,0.200000,0.200000,-,4096,4096,-,api,fireworks,2026-01-04,"Meta developed and released the Meta Llama 3 family of large language models (LLMs), a collection of",-,-,-
fireworks/llama-v3p1-405b-instruct,llama-1-405b,Llama 3.1 405B Instruct,C,3.000000,3.000000,-,131072,16384,TJ,api,fireworks,2026-01-04,The Meta Llama 3.1 collection of multilingual large language models (LLMs) is a collection of pretra,-,-,-
fireworks/llama-v3p1-405b-instruct-long,llama-1-405b-long,Llama 3.1 405B Instruct Long,C,3.000000,3.000000,-,4096,4096,-,api,fireworks,2026-01-04,The Meta Llama 3.1 collection of multilingual large language models (LLMs) is a collection of pretra,-,-,-
fireworks/llama-v3p1-70b-instruct,llama-1-70b,Llama 3.1 70B Instruct,C,0.900000,0.900000,-,131072,16384,TJ,api,fireworks,2026-01-04,The Meta Llama 3.1 collection of multilingual large language models (LLMs) is a collection of pretra,-,-,-
fireworks/llama-v3p1-70b-instruct-1b,llama-1-70b-1b,Llama 3.1 70B Instruct 1B,C,0.900000,0.900000,-,4096,4096,-,api,fireworks,2026-01-04,The Meta Llama 3.1 collection of multilingual large language models (LLMs) is a collection of pretra,-,-,-
fireworks/llama-v3p1-8b-instruct,llama-1-8b,Llama 3.1 8B Instruct,C,0.200000,0.200000,-,131072,16384,J,api,fireworks,2026-01-04,The Meta Llama 3.1 collection of multilingual large language models (LLMs) is a collection of pretra,-,-,-
fireworks/llama-v3p1-nemotron-70b-instruct,llama-1-nemotron-70b,Llama 3.1 Nemotron 70B,C,0.900000,0.900000,-,131072,16384,J,api,fireworks,2026-01-04,Llama-3.1-Nemotron-70B-Instruct is a large language model customized by NVIDIA to improve the helpfu,-,-,-
fireworks/llama-v3p2-11b-vision-instruct,llama-2-11b-vision,Llama 3.2 11B Vision Instruct,C,0.200000,0.200000,-,131072,16384,VJ,api,fireworks,2026-01-04,Instruction-tuned image reasoning model from Meta with 11B parameters. Optimized for visual recognit,-,-,-
fireworks/llama-v3p2-1b,llama-2-1b,Llama 3.2 1B,C,0.200000,0.200000,-,131072,16384,J,api,fireworks,2026-01-04,The Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained ,-,-,-
fireworks/llama-v3p2-1b-instruct,llama-2-1b,Llama 3.2 1B Instruct,C,0.200000,0.200000,-,131072,16384,J,api,fireworks,2026-01-04,The Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained ,-,-,-
fireworks/llama-v3p2-3b,llama-2-3b,Llama 3.2 3B,C,0.100000,0.100000,-,131072,16384,J,api,fireworks,2026-01-04,The Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained ,-,-,-
fireworks/llama-v3p2-3b-instruct,llama-2-3b,Llama 3.2 3B Instruct,C,0.100000,0.100000,-,131072,16384,J,api,fireworks,2026-01-04,The Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained ,-,-,-
fireworks/llama-v3p2-90b-vision-instruct,llama-2-90b-vision,Llama 3.2 90B Vision Instruct,C,0.200000,0.200000,-,131072,16384,VJ,api,fireworks,2026-01-04,Instruction-tuned image reasoning model with 90B parameters from Meta. Optimized for visual recognit,-,-,-
fireworks/llama-v3p3-70b-instruct,llama-3-70b,Llama 3.3 70B Instruct,C,0.900000,0.900000,-,131072,16384,J,api,fireworks,2026-01-04,Llama 3.3 70B Instruct is the December update of Llama 3.1 70B. The model improves upon Llama 3.1 70,-,-,-
fireworks/minimax-m1-80k,minimax-m1-80k,MiniMax-M1-80k,C,0.200000,0.200000,-,4096,4096,-,api,fireworks,2026-01-04,"We introduce MiniMax-M1, the world's first open-weight, large-scale hybrid-attention reasoning model",-,-,-
fireworks/minimax-m2,minimax-m2,MiniMax-M2,C,0.200000,0.200000,-,196608,16384,TJ,api,fireworks,2026-01-04,**Preview:** This model is currently in preview. Full production support coming soon. MiniMax-M2 red,-,-,-
fireworks/minimax-m2p1,minimax-m2p1,MiniMax-M2.1,C,0.200000,0.200000,-,204800,16384,TJ,api,fireworks,2026-01-04,"MiniMax M2.1 is built for strong real-world performance across complex, multi-language, and agent-dr",-,-,-
fireworks/ministral-3-14b-instruct-2512,ministral-3-14b-2512,Ministral 3 14B Instruct 2512,C,0.100000,0.100000,-,256000,16384,VTJ,api,fireworks,2026-01-04,Mistral's Ministral 3 14B dense model with vision encoder. The largest model in the Ministral 3 fami,-,-,-
fireworks/ministral-3-3b-instruct-2512,ministral-3-3b-2512,Ministral 3 3B Instruct 2512,C,0.100000,0.100000,-,256000,16384,VTJ,api,fireworks,2026-01-04,Mistral's Ministral 3 3B dense model with vision encoder. The smallest model in the Ministral 3 fami,-,-,-
fireworks/ministral-3-8b-instruct-2512,ministral-3-8b-2512,Ministral 3 8B Instruct 2512,C,0.200000,0.200000,-,256000,16384,VTJ,api,fireworks,2026-01-04,Mistral's Ministral 3 8B dense model with vision encoder. A balanced model in the Ministral 3 family,-,-,-
fireworks/mistral-7b,mistral-7b,Mistral 7B,C,0.200000,0.200000,-,32768,16384,J,api,fireworks,2026-01-04,The Mistral-7B-v0.1 Large Language Model (LLM) is a pretrained generative text model with 7 billion ,-,-,-
fireworks/mistral-7b-instruct-4k,mistral-7b-4k,Mistal 7B Instruct V0.1,C,0.200000,0.200000,-,32768,16384,J,api,fireworks,2026-01-04,The Mistral-7B-Instruct-v0.1 Large Language Model (LLM) is a instruct fine-tuned version of the Mist,-,-,-
fireworks/mistral-7b-instruct-v0p2,mistral-7b-v0p2,Mistral 7B Instruct v0.2,C,0.200000,0.200000,-,32768,16384,J,api,fireworks,2026-01-04,Mistral 7B Instruct v0.2 is an instruction fine-tuned version of the Mistral 7B v0.2 language model.,-,-,-
fireworks/mistral-7b-instruct-v3,mistral-7b-v3,Mistral 7B Instruct v0.3,C,0.200000,0.200000,-,32768,16384,TJ,api,fireworks,2026-01-04,Mistral 7B Instruct v0.3 is an instruction fine-tuned version of the Mistral 7B v0.3 language model.,-,-,-
fireworks/mistral-7b-v0p2,mistral-7b-v0p2,Mistral 7B v0.2,C,0.200000,0.200000,-,32768,16384,J,api,fireworks,2026-01-04,The Mistral-7B-v0.2 Large Language Model (LLM) is the successor to the Mistral-7B-v0.1 LLM featuring,-,-,-
fireworks/mistral-large-3-fp8,mistral-large-3-fp8,Mistral Large 3 675B Instruct 2512,C,0.200000,0.200000,-,256000,16384,VTJ,api,fireworks,2026-01-04,Mistral Large 3 is a state-of-the-art general-purpose Multimodal granular Mixture-of-Experts model w,-,-,-
fireworks/mistral-nemo-base-2407,mistral-nemo-base-2407,Mistral Nemo Base 2407,C,0.200000,0.200000,-,128000,16384,J,api,fireworks,2026-01-04,The Mistral-Nemo-Base-2407 Large Language Model (LLM) is a pretrained generative text model of 12B p,-,-,-
fireworks/mistral-nemo-instruct-2407,mistral-nemo-2407,Mistral Nemo Instruct 2407,C,0.200000,0.200000,-,128000,16384,J,api,fireworks,2026-01-04,The Mistral-Nemo-Instruct-2407 Large Language Model (LLM) is the instruction-tuned version of Mistra,-,-,-
fireworks/mistral-small-24b-instruct-2501,mistral-small-24b-2501,Mistral Small 24B Instruct 2501,C,0.100000,0.100000,-,32768,16384,J,api,fireworks,2026-01-04,"Mistral Small 3 ( 2501 ) sets a new benchmark in the ""small"" Large Language Models category below 70",-,-,-
fireworks/mixtral-8x22b,mixtral-8x22b,Mixtral Moe 8x22B,C,1.200000,1.200000,-,65536,16384,J,api,fireworks,2026-01-04,The Mixtral MoE 8x22B v0.1 Large Language Model (LLM) is a pretrained generative sparse Mixture-of-E,-,-,-
fireworks/mixtral-8x22b-instruct,mixtral-8x22b,Mixtral MoE 8x22B Instruct,C,1.200000,1.200000,-,65536,16384,TJ,api,fireworks,2026-01-04,Mixtral MoE 8x22B Instruct v0.1 is the instruction-tuned version of Mixtral MoE 8x22B v0.1 and has t,-,-,-
fireworks/mixtral-8x7b,mixtral-8x7b,Mixtral 8x7B v0.1,C,0.200000,0.200000,-,32768,16384,J,api,fireworks,2026-01-04,Mixtral 8x7B v0.1 is a sparse mixture-of-experts (SMoE) large language model developed by Mistral AI,-,-,-
fireworks/mixtral-8x7b-instruct,mixtral-8x7b,Mixtral MoE 8x7B Instruct,C,0.200000,0.200000,-,32768,16384,J,api,fireworks,2026-01-04,Mixtral MoE 8x7B Instruct is the instruction-tuned version of Mixtral MoE 8x7B and has the chat comp,-,-,-
fireworks/mixtral-8x7b-instruct-hf,mixtral-8x7b-hf,Mixtral MoE 8x7B Instruct (HF version),C,0.200000,0.200000,-,32768,16384,J,api,fireworks,2026-01-04,"Mixtral MoE 8x7B Instruct (HF Version) is the original, FP16 version of Mixtral MoE 8x7B Instruct wh",-,-,-
fireworks/mixtral-8x7b-instruct-v0-oss,mixtral-8x7b-v0-oss,Mixtral 8x7b Instruct V0 Draft Model,C,0.200000,0.200000,-,4096,4096,-,api,fireworks,2026-01-04,Mixtral MoE 8x7B Instruct is the instruction-tuned version of Mixtral MoE 8x7B and has the chat comp,-,-,-
fireworks/mythomax-l2-13b,mythomax-l2-13b,MythoMax L2 13B,C,0.200000,0.200000,-,4096,4096,J,api,fireworks,2026-01-04,"An improved, potentially even perfected variant of MythoMix, a MythoLogic-L2 and Huginn merge using ",-,-,-
fireworks/nemotron-nano-3-30b-a3b,nemotron-nano-3-30b-a3b,NVIDIA Nemotron Nano 3 30B A3B,C,0.900000,0.900000,-,262144,16384,TJ,api,fireworks,2026-01-04,"Nemotron-Nano-3-30B-A3B is a 
large language model trained by NVIDIA, designed 
as a unified model f",-,-,-
fireworks/nemotron-nano-v2-12b-vl,nemotron-nano-v2-12b-vl,NVIDIA Nemotron Nano 2 VL,C,0.200000,0.200000,-,131072,16384,VJ,api,fireworks,2026-01-04,NVIDIA Nemotron Nano 2 VL is an open 12B multimodal reasoning model for document intelligence and vi,-,-,-
fireworks/nous-capybara-7b-v1p9,nous-capybara-7b-v1p9,Nous Capybara 7B V1.9,C,0.200000,0.200000,-,32768,16384,J,api,fireworks,2026-01-04,"Nous-Capybara 7B V1.9 is a new model trained for multiple epochs on a dataset of roughly 20,000 care",-,-,-
fireworks/nous-hermes-2-mixtral-8x7b-dpo,nous-hermes-2-mixtral-8x7b-dpo,Nouse Hermes 2 Mixtral 8x7B DPO,C,0.200000,0.200000,-,32768,16384,J,api,fireworks,2026-01-04,Nous Hermes 2 Mixtral 8x7B DPO is the new flagship Nous Research model trained over the Mixtral 8x7B,-,-,-
fireworks/nous-hermes-llama2-13b,nous-hermes-llama2-13b,Nous Hermes Llama2 13B,C,0.200000,0.200000,-,4096,4096,J,api,fireworks,2026-01-04,"Nous-Hermes-Llama2-13b is a state-of-the-art language model fine-tuned on over 300,000 instructions.",-,-,-
fireworks/nous-hermes-llama2-70b,nous-hermes-llama2-70b,Nous Hermes Llama2 70B,C,0.900000,0.900000,-,4096,4096,J,api,fireworks,2026-01-04,"Nous-Hermes-Llama2-70b is a state-of-the-art language model fine-tuned on over 300,000 instructions.",-,-,-
fireworks/nous-hermes-llama2-7b,nous-hermes-llama2-7b,Nous Hermes Llama2 7B,C,0.200000,0.200000,-,4096,4096,J,api,fireworks,2026-01-04,"Nous-Hermes-Llama2-7b is a state-of-the-art language model fine-tuned on over 300,000 instructions. ",-,-,-
fireworks/nvidia-nemotron-nano-12b-v2,nvidia-nemotron-nano-12b-v2,NVIDIA Nemotron Nano 12B v2,C,0.200000,0.200000,-,128000,16384,TJ,api,fireworks,2026-01-04,"NVIDIA-Nemotron-Nano-12B-v2 is a large language model (LLM) trained from scratch by NVIDIA, and desi",-,-,-
fireworks/nvidia-nemotron-nano-9b-v2,nvidia-nemotron-nano-9b-v2,NVIDIA Nemotron Nano 9B v2,C,0.200000,0.200000,-,128000,16384,TJ,api,fireworks,2026-01-04,"NVIDIA-Nemotron-Nano-9B-v2 is a large language model (LLM) trained from scratch by NVIDIA, and desig",-,-,-
fireworks/openchat-3p5-0106-7b,openchat-3p5-0106-7b,OpenChat 3.5 0106,C,0.200000,0.200000,-,8192,8192,J,api,fireworks,2026-01-04,"OpenChat is an innovative library of open-source language models, fine-tuned with C-RLFT - a strateg",-,-,-
fireworks/openhermes-2-mistral-7b,openhermes-2-mistral-7b,OpenHermes 2 Mistral 7B,C,0.200000,0.200000,-,32768,16384,J,api,fireworks,2026-01-04,"OpenHermes 2 Mistral 7B is a state of the art Mistral Fine-tune. OpenHermes was trained on 900,000 e",-,-,-
fireworks/openhermes-2p5-mistral-7b,openhermes-2p5-mistral-7b,OpenHermes 2.5 Mistral 7B,C,0.200000,0.200000,-,32768,16384,J,api,fireworks,2026-01-04,"OpenHermes 2.5 Mistral 7B is a state of the art Mistral Fine-tune, a continuation of OpenHermes 2 mo",-,-,-
fireworks/openorca-7b,openorca-7b,Mistral 7B OpenOrca,C,0.200000,0.200000,-,32768,16384,J,api,fireworks,2026-01-04,"A fine-tuned version of Mistral-7B trained on the OpenOrca dataset, based on the dataset generated f",-,-,-
fireworks/phi-3-mini-128k-instruct,phi-3-mini-128k,Phi-3 Mini 128k Instruct,C,0.200000,0.200000,-,131072,16384,J,api,fireworks,2026-01-04,"Phi-3-Mini-128K-Instruct is a 3.8 billion-parameter, lightweight, state-of-the-art open model traine",-,-,-
fireworks/phi-3-vision-128k-instruct,phi-3-vision-128k,Phi-3.5 Vision Instruct,C,0.200000,0.200000,-,32064,16384,VJ,api,fireworks,2026-01-04,"Phi-3-Vision-128K-Instruct is a lightweight, state-of-the-art open multimodal model built upon datas",-,-,-
fireworks/phi4-eagle,phi4-eagle,Phi-4 Eagle,C,0.200000,0.200000,-,4096,4096,-,api,fireworks,2026-01-04,EAGLE draft model for Phi4,-,-,-
fireworks/phind-code-llama-34b-python-v1,phind-code-llama-34b-python-v1,Phind CodeLlama 34B Python v1,C,0.100000,0.100000,-,16384,16384,J,api,fireworks,2026-01-04,Phind CodeLlama 34B Python V1 is a fine-tuned version of the CodeLlama 34B Python LLM using an inter,-,-,-
fireworks/phind-code-llama-34b-v1,phind-code-llama-34b-v1,Phind CodeLlama 34B v1,C,0.100000,0.100000,-,16384,16384,J,api,fireworks,2026-01-04,Phind CodeLlama 34B V1 is a fine-tuned version of the Code-Llama 34B LLM using an internal Phind dat,-,-,-
fireworks/phind-code-llama-34b-v2,phind-code-llama-34b-v2,Phind CodeLlama 34B v2,C,0.100000,0.100000,-,16384,16384,J,api,fireworks,2026-01-04,This model is fine-tuned from Phind-CodeLlama-34B-v1 and achieves 73.8% pass@1 on HumanEval. Phind-C,-,-,-
fireworks/pythia-12b,pythia-12b,Pythia 12B,C,0.200000,0.200000,-,2048,2048,J,api,fireworks,2026-01-04,The Pythia model suite was deliberately designed to promote scientific research on large language mo,-,-,-
fireworks/qwen1p5-72b-chat,qwen1p5-72b,Qwen1.5 72B Chat,C,0.900000,0.900000,-,32768,16384,J,api,fireworks,2026-01-04,"Qwen1.5 is the beta version of Qwen2, a transformer-based decoder-only language model pretrained on ",-,-,-
fireworks/qwen2-72b-instruct,qwen2-72b,Qwen2 72B Instruct,C,0.900000,0.900000,-,32768,16384,J,api,fireworks,2026-01-04,Qwen2 72B Instruct is a 72 billion parameter model developed by Alibaba for instruction-tuned tasks.,-,-,-
fireworks/qwen2-7b-instruct,qwen2-7b,Qwen2 7B Instruct,C,0.200000,0.200000,-,32768,16384,J,api,fireworks,2026-01-04,Qwen2 7B Instruct is a 7-billion-parameter instruction-tuned language model developed by the Qwen te,-,-,-
fireworks/qwen2p5-0p5b-instruct,qwen-2.5-0p5b,Qwen2.5 0.5B Instruct,C,0.200000,0.200000,-,32768,16384,J,api,fireworks,2026-01-04,"Qwen2.5 are a series of decoder-only language models developed by Qwen team, Alibaba Cloud, availabl",-,-,-
fireworks/qwen2p5-14b,qwen-2.5-14b,Qwen2.5 14B,C,0.100000,0.100000,-,131072,16384,J,api,fireworks,2026-01-04,"Qwen2.5 are a series of decoder-only language models developed by Qwen team, Alibaba Cloud, availabl",-,-,-
fireworks/qwen2p5-14b-instruct,qwen-2.5-14b,Qwen2.5 14B Instruct,C,0.100000,0.100000,-,32768,16384,J,api,fireworks,2026-01-04,"Qwen2.5 are a series of decoder-only language models developed by Qwen team, Alibaba Cloud, availabl",-,-,-
fireworks/qwen2p5-1p5b-instruct,qwen-2.5-1p5b,Qwen2.5 1.5B Instruct,C,0.200000,0.200000,-,32768,16384,J,api,fireworks,2026-01-04,"Qwen2.5 are a series of decoder-only language models developed by Qwen team, Alibaba Cloud, availabl",-,-,-
fireworks/qwen2p5-32b,qwen-2.5-32b,Qwen2.5 32B,C,0.900000,0.900000,-,131072,16384,J,api,fireworks,2026-01-04,"Qwen2.5 are a series of decoder-only language models developed by Qwen team, Alibaba Cloud, availabl",-,-,-
fireworks/qwen2p5-32b-instruct,qwen-2.5-32b,Qwen2.5 32B Instruct,C,0.900000,0.900000,-,32768,16384,J,api,fireworks,2026-01-04,"Qwen2.5 are a series of decoder-only language models developed by Qwen team, Alibaba Cloud, availabl",-,-,-
fireworks/qwen2p5-72b,qwen-2.5-72b,Qwen2.5 72B,C,0.900000,0.900000,-,131072,16384,J,api,fireworks,2026-01-04,"Qwen2.5 are a series of decoder-only language models developed by Qwen team, Alibaba Cloud, availabl",-,-,-
fireworks/qwen2p5-72b-instruct,qwen-2.5-72b,Qwen2.5 72B Instruct,C,0.900000,0.900000,-,32768,16384,TJ,api,fireworks,2026-01-04,"Qwen2.5 are a series of decoder-only language models developed by Qwen team, Alibaba Cloud, availabl",-,-,-
fireworks/qwen2p5-7b,qwen-2.5-7b,Qwen2.5 7B,C,0.200000,0.200000,-,131072,16384,J,api,fireworks,2026-01-04,"Qwen2.5 are a series of decoder-only language models developed by Qwen team, Alibaba Cloud, availabl",-,-,-
fireworks/qwen2p5-7b-instruct,qwen-2.5-7b,Qwen2.5 7B Instruct,C,0.200000,0.200000,-,32768,16384,J,api,fireworks,2026-01-04,"Qwen2.5 are a series of decoder-only language models developed by Qwen team, Alibaba Cloud, availabl",-,-,-
fireworks/qwen2p5-coder-0p5b,qwen-2.5-coder-0p5b,Qwen2.5-Coder 0.5B,C,0.200000,0.200000,-,32768,16384,J,api,fireworks,2026-01-04,Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as Co,-,-,-
fireworks/qwen2p5-coder-0p5b-instruct,qwen-2.5-coder-0p5b,Qwen2.5-Coder 0.5B Instruct,C,0.200000,0.200000,-,32768,16384,J,api,fireworks,2026-01-04,Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as Co,-,-,-
fireworks/qwen2p5-coder-14b,qwen-2.5-coder-14b,Qwen2.5-Coder 14B,C,0.100000,0.100000,-,32768,16384,J,api,fireworks,2026-01-04,Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as Co,-,-,-
fireworks/qwen2p5-coder-14b-instruct,qwen-2.5-coder-14b,Qwen2.5-Coder 14B Instruct,C,0.100000,0.100000,-,32768,16384,J,api,fireworks,2026-01-04,Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as Co,-,-,-
fireworks/qwen2p5-coder-1p5b,qwen-2.5-coder-1p5b,Qwen2.5-Coder 1.5B,C,0.200000,0.200000,-,32768,16384,J,api,fireworks,2026-01-04,Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as Co,-,-,-
fireworks/qwen2p5-coder-1p5b-instruct,qwen-2.5-coder-1p5b,Qwen2.5-Coder 1.5B Instruct,C,0.200000,0.200000,-,32768,16384,J,api,fireworks,2026-01-04,Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as Co,-,-,-
fireworks/qwen2p5-coder-32b,qwen-2.5-coder-32b,Qwen2.5-Coder 32B,C,0.900000,0.900000,-,32768,16384,J,api,fireworks,2026-01-04,Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as Co,-,-,-
fireworks/qwen2p5-coder-32b-instruct,qwen-2.5-coder-32b,Qwen2.5-Coder 32B Instruct,C,0.900000,0.900000,-,32768,16384,J,api,fireworks,2026-01-04,Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as Co,-,-,-
fireworks/qwen2p5-coder-32b-instruct-128k,qwen-2.5-coder-32b-128k,Qwen2.5-Coder 32B Instruct 128K,C,0.900000,0.900000,-,131072,16384,J,api,fireworks,2026-01-04,Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as Co,-,-,-
fireworks/qwen2p5-coder-32b-instruct-32k-rope,qwen-2.5-coder-32b-32k-rope,Qwen2.5-Coder 32B Instruct 32K RoPE,C,0.900000,0.900000,-,32768,16384,J,api,fireworks,2026-01-04,Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as Co,-,-,-
fireworks/qwen2p5-coder-32b-instruct-64k,qwen-2.5-coder-32b-64k,Qwen2.5-Coder 32B Instruct 64k,C,0.900000,0.900000,-,65536,16384,J,api,fireworks,2026-01-04,Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as Co,-,-,-
fireworks/qwen2p5-coder-3b,qwen-2.5-coder-3b,Qwen2.5-Coder 3B,C,0.100000,0.100000,-,32768,16384,J,api,fireworks,2026-01-04,Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as Co,-,-,-
fireworks/qwen2p5-coder-3b-instruct,qwen-2.5-coder-3b,Qwen2.5-Coder 3B Instruct,C,0.100000,0.100000,-,32768,16384,J,api,fireworks,2026-01-04,Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as Co,-,-,-
fireworks/qwen2p5-coder-7b,qwen-2.5-coder-7b,Qwen2.5-Coder 7B,C,0.200000,0.200000,-,32768,16384,J,api,fireworks,2026-01-04,Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as Co,-,-,-
fireworks/qwen2p5-coder-7b-instruct,qwen-2.5-coder-7b,Qwen2.5-Coder 7B Instruct,C,0.200000,0.200000,-,32768,16384,J,api,fireworks,2026-01-04,Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as Co,-,-,-
fireworks/qwen2p5-math-72b-instruct,qwen-2.5-math-72b,Qwen2.5-Math 72B Instruct,C,0.900000,0.900000,-,4096,4096,J,api,fireworks,2026-01-04,Qwen2.5-Math series is expanded to support using both CoT and Tool-integrated Reasoning (TIR) to sol,-,-,-
fireworks/qwen2p5-vl-32b-instruct,qwen-2.5-vl-32b,Qwen2.5-VL 32B Instruct,C,0.900000,0.900000,-,128000,16384,VJ,api,fireworks,2026-01-04,"Qwen2.5-VL is a multimodal large language model series developed by Qwen team, Alibaba Cloud, availa",-,-,-
fireworks/qwen2p5-vl-3b-instruct,qwen-2.5-vl-3b,Qwen2.5-VL 3B Instruct,C,0.100000,0.100000,-,128000,16384,VJ,api,fireworks,2026-01-04,"Qwen2.5-VL is a multimodal large language model series developed by Qwen team, Alibaba Cloud, availa",-,-,-
fireworks/qwen2p5-vl-72b-instruct,qwen-2.5-vl-72b,Qwen2.5-VL 72B Instruct,C,0.900000,0.900000,-,128000,16384,VJ,api,fireworks,2026-01-04,"Qwen2.5-VL is a multimodal large language model series developed by Qwen team, Alibaba Cloud, availa",-,-,-
fireworks/qwen2p5-vl-7b-instruct,qwen-2.5-vl-7b,Qwen2.5-VL 7B Instruct,C,0.200000,0.200000,-,128000,16384,VJ,api,fireworks,2026-01-04,"Qwen2.5-VL is a multimodal large language model series developed by Qwen team, Alibaba Cloud, availa",-,-,-
fireworks/qwen2-vl-2b-instruct,qwen2-vl-2b,Qwen2-VL 2B Instruct,C,0.200000,0.200000,-,32768,16384,VJ,api,fireworks,2026-01-04,"Qwen2-VL is a multimodal large language model series developed by Qwen team, Alibaba Cloud, availabl",-,-,-
fireworks/qwen2-vl-72b-instruct,qwen2-vl-72b,Qwen2-VL 72B Instruct,C,0.900000,0.900000,-,32768,16384,VJ,api,fireworks,2026-01-04,"Qwen2-VL is a multimodal large language model series developed by Qwen team, Alibaba Cloud, availabl",-,-,-
fireworks/qwen2-vl-7b-instruct,qwen2-vl-7b,Qwen2-VL 7B Instruct,C,0.200000,0.200000,-,32768,16384,VJ,api,fireworks,2026-01-04,"Qwen2-VL is a multimodal large language model series developed by Qwen team, Alibaba Cloud, availabl",-,-,-
fireworks/qwen3-0p6b,qwen-3-0p6b,Qwen3 0.6B,C,0.200000,0.200000,-,40960,16384,TJ,api,fireworks,2026-01-04,"Qwen3 0.6B model developed by Qwen team, Alibaba Cloud,",-,-,-
fireworks/qwen3-14b,qwen-3-14b,Qwen3 14B,C,0.100000,0.100000,-,40960,16384,TJ,api,fireworks,2026-01-04,"Qwen3 14B model developed by Qwen team, Alibaba Cloud,",-,-,-
fireworks/qwen3-1p7b,qwen-3-1p7b,Qwen3 1.7B,C,0.200000,0.200000,-,131072,16384,TJ,api,fireworks,2026-01-04,"Qwen 1.7B Model developed by Qwen team, Alibaba Cloud,",-,-,-
fireworks/qwen3-1p7b-fp8-draft,qwen-3-1p7b-fp8-draft,Qwen3 1.7B fp8 model used for drafting,C,0.200000,0.200000,-,262144,16384,J,api,fireworks,2026-01-04,qwen 1.7b fp8 used as draft model,-,-,-
fireworks/qwen3-1p7b-fp8-draft-131072,qwen-3-1p7b-fp8-draft-131072,Qwen3 1.7B fp8 model used for drafting for 131072 context,C,0.200000,0.200000,-,131072,16384,J,api,fireworks,2026-01-04,qwen 1.7b fp8 used as draft model for 131072 context length,-,-,-
fireworks/qwen3-1p7b-fp8-draft-40960,qwen-3-1p7b-fp8-draft-40960,Qwen3 1.7B fp8 model used for drafting for 40960 context len,C,0.200000,0.200000,-,40960,16384,J,api,fireworks,2026-01-04,qwen 1.7b fp8 used as draft model for 40960 context length,-,-,-
fireworks/qwen3-235b-a22b,qwen-3-235b-a22b,Qwen3 235B A22B,C,0.900000,0.900000,-,131072,16384,TJ,api,fireworks,2026-01-04,"Latest Qwen3 state of the art model, 235B with 22B active parameter model",-,-,-
fireworks/qwen3-235b-a22b-instruct-2507,qwen-3-235b-a22b-2507,Qwen3 235B A22B Instruct 2507,C,0.900000,0.900000,-,262144,16384,TJ,api,fireworks,2026-01-04,"Updated FP8 version of Qwen3-235B-A22B non-thinking mode, with better tool use, coding, instruction ",-,-,-
fireworks/qwen3-235b-a22b-thinking-2507,qwen-3-235b-a22b-thinking-2507,Qwen3 235B A22B Thinking 2507,C,0.900000,0.900000,-,262144,16384,JK,api,fireworks,2026-01-04,"Latest Qwen3 thinking model, competitive against the best close source models in Jul 2025.",-,-,-
fireworks/qwen3-30b-a3b,qwen-3-30b-a3b,Qwen3 30B-A3B,C,0.900000,0.900000,-,131072,16384,TJ,api,fireworks,2026-01-04,"Latest Qwen3 state of the art model, 30B with 3B active parameter model",-,-,-
fireworks/qwen3-30b-a3b-instruct-2507,qwen-3-30b-a3b-2507,Qwen3 30B A3B Instruct 2507,C,0.900000,0.900000,-,262144,16384,J,api,fireworks,2026-01-04,"Updated FP8 version of Qwen3-30B-A3B non-thinking mode, with better tool use, coding, instruction fo",-,-,-
fireworks/qwen3-30b-a3b-thinking-2507,qwen-3-30b-a3b-thinking-2507,Qwen3 30B A3B Thinking 2507,C,0.900000,0.900000,-,262144,16384,TJK,api,fireworks,2026-01-04,"Updated FP8 version of Qwen3-30B-A3B thinking mode, with better tool use, coding, instruction follow",-,-,-
fireworks/qwen3-32b,qwen-3-32b,Qwen3 32B,C,0.900000,0.900000,-,131072,16384,TJ,api,fireworks,2026-01-04,"Latest Qwen3 state of the art model, 32B model",-,-,-
fireworks/qwen3-32b-eagle3-v2,qwen-3-32b-eagle3-v2,qwen3-32b-eagle3-drafter,C,0.900000,0.900000,-,4096,4096,-,api,fireworks,2026-01-04,qwen3 32b eagle3,-,-,-
fireworks/qwen3-4b,qwen-3-4b,Qwen3 4B,C,0.100000,0.100000,-,40960,16384,TJ,api,fireworks,2026-01-04,"Latest Qwen3 state of the art model, 4B model",-,-,-
fireworks/qwen3-4b-instruct-2507,qwen-3-4b-2507,Qwen 3 4B Instruct 2507,C,0.100000,0.100000,-,262144,16384,J,api,fireworks,2026-01-04,"Introducing Qwen3-4B-Instruct-2507, with improved instruction following, reasoning, coding, multilin",-,-,-
fireworks/qwen3-8b,qwen-3-8b,Qwen3 8B,C,0.200000,0.200000,-,40960,16384,TJ,api,fireworks,2026-01-04,"Latest Qwen3 state of the art model, FP8 version 8B Model",-,-,-
fireworks/qwen3-coder-30b-a3b-instruct,qwen-3-coder-30b-a3b,Qwen3 Coder 30B A3B Instruct,C,0.900000,0.900000,-,262144,16384,J,api,fireworks,2026-01-04,"Latest Qwen3 coder model, 30B with 3B active parameter model",-,-,-
fireworks/qwen3-coder-480b-a35b-instruct,qwen-3-coder-480b-a35b,Qwen3 Coder 480B A35B Instruct,C,0.900000,0.900000,-,262144,16384,TJ,api,fireworks,2026-01-04,Qwen3's most agentic code model to date,-,-,-
fireworks/qwen3-coder-480b-instruct-bf16,qwen-3-coder-480b-bf16,Qwen3 Coder 480B Instruct BF16,C,0.900000,0.900000,-,262144,16384,J,api,fireworks,2026-01-04,The BF16 version of the 480B coder model,-,-,-
fireworks/qwen3-embedding-0p6b,qwen-3-embedding-0p6b,Qwen3 Embedding 0.6B,C,0.008000,-,-,32768,32768,E,api,fireworks,2026-01-04,"significant advancements in multiple text embedding and ranking tasks, including text retrieval, cod",-,-,-
fireworks/qwen3-embedding-4b,qwen-3-embedding-4b,Qwen3 Embedding 4B,C,0.008000,-,-,40960,40960,E,api,fireworks,2026-01-04,"significant advancements in multiple text embedding and ranking tasks, including text retrieval, cod",-,-,-
fireworks/qwen3-embedding-8b,qwen-3-embedding-8b,Qwen3 Embedding 8B,C,0.008000,-,-,40960,40960,E,api,fireworks,2026-01-04,"The Qwen3 Embedding 8B model is the latest proprietary model of the Qwen family, specifically design",-,-,-
fireworks/qwen3-next-80b-a3b-instruct,qwen-3-next-80b-a3b,Qwen3 Next 80B A3B Instruct,C,0.100000,0.100000,-,4096,4096,-,api,fireworks,2026-01-04,Qwen3 Next 80B A3B Instruct is a state-of-the-art mixture-of-experts (MoE) language model with 3 bil,-,-,-
fireworks/qwen3-next-80b-a3b-thinking,qwen-3-next-80b-a3b-thinking,Qwen3 Next 80B A3B Thinking,C,0.100000,0.100000,-,4096,4096,K,api,fireworks,2026-01-04,Qwen3 Next 80B A3B Thinking is a state-of-the-art mixture-of-experts (MoE) language model with 3 bil,-,-,-
fireworks/qwen3-omni-30b-a3b-instruct,qwen-3-omni-30b-a3b,Qwen3 Omni 30B A3B Instruct,C,0.900000,0.900000,-,65536,16384,VTJ,api,fireworks,2026-01-04,"Qwen3-Omni is a natively end-to-end multilingual omni-modal foundation model. It processes text, ima",-,-,-
fireworks/qwen3-reranker-0p6b,qwen-3-reranker-0p6b,Qwen3 Reranker 0.6B,C,0.008000,-,-,40960,40960,E,api,fireworks,2026-01-04,"significant advancements in multiple text embedding and ranking tasks, including text retrieval, cod",-,-,-
fireworks/qwen3-reranker-4b,qwen-3-reranker-4b,Qwen3 Reranker 4B,C,0.008000,-,-,40960,40960,E,api,fireworks,2026-01-04,"significant advancements in multiple text embedding and ranking tasks, including text retrieval, cod",-,-,-
fireworks/qwen3-reranker-8b,qwen-3-reranker-8b,Qwen3 Reranker 8B,C,0.008000,-,-,40960,40960,E,api,fireworks,2026-01-04,"significant advancements in multiple text embedding and ranking tasks, including text retrieval, cod",-,-,-
fireworks/qwen3-vl-235b-a22b-instruct,qwen-3-vl-235b-a22b,Qwen3 VL 235B A22B Instruct,C,0.900000,0.900000,-,262144,16384,VTJ,api,fireworks,2026-01-04,Qwen3 VL 235B A22B Instruct is a state-of-the-art vision-language model with 22 billion activated pa,-,-,-
fireworks/qwen3-vl-235b-a22b-thinking,qwen-3-vl-235b-a22b-thinking,Qwen3 VL 235B A22B Thinking,C,0.900000,0.900000,-,262144,16384,VTJK,api,fireworks,2026-01-04,Qwen3 VL 235B A22B Thinking is a state-of-the-art vision-language model with 22 billion activated pa,-,-,-
fireworks/qwen3-vl-30b-a3b-instruct,qwen-3-vl-30b-a3b,Qwen3 VL 30B A3B Instruct,C,0.900000,0.900000,-,262144,16384,VTJ,api,fireworks,2026-01-04,"Qwen3-VL series delivers superior text understanding & generation, deeper visual perception & reason",-,-,-
fireworks/qwen3-vl-30b-a3b-thinking,qwen-3-vl-30b-a3b-thinking,Qwen3 VL 30B A3B Thinking,C,0.900000,0.900000,-,262144,16384,VTJK,api,fireworks,2026-01-04,"Qwen3-VL series delivers superior text understanding & generation, deeper visual perception & reason",-,-,-
fireworks/qwen3-vl-32b-instruct,qwen-3-vl-32b,Qwen3 VL 32B Instruct,C,0.900000,0.900000,-,4096,4096,V,api,fireworks,2026-01-04,The Qwen3-VL-32B-Instruct model is an advanced vision-language model that significantly enhances tex,-,-,-
fireworks/qwen3-vl-8b-instruct,qwen-3-vl-8b,Qwen3-VL-8B-Instruct,C,0.200000,0.200000,-,4096,4096,V,api,fireworks,2026-01-04,The Qwen3-VL-8B-Instruct model is an advanced vision-language model that significantly enhances text,-,-,-
fireworks/qwen-qwq-32b-preview,qwen-qwq-32b-preview,Qwen QWQ 32B Preview,C,0.900000,0.900000,-,32768,16384,JK,api,fireworks,2026-01-04,"Qwen QwQ model focuses on advancing AI reasoning, and showcases the power of open models to match cl",-,-,-
fireworks/qwen-v2p5-14b-instruct,qwen-v2p5-14b,Qwen2.5 14B Instruct,C,0.100000,0.100000,-,32768,16384,J,api,fireworks,2026-01-04,"Qwen2.5 are a series of decoder-only language models developed by Qwen team, Alibaba Cloud, availabl",-,-,-
fireworks/qwen-v2p5-7b,qwen-v2p5-7b,Qwen2.5 7B,C,0.200000,0.200000,-,131072,16384,J,api,fireworks,2026-01-04,"Qwen2.5 are a series of decoder-only language models developed by Qwen team, Alibaba Cloud, availabl",-,-,-
fireworks/qwq-32b,qwq-32b,QWQ 32B,C,0.900000,0.900000,-,131072,16384,JK,api,fireworks,2026-01-04,Medium-sized reasoning model from Qwen.,-,-,-
fireworks/rolm-ocr,rolm-ocr,Rolm OCR,C,0.200000,0.200000,-,128000,16384,VJ,api,fireworks,2026-01-04,RolmOCR is an open-source document OCR model developed by Reducto AI as a drop-in alternative to olm,-,-,-
fireworks/seed-oss-36b-instruct,seed-oss-36b,Seed OSS 36B Instruct,C,0.200000,0.200000,-,524288,16384,TJ,api,fireworks,2026-01-04,"Seed-OSS is a series of open-source large language models developed by ByteDance's Seed Team, design",-,-,-
fireworks/snorkel-mistral-7b-pairrm-dpo,snorkel-mistral-7b-pairrm-dpo,Snorkel Mistral PairRM DPO,C,0.200000,0.200000,-,32768,16384,J,api,fireworks,2026-01-04,A fine-tuned version of the Mistral-7B model developed by Snorkel using PairRM for response ranking ,-,-,-
fireworks/toppy-m-7b,toppy-m-7b,Toppy M 7B,C,0.200000,0.200000,-,32768,16384,J,api,fireworks,2026-01-04,A wild 7B parameter model that merges several models using the new task_arithmetic merge method from,-,-,-
fireworks/zephyr-7b-beta,zephyr-7b-beta,Zephyr 7B Beta,C,0.200000,0.200000,-,32768,16384,J,api,fireworks,2026-01-04,Zephyr is a series of language models that are trained to act as helpful assistants. Zephyr-7B-β is ,-,-,-
