id,alias,name,status,input_price,output_price,cache_input_price,context_window,max_output,capabilities,quality,source,updated,description,mmlu_score,humaneval_score,math_score
bytedance-seed/seed-1.6-flash,seed-1.6-flash,ByteDance Seed: Seed 1.6 Flash,C,0.00000007,0.00000030,-,262144,16384,JKSTV,verified,openrouter,2025-12-23,"Seed 1.6 Flash is an ultra-fast multimodal deep thinking model by ByteDance Seed, supporting both te",-,-,-
bytedance-seed/seed-1.6,seed-1.6,ByteDance Seed: Seed 1.6,C,0.00000025,0.00000200,-,262144,32768,JKSTV,verified,openrouter,2025-12-23,Seed 1.6 is a general-purpose model released by the ByteDance Seed team. It incorporates multimodal ,-,-,-
minimax/minimax-m2.1,minimax-m2.1,MiniMax: MiniMax M2.1,C,0.00000012,0.00000048,-,196608,49152,JKST,verified,openrouter,2025-12-22,"MiniMax-M2.1 is a lightweight, state-of-the-art large language model optimized for coding, agentic w",-,-,-
z-ai/glm-4.7,glm-4.7,Z.AI: GLM 4.7,C,0.00000040,0.00000150,-,202752,65535,JKST,verified,openrouter,2025-12-21,"GLM-4.7 is Z.AI’s latest flagship model, featuring upgrades in two key areas: enhanced programming c",-,-,-
google/gemini-3-flash-preview,gemini-3-flash-previ,Google: Gemini 3 Flash Preview,C,0.00000050,0.00000300,-,1048576,65535,JKSTV,verified,openrouter,2025-12-17,"Gemini 3 Flash Preview is a high speed, high value thinking model designed for agentic workflows, mu",-,-,-
mistralai/mistral-small-creative,mistral-small-creati,Mistral: Mistral Small Creative,C,0.00000010,0.00000030,-,32768,8192,T,verified,openrouter,2025-12-16,"Mistral Small Creative is an experimental small model designed for creative writing, narrative gener",-,-,-
allenai/olmo-3.1-32b-think:free,olmo-3.1-32b-think:f,AllenAI: Olmo 3.1 32B Think (free),C,-,-,-,65536,65536,JKS,verified,openrouter,2025-12-16,"Olmo 3.1 32B Think is a large-scale, 32-billion-parameter model designed for deep reasoning, complex",-,-,-
xiaomi/mimo-v2-flash:free,mimo-v2-flash:free,Xiaomi: MiMo-V2-Flash (free),C,-,-,-,262144,65536,JKT,verified,openrouter,2025-12-14,MiMo-V2-Flash is an open-source foundation language model developed by Xiaomi. It is a Mixture-of-Ex,-,-,-
nvidia/nemotron-3-nano-30b-a3b:free,nemotron-3-nano-30b-,NVIDIA: Nemotron 3 Nano 30B A3B (free),C,-,-,-,256000,64000,KT,verified,openrouter,2025-12-14,NVIDIA Nemotron 3 Nano 30B A3B is a small language MoE model with highest compute efficiency and acc,-,-,-
nvidia/nemotron-3-nano-30b-a3b,nemotron-3-nano-30b-,NVIDIA: Nemotron 3 Nano 30B A3B,C,0.00000006,0.00000024,-,262144,262144,JKST,verified,openrouter,2025-12-14,NVIDIA Nemotron 3 Nano 30B A3B is a small language MoE model with highest compute efficiency and acc,-,-,-
openai/gpt-5.2-chat,gpt-5.2-chat,OpenAI: GPT-5.2 Chat,C,0.00000175,0.00001400,-,128000,16384,JSTV,verified,openrouter,2025-12-10,"GPT-5.2 Chat (AKA Instant) is the fast, lightweight member of the 5.2 family, optimized for low-late",-,-,-
openai/gpt-5.2-pro,gpt-5.2-pro,OpenAI: GPT-5.2 Pro,C,0.00002100,0.00016800,-,400000,128000,JKSTV,verified,openrouter,2025-12-10,"GPT-5.2 Pro is OpenAI’s most advanced model, offering major improvements in agentic coding and long ",-,-,-
openai/gpt-5.2,gpt-5.2,OpenAI: GPT-5.2,C,0.00000175,0.00001400,-,400000,128000,JKSTV,verified,openrouter,2025-12-10,"GPT-5.2 is the latest frontier-grade model in the GPT-5 series, offering stronger agentic and long c",-,-,-
mistralai/devstral-2512:free,devstral-2512:free,Mistral: Devstral 2 2512 (free),C,-,-,-,262144,65536,JST,verified,openrouter,2025-12-09,Devstral 2 is a state-of-the-art open-source model by Mistral AI specializing in agentic coding. It ,-,-,-
mistralai/devstral-2512,devstral-2512,Mistral: Devstral 2 2512,C,0.00000005,0.00000022,-,262144,65536,JST,verified,openrouter,2025-12-09,Devstral 2 is a state-of-the-art open-source model by Mistral AI specializing in agentic coding. It ,-,-,-
relace/relace-search,relace-search,Relace: Relace Search,C,0.00000100,0.00000300,-,256000,128000,T,verified,openrouter,2025-12-08,The relace-search model uses 4-12 `view_file` and `grep` tools in parallel to explore a codebase and,-,-,-
z-ai/glm-4.6v,glm-4.6v,Z.AI: GLM 4.6V,C,0.00000030,0.00000090,-,131072,24000,JKSTV,verified,openrouter,2025-12-08,GLM-4.6V is a large multimodal model designed for high-fidelity visual understanding and long-contex,-,-,-
nex-agi/deepseek-v3.1-nex-n1:free,deepseek-v3.1-nex-n1,Nex AGI: DeepSeek V3.1 Nex N1 (free),C,-,-,-,131072,163840,JST,verified,openrouter,2025-12-08,DeepSeek V3.1 Nex-N1 is the flagship release of the Nex-N1 series — a post-trained model designed to,-,-,-
essentialai/rnj-1-instruct,rnj-1-instruct,EssentialAI: Rnj 1 Instruct,C,0.00000015,0.00000015,-,32768,8192,JS,verified,openrouter,2025-12-07,"Rnj-1 is an 8B-parameter, dense, open-weight model family developed by Essential AI and trained from",-,-,-
openrouter/bodybuilder,bodybuilder,Body Builder (beta),C,-,-,-,128000,32000,-,verified,openrouter,2025-12-04,Transform your natural language requests into structured OpenRouter API request objects. Describe wh,-,-,-
openai/gpt-5.1-codex-max,gpt-5.1-codex-max,OpenAI: GPT-5.1-Codex-Max,C,0.00000125,0.00001000,-,400000,128000,JKSTV,verified,openrouter,2025-12-04,"GPT-5.1-Codex-Max is OpenAI’s latest agentic coding model, designed for long-running, high-context s",-,-,-
amazon/nova-2-lite-v1,nova-2-lite-v1,Amazon: Nova 2 Lite,C,0.00000030,0.00000250,-,1000000,65535,KTV,verified,openrouter,2025-12-02,"Nova 2 Lite is a fast, cost-effective reasoning model for everyday workloads that can process text, ",-,-,-
mistralai/ministral-14b-2512,ministral-14b-2512,Mistral: Ministral 3 14B 2512,C,0.00000020,0.00000020,-,262144,65536,JSTV,verified,openrouter,2025-12-02,"The largest model in the Ministral 3 family, Ministral 3 14B offers frontier capabilities and perfor",-,-,-
mistralai/ministral-8b-2512,ministral-8b-2512,Mistral: Ministral 3 8B 2512,C,0.00000015,0.00000015,-,262144,65536,JSTV,verified,openrouter,2025-12-02,"A balanced model in the Ministral 3 family, Ministral 3 8B is a powerful, efficient tiny language mo",-,-,-
mistralai/ministral-3b-2512,ministral-3b-2512,Mistral: Ministral 3 3B 2512,C,0.00000010,0.00000010,-,131072,32768,JSTV,verified,openrouter,2025-12-02,"The smallest model in the Ministral 3 family, Ministral 3 3B is a powerful, efficient tiny language ",-,-,-
mistralai/mistral-large-2512,mistral-large-2512,Mistral: Mistral Large 3 2512,C,0.00000050,0.00000150,-,262144,65536,JSTV,verified,openrouter,2025-12-01,"Mistral Large 3 2512 is Mistral’s most capable model to date, featuring a sparse mixture-of-experts ",-,-,-
arcee-ai/trinity-mini:free,trinity-mini:free,Arcee AI: Trinity Mini (free),C,-,-,-,131072,32768,JKST,verified,openrouter,2025-12-01,Trinity Mini is a 26B-parameter (3B active) sparse mixture-of-experts language model featuring 128 e,-,-,-
arcee-ai/trinity-mini,trinity-mini,Arcee AI: Trinity Mini,C,0.00000004,0.00000015,-,131072,131072,JKST,verified,openrouter,2025-12-01,Trinity Mini is a 26B-parameter (3B active) sparse mixture-of-experts language model featuring 128 e,-,-,-
deepseek/deepseek-v3.2-speciale,deepseek-v3.2-specia,DeepSeek: DeepSeek V3.2 Speciale,C,0.00000027,0.00000041,-,163840,65536,JKS,verified,openrouter,2025-12-01,DeepSeek-V3.2-Speciale is a high-compute variant of DeepSeek-V3.2 optimized for maximum reasoning an,-,-,-
deepseek/deepseek-v3.2,deepseek-v3.2,DeepSeek: DeepSeek V3.2,C,0.00000025,0.00000038,-,163840,65536,JKST,verified,openrouter,2025-12-01,DeepSeek-V3.2 is a large language model designed to harmonize high computational efficiency with str,-,-,-
prime-intellect/intellect-3,intellect-3,Prime Intellect: INTELLECT-3,C,0.00000020,0.00000110,-,131072,131072,JKST,verified,openrouter,2025-11-26,INTELLECT-3 is a 106B-parameter Mixture-of-Experts model (12B active) post-trained from GLM-4.5-Air-,-,-,-
tngtech/tng-r1t-chimera:free,tng-r1t-chimera:free,TNG: R1T Chimera (free),C,-,-,-,163840,163840,JKST,verified,openrouter,2025-11-26,TNG-R1T-Chimera is an experimental LLM with a faible for creative storytelling and character interac,-,-,-
tngtech/tng-r1t-chimera,tng-r1t-chimera,TNG: R1T Chimera,C,0.00000025,0.00000085,-,163840,65536,JKST,verified,openrouter,2025-11-26,TNG-R1T-Chimera is an experimental LLM with a faible for creative storytelling and character interac,-,-,-
anthropic/claude-opus-4.5,claude-opus-4.5,Anthropic: Claude Opus 4.5,C,0.00000500,0.00002500,-,200000,32000,JKSTV,verified,openrouter,2025-11-24,"Claude Opus 4.5 is Anthropic’s frontier reasoning model optimized for complex software engineering, ",-,-,-
allenai/olmo-3-32b-think:free,olmo-3-32b-think:fre,AllenAI: Olmo 3 32B Think (free),C,-,-,-,65536,65536,JKS,verified,openrouter,2025-11-21,"Olmo 3 32B Think is a large-scale, 32-billion-parameter model purpose-built for deep reasoning, comp",-,-,-
allenai/olmo-3-7b-instruct,olmo-3-7b-instruct,AllenAI: Olmo 3 7B Instruct,C,0.00000010,0.00000020,-,65536,65536,JST,verified,openrouter,2025-11-21,"Olmo 3 7B Instruct is a supervised instruction-fine-tuned variant of the Olmo 3 7B base model, optim",-,-,-
allenai/olmo-3-7b-think,olmo-3-7b-think,AllenAI: Olmo 3 7B Think,C,0.00000012,0.00000020,-,65536,65536,JKS,verified,openrouter,2025-11-21,Olmo 3 7B Think is a research-oriented language model in the Olmo family designed for advanced reaso,-,-,-
google/gemini-3-pro-image-preview,gemini-3-pro-image-p,Google: Nano Banana Pro (Gemini 3 Pro Image Preview),C,0.00000200,0.00001200,-,65536,32768,JKSV,verified,openrouter,2025-11-20,"Nano Banana Pro is Google’s most advanced image-generation and editing model, built on Gemini 3 Pro.",-,-,-
x-ai/grok-4.1-fast,grok-4.1-fast,xAI: Grok 4.1 Fast,C,0.00000020,0.00000050,-,2000000,30000,JKSTV,verified,openrouter,2025-11-19,Grok 4.1 Fast is xAI's best agentic tool calling model that shines in real-world use cases like cust,-,-,-
google/gemini-3-pro-preview,gemini-3-pro-preview,Google: Gemini 3 Pro Preview,C,0.00000200,0.00001200,-,1048576,65536,JKSTV,verified,openrouter,2025-11-18,"Gemini 3 Pro is Google’s flagship frontier model for high-precision multimodal reasoning, combining ",-,-,-
deepcogito/cogito-v2.1-671b,cogito-v2.1-671b,Deep Cogito: Cogito v2.1 671B,C,0.00000125,0.00000125,-,128000,32000,JKS,verified,openrouter,2025-11-13,"Cogito v2.1 671B MoE represents one of the strongest open models globally, matching performance of f",-,-,-
openai/gpt-5.1,gpt-5.1,OpenAI: GPT-5.1,C,0.00000125,0.00001000,-,400000,128000,JKSTV,verified,openrouter,2025-11-13,"GPT-5.1 is the latest frontier-grade model in the GPT-5 series, offering stronger general-purpose re",-,-,-
openai/gpt-5.1-chat,gpt-5.1-chat,OpenAI: GPT-5.1 Chat,C,0.00000125,0.00001000,-,128000,16384,JSTV,verified,openrouter,2025-11-13,"GPT-5.1 Chat (AKA Instant is the fast, lightweight member of the 5.1 family, optimized for low-laten",-,-,-
openai/gpt-5.1-codex,gpt-5.1-codex,OpenAI: GPT-5.1-Codex,C,0.00000125,0.00001000,-,400000,128000,JKSTV,verified,openrouter,2025-11-13,GPT-5.1-Codex is a specialized version of GPT-5.1 optimized for software engineering and coding work,-,-,-
openai/gpt-5.1-codex-mini,gpt-5.1-codex-mini,OpenAI: GPT-5.1-Codex-Mini,C,0.00000025,0.00000200,-,400000,100000,JKSTV,verified,openrouter,2025-11-13,GPT-5.1-Codex-Mini is a smaller and faster version of GPT-5.1-Codex,-,-,-
kwaipilot/kat-coder-pro:free,kat-coder-pro:free,Kwaipilot: KAT-Coder-Pro V1 (free),C,-,-,-,256000,32768,JST,verified,openrouter,2025-11-09,KAT-Coder-Pro V1 is KwaiKAT's most advanced agentic coding model in the KAT-Coder series. Designed s,-,-,-
moonshotai/kimi-k2-thinking,kimi-k2-thinking,MoonshotAI: Kimi K2 Thinking,C,0.00000040,0.00000175,-,262144,65535,JKST,verified,openrouter,2025-11-06,"Kimi K2 Thinking is Moonshot AI’s most advanced open reasoning model to date, extending the K2 serie",-,-,-
amazon/nova-premier-v1,nova-premier-v1,Amazon: Nova Premier 1.0,C,0.00000250,0.00001250,-,1000000,32000,TV,verified,openrouter,2025-10-31,Amazon Nova Premier is the most capable of Amazon’s multimodal models for complex reasoning tasks an,-,-,-
perplexity/sonar-pro-search,sonar-pro-search,Perplexity: Sonar Pro Search,C,0.00000300,0.00001500,-,200000,8000,JKSV,verified,openrouter,2025-10-30,"Exclusively available on the OpenRouter API, Sonar Pro's new Pro Search mode is Perplexity's most ad",-,-,-
mistralai/voxtral-small-24b-2507,voxtral-small-24b-25,Mistral: Voxtral Small 24B 2507,C,0.00000010,0.00000030,-,32000,8000,JST,verified,openrouter,2025-10-30,"Voxtral Small is an enhancement of Mistral Small 3, incorporating state-of-the-art audio input capab",-,-,-
openai/gpt-oss-safeguard-20b,gpt-oss-safeguard-20,OpenAI: gpt-oss-safeguard-20b,C,0.00000007,0.00000030,-,131072,65536,JKT,verified,openrouter,2025-10-29,gpt-oss-safeguard-20b is a safety reasoning model from OpenAI built upon gpt-oss-20b. This open-weig,-,-,-
nvidia/nemotron-nano-12b-v2-vl:free,nemotron-nano-12b-v2,NVIDIA: Nemotron Nano 12B 2 VL (free),C,-,-,-,128000,128000,KTV,verified,openrouter,2025-10-28,NVIDIA Nemotron Nano 2 VL is a 12-billion-parameter open multimodal reasoning model designed for vid,-,-,-
nvidia/nemotron-nano-12b-v2-vl,nemotron-nano-12b-v2,NVIDIA: Nemotron Nano 12B 2 VL,C,0.00000020,0.00000060,-,131072,32768,JKV,verified,openrouter,2025-10-28,NVIDIA Nemotron Nano 2 VL is a 12-billion-parameter open multimodal reasoning model designed for vid,-,-,-
minimax/minimax-m2,minimax-m2,MiniMax: MiniMax M2,C,0.00000020,0.00000100,-,196608,65536,JKST,verified,openrouter,2025-10-23,"MiniMax-M2 is a compact, high-efficiency large language model optimized for end-to-end coding and ag",-,-,-
qwen/qwen3-vl-32b-instruct,qwen3-vl-32b-instruc,Qwen: Qwen3 VL 32B Instruct,C,0.00000050,0.00000150,-,262144,65536,JSV,verified,openrouter,2025-10-23,Qwen3-VL-32B-Instruct is a large-scale multimodal vision-language model designed for high-precision ,-,-,-
liquid/lfm2-8b-a1b,lfm2-8b-a1b,LiquidAI/LFM2-8B-A1B,C,0.00000005,0.00000010,-,32768,8192,-,verified,openrouter,2025-10-20,Model created via inbox interface,-,-,-
liquid/lfm-2.2-6b,lfm-2.2-6b,LiquidAI/LFM2-2.6B,C,0.00000005,0.00000010,-,32768,8192,-,verified,openrouter,2025-10-20,"LFM2 is a new generation of hybrid models developed by Liquid AI, specifically designed for edge AI ",-,-,-
ibm-granite/granite-4.0-h-micro,granite-4.0-h-micro,IBM: Granite 4.0 Micro,C,0.00000002,0.00000011,-,131000,32750,-,verified,openrouter,2025-10-19,Granite-4.0-H-Micro is a 3B parameter from the Granite 4 family of models. These models are the late,-,-,-
deepcogito/cogito-v2-preview-llama-405b,cogito-v2-preview-ll,Deep Cogito: Cogito V2 Preview Llama 405B,C,0.00000350,0.00000350,-,32768,8192,JKST,verified,openrouter,2025-10-17,Cogito v2 405B is a dense hybrid reasoning model that combines direct answering capabilities with ad,-,-,-
openai/gpt-5-image-mini,gpt-5-image-mini,OpenAI: GPT-5 Image Mini,C,0.00000250,0.00000200,-,400000,128000,JKSTV,verified,openrouter,2025-10-16,"GPT-5 Image Mini combines OpenAI's advanced language capabilities, powered by [GPT-5 Mini](https://o",-,-,-
anthropic/claude-haiku-4.5,claude-haiku-4.5,Anthropic: Claude Haiku 4.5,C,0.00000100,0.00000500,-,200000,64000,KTV,verified,openrouter,2025-10-15,"Claude Haiku 4.5 is Anthropic’s fastest and most efficient model, delivering near-frontier intellige",-,-,-
qwen/qwen3-vl-8b-thinking,qwen3-vl-8b-thinking,Qwen: Qwen3 VL 8B Thinking,C,0.00000018,0.00000210,-,256000,32768,JKSTV,verified,openrouter,2025-10-14,"Qwen3-VL-8B-Thinking is the reasoning-optimized variant of the Qwen3-VL-8B multimodal model, designe",-,-,-
qwen/qwen3-vl-8b-instruct,qwen3-vl-8b-instruct,Qwen: Qwen3 VL 8B Instruct,C,0.00000008,0.00000050,-,131072,32768,JSTV,verified,openrouter,2025-10-14,"Qwen3-VL-8B-Instruct is a multimodal vision-language model from the Qwen3-VL series, built for high-",-,-,-
openai/gpt-5-image,gpt-5-image,OpenAI: GPT-5 Image,C,0.00001000,0.00001000,-,400000,128000,JKSTV,verified,openrouter,2025-10-14,[GPT-5](https://openrouter.ai/openai/gpt-5) Image combines OpenAI's GPT-5 model with state-of-the-ar,-,-,-
openai/o3-deep-research,o3-deep-research,OpenAI: o3 Deep Research,C,0.00001000,0.00004000,-,200000,100000,JKSTV,verified,openrouter,2025-10-10,"o3-deep-research is OpenAI's advanced model for deep research, designed to tackle complex, multi-ste",-,-,-
openai/o4-mini-deep-research,o4-mini-deep-researc,OpenAI: o4 Mini Deep Research,C,0.00000200,0.00000800,-,200000,100000,JKSTV,verified,openrouter,2025-10-10,"o4-mini-deep-research is OpenAI's faster, more affordable deep research model—ideal for tackling com",-,-,-
nvidia/llama-3.3-nemotron-super-49b-v1.5,llama-3.3-nemotron-s,NVIDIA: Llama 3.3 Nemotron Super 49B V1.5,C,0.00000010,0.00000040,-,131072,32768,JKT,verified,openrouter,2025-10-10,"Llama-3.3-Nemotron-Super-49B-v1.5 is a 49B-parameter, English-centric reasoning/chat model derived f",-,-,-
baidu/ernie-4.5-21b-a3b-thinking,ernie-4.5-21b-a3b-th,Baidu: ERNIE 4.5 21B A3B Thinking,C,0.00000007,0.00000028,-,131072,65536,K,verified,openrouter,2025-10-09,"ERNIE-4.5-21B-A3B-Thinking is Baidu's upgraded lightweight MoE model, refined to boost reasoning dep",-,-,-
google/gemini-2.5-flash-image,gemini-2.5-flash-ima,Google: Gemini 2.5 Flash Image (Nano Banana),C,0.00000030,0.00000250,-,32768,32768,JSV,verified,openrouter,2025-10-07,"Gemini 2.5 Flash Image, a.k.a. ""Nano Banana,"" is now generally available. It is a state of the art i",-,-,-
qwen/qwen3-vl-30b-a3b-thinking,qwen3-vl-30b-a3b-thi,Qwen: Qwen3 VL 30B A3B Thinking,C,0.00000020,0.00000100,-,131072,32768,JKSTV,verified,openrouter,2025-10-06,Qwen3-VL-30B-A3B-Thinking is a multimodal model that unifies strong text generation with visual unde,-,-,-
qwen/qwen3-vl-30b-a3b-instruct,qwen3-vl-30b-a3b-ins,Qwen: Qwen3 VL 30B A3B Instruct,C,0.00000015,0.00000060,-,262144,65536,JSTV,verified,openrouter,2025-10-06,Qwen3-VL-30B-A3B-Instruct is a multimodal model that unifies strong text generation with visual unde,-,-,-
openai/gpt-5-pro,gpt-5-pro,OpenAI: GPT-5 Pro,C,0.00001500,0.00012000,-,400000,128000,JKSTV,verified,openrouter,2025-10-06,"GPT-5 Pro is OpenAI’s most advanced model, offering major improvements in reasoning, code quality, a",-,-,-
z-ai/glm-4.6,glm-4.6,Z.AI: GLM 4.6,C,0.00000035,0.00000150,-,202752,65536,JKST,verified,openrouter,2025-09-30,"Compared with GLM-4.5, this generation brings several key improvements:

Longer context window: The ",-,-,-
z-ai/glm-4.6:exacto,glm-4.6:exacto,Z.AI: GLM 4.6 (exacto),C,0.00000044,0.00000176,-,204800,131072,JKST,verified,openrouter,2025-09-30,"Compared with GLM-4.5, this generation brings several key improvements:

Longer context window: The ",-,-,-
anthropic/claude-sonnet-4.5,claude-sonnet-4.5,Anthropic: Claude Sonnet 4.5,C,0.00000300,0.00001500,-,1000000,64000,JKSTV,verified,openrouter,2025-09-29,"Claude Sonnet 4.5 is Anthropic’s most advanced Sonnet model to date, optimized for real-world agents",-,-,-
deepseek/deepseek-v3.2-exp,deepseek-v3.2-exp,DeepSeek: DeepSeek V3.2 Exp,C,0.00000021,0.00000032,-,163840,65536,JKST,verified,openrouter,2025-09-29,DeepSeek-V3.2-Exp is an experimental large language model released by DeepSeek as an intermediate st,-,-,-
thedrummer/cydonia-24b-v4.1,cydonia-24b-v4.1,TheDrummer: Cydonia 24B V4.1,C,0.00000030,0.00000050,-,131072,131072,JS,verified,openrouter,2025-09-26,"Uncensored and creative writing model based on Mistral Small 3.2 24B with good recall, prompt adhere",-,-,-
relace/relace-apply-3,relace-apply-3,Relace: Relace Apply 3,C,0.00000085,0.00000125,-,256000,128000,-,verified,openrouter,2025-09-26,Relace Apply 3 is a specialized code-patching LLM that merges AI-suggested edits straight into your ,-,-,-
google/gemini-2.5-flash-preview-09-2025,gemini-2.5-flash-pre,Google: Gemini 2.5 Flash Preview 09-2025,C,0.00000030,0.00000250,-,1048576,65536,JKSTV,verified,openrouter,2025-09-25,"Gemini 2.5 Flash Preview September 2025 Checkpoint is Google's state-of-the-art workhorse model, spe",-,-,-
google/gemini-2.5-flash-lite-preview-09-2025,gemini-2.5-flash-lit,Google: Gemini 2.5 Flash Lite Preview 09-2025,C,0.00000010,0.00000040,-,1048576,65536,JKSTV,verified,openrouter,2025-09-25,"Gemini 2.5 Flash-Lite is a lightweight reasoning model in the Gemini 2.5 family, optimized for ultra",-,-,-
qwen/qwen3-vl-235b-a22b-thinking,qwen3-vl-235b-a22b-t,Qwen: Qwen3 VL 235B A22B Thinking,C,0.00000030,0.00000120,-,262144,262144,JKSTV,verified,openrouter,2025-09-23,Qwen3-VL-235B-A22B Thinking is a multimodal model that unifies strong text generation with visual un,-,-,-
qwen/qwen3-vl-235b-a22b-instruct,qwen3-vl-235b-a22b-i,Qwen: Qwen3 VL 235B A22B Instruct,C,0.00000020,0.00000120,-,262144,65536,JSTV,verified,openrouter,2025-09-23,Qwen3-VL-235B-A22B Instruct is an open-weight multimodal model that unifies strong text generation w,-,-,-
qwen/qwen3-max,qwen3-max,Qwen: Qwen3 Max,C,0.00000120,0.00000600,-,256000,32768,JT,verified,openrouter,2025-09-23,"Qwen3-Max is an updated release built on the Qwen3 series, offering major improvements in reasoning,",-,-,-
qwen/qwen3-coder-plus,qwen3-coder-plus,Qwen: Qwen3 Coder Plus,C,0.00000100,0.00000500,-,128000,65536,JST,verified,openrouter,2025-09-23,Qwen3 Coder Plus is Alibaba's proprietary version of the Open Source Qwen3 Coder 480B A35B. It is a ,-,-,-
openai/gpt-5-codex,gpt-5-codex,OpenAI: GPT-5 Codex,C,0.00000125,0.00001000,-,400000,128000,JKSTV,verified,openrouter,2025-09-23,GPT-5-Codex is a specialized version of GPT-5 optimized for software engineering and coding workflow,-,-,-
deepseek/deepseek-v3.1-terminus:exacto,deepseek-v3.1-termin,DeepSeek: DeepSeek V3.1 Terminus (exacto),C,0.00000021,0.00000079,-,163840,40960,JKST,verified,openrouter,2025-09-22,DeepSeek-V3.1 Terminus is an update to [DeepSeek V3.1](/deepseek/deepseek-chat-v3.1) that maintains ,-,-,-
deepseek/deepseek-v3.1-terminus,deepseek-v3.1-termin,DeepSeek: DeepSeek V3.1 Terminus,C,0.00000021,0.00000079,-,163840,40960,JKST,verified,openrouter,2025-09-22,DeepSeek-V3.1 Terminus is an update to [DeepSeek V3.1](/deepseek/deepseek-chat-v3.1) that maintains ,-,-,-
x-ai/grok-4-fast,grok-4-fast,xAI: Grok 4 Fast,C,0.00000020,0.00000050,-,2000000,30000,JKSTV,verified,openrouter,2025-09-18,Grok 4 Fast is xAI's latest multimodal model with SOTA cost-efficiency and a 2M token context window,-,-,-
alibaba/tongyi-deepresearch-30b-a3b:free,tongyi-deepresearch-,Tongyi DeepResearch 30B A3B (free),C,-,-,-,131072,131072,JKST,verified,openrouter,2025-09-18,"Tongyi DeepResearch is an agentic large language model developed by Tongyi Lab, with 30 billion tota",-,-,-
alibaba/tongyi-deepresearch-30b-a3b,tongyi-deepresearch-,Tongyi DeepResearch 30B A3B,C,0.00000009,0.00000040,-,131072,131072,JKST,verified,openrouter,2025-09-18,"Tongyi DeepResearch is an agentic large language model developed by Tongyi Lab, with 30 billion tota",-,-,-
qwen/qwen3-coder-flash,qwen3-coder-flash,Qwen: Qwen3 Coder Flash,C,0.00000030,0.00000150,-,128000,65536,JT,verified,openrouter,2025-09-17,Qwen3 Coder Flash is Alibaba's fast and cost efficient version of their proprietary Qwen3 Coder Plus,-,-,-
opengvlab/internvl3-78b,internvl3-78b,OpenGVLab: InternVL3 78B,C,0.00000010,0.00000039,-,32768,32768,JSV,verified,openrouter,2025-09-15,The InternVL3 series is an advanced multimodal large language model (MLLM). Compared to InternVL 2.5,-,-,-
qwen/qwen3-next-80b-a3b-thinking,qwen3-next-80b-a3b-t,Qwen: Qwen3 Next 80B A3B Thinking,C,0.00000015,0.00000120,-,262144,262144,JKST,verified,openrouter,2025-09-11,Qwen3-Next-80B-A3B-Thinking is a reasoning-first chat model in the Qwen3-Next line that outputs stru,-,-,-
qwen/qwen3-next-80b-a3b-instruct,qwen3-next-80b-a3b-i,Qwen: Qwen3 Next 80B A3B Instruct,C,0.00000006,0.00000060,-,262144,65536,JST,verified,openrouter,2025-09-11,Qwen3-Next-80B-A3B-Instruct is an instruction-tuned chat model in the Qwen3-Next series optimized fo,-,-,-
meituan/longcat-flash-chat,longcat-flash-chat,Meituan: LongCat Flash Chat,C,0.00000020,0.00000080,-,131072,131072,-,verified,openrouter,2025-09-09,"LongCat-Flash-Chat is a large-scale Mixture-of-Experts (MoE) model with 560B total parameters, of wh",-,-,-
qwen/qwen-plus-2025-07-28,qwen-plus-2025-07-28,Qwen: Qwen Plus 0728,C,0.00000040,0.00000120,-,1000000,32768,JST,verified,openrouter,2025-09-08,"Qwen Plus 0728, based on the Qwen3 foundation model, is a 1 million context hybrid reasoning model w",-,-,-
qwen/qwen-plus-2025-07-28:thinking,qwen-plus-2025-07-28,Qwen: Qwen Plus 0728 (thinking),C,0.00000040,0.00000400,-,1000000,32768,JKST,verified,openrouter,2025-09-08,"Qwen Plus 0728, based on the Qwen3 foundation model, is a 1 million context hybrid reasoning model w",-,-,-
nvidia/nemotron-nano-9b-v2:free,nemotron-nano-9b-v2:,NVIDIA: Nemotron Nano 9B V2 (free),C,-,-,-,128000,32000,JKST,verified,openrouter,2025-09-05,"NVIDIA-Nemotron-Nano-9B-v2 is a large language model (LLM) trained from scratch by NVIDIA, and desig",-,-,-
nvidia/nemotron-nano-9b-v2,nemotron-nano-9b-v2,NVIDIA: Nemotron Nano 9B V2,C,0.00000004,0.00000016,-,131072,32768,JKT,verified,openrouter,2025-09-05,"NVIDIA-Nemotron-Nano-9B-v2 is a large language model (LLM) trained from scratch by NVIDIA, and desig",-,-,-
moonshotai/kimi-k2-0905,kimi-k2-0905,MoonshotAI: Kimi K2 0905,C,0.00000039,0.00000190,-,262144,262144,JST,verified,openrouter,2025-09-04,Kimi K2 0905 is the September update of [Kimi K2 0711](moonshotai/kimi-k2). It is a large-scale Mixt,-,-,-
moonshotai/kimi-k2-0905:exacto,kimi-k2-0905:exacto,MoonshotAI: Kimi K2 0905 (exacto),C,0.00000060,0.00000250,-,262144,65536,JST,verified,openrouter,2025-09-04,Kimi K2 0905 is the September update of [Kimi K2 0711](moonshotai/kimi-k2). It is a large-scale Mixt,-,-,-
deepcogito/cogito-v2-preview-llama-70b,cogito-v2-preview-ll,Deep Cogito: Cogito V2 Preview Llama 70B,C,0.00000088,0.00000088,-,32768,8192,JKST,verified,openrouter,2025-09-02,Cogito v2 70B is a dense hybrid reasoning model that combines direct answering capabilities with adv,-,-,-
deepcogito/cogito-v2-preview-llama-109b-moe,cogito-v2-preview-ll,Cogito V2 Preview Llama 109B,C,0.00000018,0.00000059,-,32767,8191,KTV,verified,openrouter,2025-09-02,"An instruction-tuned, hybrid-reasoning Mixture-of-Experts model built on Llama-4-Scout-17B-16E. Cogi",-,-,-
stepfun-ai/step3,step3,StepFun: Step3,C,0.00000057,0.00000142,-,65536,65536,JKSTV,verified,openrouter,2025-08-28,Step3 is a cutting-edge multimodal reasoning model—built on a Mixture-of-Experts architecture with 3,-,-,-
qwen/qwen3-30b-a3b-thinking-2507,qwen3-30b-a3b-thinki,Qwen: Qwen3 30B A3B Thinking 2507,C,0.00000005,0.00000034,-,32768,8192,JKST,verified,openrouter,2025-08-28,Qwen3-30B-A3B-Thinking-2507 is a 30B parameter Mixture-of-Experts reasoning model optimized for comp,-,-,-
x-ai/grok-code-fast-1,grok-code-fast-1,xAI: Grok Code Fast 1,C,0.00000020,0.00000150,-,256000,10000,JKST,verified,openrouter,2025-08-26,Grok Code Fast 1 is a speedy and economical reasoning model that excels at agentic coding. With reas,-,-,-
nousresearch/hermes-4-70b,hermes-4-70b,Nous: Hermes 4 70B,C,0.00000011,0.00000038,-,131072,131072,JKST,verified,openrouter,2025-08-26,"Hermes 4 70B is a hybrid reasoning model from Nous Research, built on Meta-Llama-3.1-70B. It introdu",-,-,-
nousresearch/hermes-4-405b,hermes-4-405b,Nous: Hermes 4 405B,C,0.00000030,0.00000120,-,131072,131072,JKST,verified,openrouter,2025-08-26,Hermes 4 is a large-scale reasoning model built on Meta-Llama-3.1-405B and released by Nous Research,-,-,-
google/gemini-2.5-flash-image-preview,gemini-2.5-flash-ima,Google: Gemini 2.5 Flash Image Preview (Nano Banana),C,0.00000030,0.00000250,-,32768,32768,JSV,verified,openrouter,2025-08-26,"Gemini 2.5 Flash Image Preview, a.k.a. ""Nano Banana,"" is a state of the art image generation model w",-,-,-
deepseek/deepseek-chat-v3.1,deepseek-chat-v3.1,DeepSeek: DeepSeek V3.1,C,0.00000015,0.00000075,-,32768,7168,JKST,verified,openrouter,2025-08-21,"DeepSeek-V3.1 is a large hybrid reasoning model (671B parameters, 37B active) that supports both thi",-,-,-
openai/gpt-4o-audio-preview,gpt-4o-audio-preview,OpenAI: GPT-4o Audio,C,0.00000250,0.00001000,-,128000,16384,JST,verified,openrouter,2025-08-14,The gpt-4o-audio-preview model adds support for audio inputs as prompts. This enhancement allows the,-,-,-
mistralai/mistral-medium-3.1,mistral-medium-3.1,Mistral: Mistral Medium 3.1,C,0.00000040,0.00000200,-,131072,32768,JSTV,verified,openrouter,2025-08-13,"Mistral Medium 3.1 is an updated version of Mistral Medium 3, which is a high-performance enterprise",-,-,-
baidu/ernie-4.5-21b-a3b,ernie-4.5-21b-a3b,Baidu: ERNIE 4.5 21B A3B,C,0.00000007,0.00000028,-,120000,8000,T,verified,openrouter,2025-08-12,A sophisticated text-based Mixture-of-Experts (MoE) model featuring 21B total parameters with 3B act,-,-,-
baidu/ernie-4.5-vl-28b-a3b,ernie-4.5-vl-28b-a3b,Baidu: ERNIE 4.5 VL 28B A3B,C,0.00000014,0.00000056,-,30000,8000,KTV,verified,openrouter,2025-08-12,A powerful multimodal Mixture-of-Experts chat model featuring 28B total parameters with 3B activated,-,-,-
z-ai/glm-4.5v,glm-4.5v,Z.AI: GLM 4.5V,C,0.00000060,0.00000180,-,65536,16384,JKSTV,verified,openrouter,2025-08-11,GLM-4.5V is a vision-language foundation model for multimodal agent applications. Built on a Mixture,-,-,-
ai21/jamba-mini-1.7,jamba-mini-1.7,AI21: Jamba Mini 1.7,C,0.00000020,0.00000040,-,256000,4096,JT,verified,openrouter,2025-08-08,"Jamba Mini 1.7 is a compact and efficient member of the Jamba open model family, incorporating key i",-,-,-
ai21/jamba-large-1.7,jamba-large-1.7,AI21: Jamba Large 1.7,C,0.00000200,0.00000800,-,256000,4096,JT,verified,openrouter,2025-08-08,"Jamba Large 1.7 is the latest model in the Jamba open family, offering improvements in grounding, in",-,-,-
openai/gpt-5-chat,gpt-5-chat,OpenAI: GPT-5 Chat,C,0.00000125,0.00001000,-,128000,16384,JSV,verified,openrouter,2025-08-07,"GPT-5 Chat is designed for advanced, natural, multimodal, and context-aware conversations for enterp",-,-,-
openai/gpt-5,gpt-5,OpenAI: GPT-5,C,0.00000125,0.00001000,-,400000,128000,JKSTV,verified,openrouter,2025-08-07,"GPT-5 is OpenAI’s most advanced model, offering major improvements in reasoning, code quality, and u",-,-,-
openai/gpt-5-mini,gpt-5-mini,OpenAI: GPT-5 Mini,C,0.00000025,0.00000200,-,400000,128000,JKSTV,verified,openrouter,2025-08-07,"GPT-5 Mini is a compact version of GPT-5, designed to handle lighter-weight reasoning tasks. It prov",-,-,-
openai/gpt-5-nano,gpt-5-nano,OpenAI: GPT-5 Nano,C,0.00000005,0.00000040,-,400000,128000,JKSTV,verified,openrouter,2025-08-07,"GPT-5-Nano is the smallest and fastest variant in the GPT-5 system, optimized for developer tools, r",-,-,-
openai/gpt-oss-120b:free,gpt-oss-120b:free,OpenAI: gpt-oss-120b (free),C,-,-,-,131072,32768,KT,verified,openrouter,2025-08-05,"gpt-oss-120b is an open-weight, 117B-parameter Mixture-of-Experts (MoE) language model from OpenAI d",-,-,-
openai/gpt-oss-120b,gpt-oss-120b,OpenAI: gpt-oss-120b,C,0.00000002,0.00000010,-,131072,32768,JKST,verified,openrouter,2025-08-05,"gpt-oss-120b is an open-weight, 117B-parameter Mixture-of-Experts (MoE) language model from OpenAI d",-,-,-
openai/gpt-oss-120b:exacto,gpt-oss-120b:exacto,OpenAI: gpt-oss-120b (exacto),C,0.00000004,0.00000019,-,131072,32768,JKST,verified,openrouter,2025-08-05,"gpt-oss-120b is an open-weight, 117B-parameter Mixture-of-Experts (MoE) language model from OpenAI d",-,-,-
openai/gpt-oss-20b:free,gpt-oss-20b:free,OpenAI: gpt-oss-20b (free),C,-,-,-,131072,131072,JKST,verified,openrouter,2025-08-05,gpt-oss-20b is an open-weight 21B parameter model released by OpenAI under the Apache 2.0 license. I,-,-,-
openai/gpt-oss-20b,gpt-oss-20b,OpenAI: gpt-oss-20b,C,0.00000002,0.00000006,-,131072,32768,JKST,verified,openrouter,2025-08-05,gpt-oss-20b is an open-weight 21B parameter model released by OpenAI under the Apache 2.0 license. I,-,-,-
anthropic/claude-opus-4.1,claude-opus-4.1,Anthropic: Claude Opus 4.1,C,0.00001500,0.00007500,-,200000,50000,JKSTV,verified,openrouter,2025-08-05,"Claude Opus 4.1 is an updated version of Anthropic’s flagship model, offering improved performance i",-,-,-
mistralai/codestral-2508,codestral-2508,Mistral: Codestral 2508,C,0.00000030,0.00000090,-,256000,64000,JST,verified,openrouter,2025-08-01,Mistral's cutting-edge language model for coding released end of July 2025. Codestral specializes in,-,-,-
qwen/qwen3-coder-30b-a3b-instruct,qwen3-coder-30b-a3b-,Qwen: Qwen3 Coder 30B A3B Instruct,C,0.00000007,0.00000027,-,160000,32768,JST,verified,openrouter,2025-07-31,Qwen3-Coder-30B-A3B-Instruct is a 30.5B parameter Mixture-of-Experts (MoE) model with 128 experts (8,-,-,-
qwen/qwen3-30b-a3b-instruct-2507,qwen3-30b-a3b-instru,Qwen: Qwen3 30B A3B Instruct 2507,C,0.00000008,0.00000033,-,262144,262144,JST,verified,openrouter,2025-07-29,"Qwen3-30B-A3B-Instruct-2507 is a 30.5B-parameter mixture-of-experts language model from Qwen, with 3",-,-,-
z-ai/glm-4.5,glm-4.5,Z.AI: GLM 4.5,C,0.00000035,0.00000155,-,131072,65536,JKST,verified,openrouter,2025-07-25,"GLM-4.5 is our latest flagship foundation model, purpose-built for agent-based applications. It leve",-,-,-
z-ai/glm-4.5-air:free,glm-4.5-air:free,Z.AI: GLM 4.5 Air (free),C,-,-,-,131072,131072,JKST,verified,openrouter,2025-07-25,"GLM-4.5-Air is the lightweight variant of our latest flagship model family, also purpose-built for a",-,-,-
z-ai/glm-4.5-air,glm-4.5-air,Z.AI: GLM 4.5 Air,C,0.00000013,0.00000085,-,131072,98304,JKST,verified,openrouter,2025-07-25,"GLM-4.5-Air is the lightweight variant of our latest flagship model family, also purpose-built for a",-,-,-
qwen/qwen3-235b-a22b-thinking-2507,qwen3-235b-a22b-thin,Qwen: Qwen3 235B A22B Thinking 2507,C,0.00000011,0.00000060,-,262144,262144,JKST,verified,openrouter,2025-07-25,"Qwen3-235B-A22B-Thinking-2507 is a high-performance, open-weight Mixture-of-Experts (MoE) language m",-,-,-
z-ai/glm-4-32b,glm-4-32b,Z.AI: GLM 4 32B ,C,0.00000010,0.00000010,-,128000,32000,T,verified,openrouter,2025-07-24,"GLM 4 32B is a cost-effective foundation language model.

It can efficiently perform complex tasks a",-,-,-
qwen/qwen3-coder:free,qwen3-coder:free,Qwen: Qwen3 Coder 480B A35B (free),C,-,-,-,262000,262000,T,verified,openrouter,2025-07-22,Qwen3-Coder-480B-A35B-Instruct is a Mixture-of-Experts (MoE) code generation model developed by the ,-,-,-
qwen/qwen3-coder,qwen3-coder,Qwen: Qwen3 Coder 480B A35B,C,0.00000022,0.00000095,-,262144,262144,JKST,verified,openrouter,2025-07-22,Qwen3-Coder-480B-A35B-Instruct is a Mixture-of-Experts (MoE) code generation model developed by the ,-,-,-
qwen/qwen3-coder:exacto,qwen3-coder:exacto,Qwen: Qwen3 Coder 480B A35B (exacto),C,0.00000022,0.00000180,-,262144,65536,JKST,verified,openrouter,2025-07-22,Qwen3-Coder-480B-A35B-Instruct is a Mixture-of-Experts (MoE) code generation model developed by the ,-,-,-
bytedance/ui-tars-1.5-7b,ui-tars-1.5-7b,ByteDance: UI-TARS 7B ,C,0.00000010,0.00000020,-,128000,2048,V,verified,openrouter,2025-07-22,"UI-TARS-1.5 is a multimodal vision-language agent optimized for GUI-based environments, including de",-,-,-
google/gemini-2.5-flash-lite,gemini-2.5-flash-lit,Google: Gemini 2.5 Flash Lite,C,0.00000010,0.00000040,-,1048576,65535,JKSTV,verified,openrouter,2025-07-22,"Gemini 2.5 Flash-Lite is a lightweight reasoning model in the Gemini 2.5 family, optimized for ultra",-,-,-
qwen/qwen3-235b-a22b-2507,qwen3-235b-a22b-2507,Qwen: Qwen3 235B A22B Instruct 2507,C,0.00000007,0.00000046,-,262144,65536,JKST,verified,openrouter,2025-07-21,"Qwen3-235B-A22B-Instruct-2507 is a multilingual, instruction-tuned mixture-of-experts language model",-,-,-
switchpoint/router,router,Switchpoint Router,C,0.00000085,0.00000340,-,131072,32768,K,verified,openrouter,2025-07-11,Switchpoint AI's router instantly analyzes your request and directs it to the optimal AI from an eve,-,-,-
moonshotai/kimi-k2:free,kimi-k2:free,MoonshotAI: Kimi K2 0711 (free),C,-,-,-,32768,8192,-,verified,openrouter,2025-07-11,"Kimi K2 Instruct is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, ",-,-,-
moonshotai/kimi-k2,kimi-k2,MoonshotAI: Kimi K2 0711,C,0.00000046,0.00000184,-,131072,131072,JST,verified,openrouter,2025-07-11,"Kimi K2 Instruct is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, ",-,-,-
thudm/glm-4.1v-9b-thinking,glm-4.1v-9b-thinking,THUDM: GLM 4.1V 9B Thinking,C,0.00000004,0.00000014,-,65536,8000,KV,verified,openrouter,2025-07-11,"GLM-4.1V-9B-Thinking is a 9B parameter vision-language model developed by THUDM, based on the GLM-4-",-,-,-
mistralai/devstral-medium,devstral-medium,Mistral: Devstral Medium,C,0.00000040,0.00000200,-,131072,32768,JST,verified,openrouter,2025-07-10,Devstral Medium is a high-performance code generation and agentic reasoning model developed jointly ,-,-,-
mistralai/devstral-small,devstral-small,Mistral: Devstral Small 1.1,C,0.00000007,0.00000028,-,128000,32000,JST,verified,openrouter,2025-07-10,"Devstral Small 1.1 is a 24B parameter open-weight language model for software engineering agents, de",-,-,-
cognitivecomputations/dolphin-mistral-24b-venice-edition:free,dolphin-mistral-24b-,Venice: Uncensored (free),C,-,-,-,32768,8192,JS,verified,openrouter,2025-07-09,Venice Uncensored Dolphin Mistral 24B Venice Edition is a fine-tuned variant of Mistral-Small-24B-In,-,-,-
x-ai/grok-4,grok-4,xAI: Grok 4,C,0.00000300,0.00001500,-,256000,64000,JKSTV,verified,openrouter,2025-07-09,Grok 4 is xAI's latest reasoning model with a 256k context window. It supports parallel tool calling,-,-,-
google/gemma-3n-e2b-it:free,gemma-3n-e2b-it:free,Google: Gemma 3n 2B (free),C,-,-,-,8192,2048,J,verified,openrouter,2025-07-09,"Gemma 3n E2B IT is a multimodal, instruction-tuned model developed by Google DeepMind, designed to o",-,-,-
tencent/hunyuan-a13b-instruct,hunyuan-a13b-instruc,Tencent: Hunyuan A13B Instruct,C,0.00000014,0.00000057,-,131072,131072,JKS,verified,openrouter,2025-07-08,"Hunyuan-A13B is a 13B active parameter Mixture-of-Experts (MoE) language model developed by Tencent,",-,-,-
tngtech/deepseek-r1t2-chimera:free,deepseek-r1t2-chimer,TNG: DeepSeek R1T2 Chimera (free),C,-,-,-,163840,40960,K,verified,openrouter,2025-07-08,DeepSeek-TNG-R1T2-Chimera is the second-generation Chimera model from TNG Tech. It is a 671 B-parame,-,-,-
tngtech/deepseek-r1t2-chimera,deepseek-r1t2-chimer,TNG: DeepSeek R1T2 Chimera,C,0.00000025,0.00000085,-,163840,163840,JKST,verified,openrouter,2025-07-08,DeepSeek-TNG-R1T2-Chimera is the second-generation Chimera model from TNG Tech. It is a 671 B-parame,-,-,-
morph/morph-v3-large,morph-v3-large,Morph: Morph V3 Large,C,0.00000090,0.00000190,-,262144,131072,-,verified,openrouter,2025-07-07,"Morph's high-accuracy apply model for complex code edits. ~4,500 tokens/sec with 98% accuracy for pr",-,-,-
morph/morph-v3-fast,morph-v3-fast,Morph: Morph V3 Fast,C,0.00000080,0.00000120,-,81920,38000,-,verified,openrouter,2025-07-07,"Morph's fastest apply model for code edits. ~10,500 tokens/sec with 96% accuracy for rapid code tran",-,-,-
baidu/ernie-4.5-vl-424b-a47b,ernie-4.5-vl-424b-a4,Baidu: ERNIE 4.5 VL 424B A47B ,C,0.00000042,0.00000125,-,123000,16000,KV,verified,openrouter,2025-06-30,"ERNIE-4.5-VL-424B-A47B is a multimodal Mixture-of-Experts (MoE) model from Baidu’s ERNIE 4.5 series,",-,-,-
baidu/ernie-4.5-300b-a47b,ernie-4.5-300b-a47b,Baidu: ERNIE 4.5 300B A47B ,C,0.00000028,0.00000110,-,123000,12000,JS,verified,openrouter,2025-06-30,ERNIE-4.5-300B-A47B is a 300B parameter Mixture-of-Experts (MoE) language model developed by Baidu a,-,-,-
inception/mercury,mercury,Inception: Mercury,C,0.00000025,0.00000100,-,128000,16384,JST,verified,openrouter,2025-06-26,Mercury is the first diffusion large language model (dLLM). Applying a breakthrough discrete diffusi,-,-,-
mistralai/mistral-small-3.2-24b-instruct,mistral-small-3.2-24,Mistral: Mistral Small 3.2 24B,C,0.00000006,0.00000018,-,131072,131072,JSTV,verified,openrouter,2025-06-20,Mistral-Small-3.2-24B-Instruct-2506 is an updated 24B parameter model from Mistral optimized for ins,-,-,-
minimax/minimax-m1,minimax-m1,MiniMax: MiniMax M1,C,0.00000040,0.00000220,-,1000000,40000,KT,verified,openrouter,2025-06-17,"MiniMax-M1 is a large-scale, open-weight reasoning model designed for extended context and high-effi",-,-,-
google/gemini-2.5-flash,gemini-2.5-flash,Google: Gemini 2.5 Flash,C,0.00000030,0.00000250,-,1048576,65535,JKSTV,verified,openrouter,2025-06-17,"Gemini 2.5 Flash is Google's state-of-the-art workhorse model, specifically designed for advanced re",-,-,-
google/gemini-2.5-pro,gemini-2.5-pro,Google: Gemini 2.5 Pro,C,0.00000125,0.00001000,-,1048576,65536,JKSTV,verified,openrouter,2025-06-17,"Gemini 2.5 Pro is Google’s state-of-the-art AI model designed for advanced reasoning, coding, mathem",-,-,-
moonshotai/kimi-dev-72b,kimi-dev-72b,MoonshotAI: Kimi Dev 72B,C,0.00000029,0.00000115,-,131072,131072,JKS,verified,openrouter,2025-06-16,Kimi-Dev-72B is an open-source large language model fine-tuned for software engineering and issue re,-,-,-
openai/o3-pro,o3-pro,OpenAI: o3 Pro,C,0.00002000,0.00008000,-,200000,100000,JKSTV,verified,openrouter,2025-06-10,The o-series of models are trained with reinforcement learning to think before they answer and perfo,-,-,-
x-ai/grok-3-mini,grok-3-mini,xAI: Grok 3 Mini,C,0.00000030,0.00000050,-,131072,32768,JKST,verified,openrouter,2025-06-10,"A lightweight model that thinks before responding. Fast, smart, and great for logic-based tasks that",-,-,-
x-ai/grok-3,grok-3,xAI: Grok 3,C,0.00000300,0.00001500,-,131072,32768,JST,verified,openrouter,2025-06-10,Grok 3 is the latest model from xAI. It's their flagship model that excels at enterprise use cases l,-,-,-
google/gemini-2.5-pro-preview,gemini-2.5-pro-previ,Google: Gemini 2.5 Pro Preview 06-05,C,0.00000125,0.00001000,-,1048576,65536,JKSTV,verified,openrouter,2025-06-05,"Gemini 2.5 Pro is Google’s state-of-the-art AI model designed for advanced reasoning, coding, mathem",-,-,-
deepseek/deepseek-r1-0528-qwen3-8b,deepseek-r1-0528-qwe,DeepSeek: DeepSeek R1 0528 Qwen3 8B,C,0.00000006,0.00000009,-,128000,32000,K,verified,openrouter,2025-05-29,DeepSeek-R1-0528 is a lightly upgraded release of DeepSeek R1 that taps more compute and smarter pos,-,-,-
deepseek/deepseek-r1-0528:free,deepseek-r1-0528:fre,DeepSeek: R1 0528 (free),C,-,-,-,163840,40960,K,verified,openrouter,2025-05-28,May 28th update to the [original DeepSeek R1](/deepseek/deepseek-r1) Performance on par with [OpenAI,-,-,-
deepseek/deepseek-r1-0528,deepseek-r1-0528,DeepSeek: R1 0528,C,0.00000040,0.00000175,-,163840,65536,JKST,verified,openrouter,2025-05-28,May 28th update to the [original DeepSeek R1](/deepseek/deepseek-r1) Performance on par with [OpenAI,-,-,-
anthropic/claude-opus-4,claude-opus-4,Anthropic: Claude Opus 4,C,0.00001500,0.00007500,-,200000,32000,KTV,verified,openrouter,2025-05-22,"Claude Opus 4 is benchmarked as the world’s best coding model, at time of release, bringing sustaine",-,-,-
anthropic/claude-sonnet-4,claude-sonnet-4,Anthropic: Claude Sonnet 4,C,0.00000300,0.00001500,-,1000000,64000,KTV,verified,openrouter,2025-05-22,"Claude Sonnet 4 significantly enhances the capabilities of its predecessor, Sonnet 3.7, excelling in",-,-,-
mistralai/devstral-small-2505,devstral-small-2505,Mistral: Devstral Small 2505,C,0.00000006,0.00000012,-,128000,32000,J,verified,openrouter,2025-05-21,"Devstral-Small-2505 is a 24B parameter agentic LLM fine-tuned from Mistral-Small-3.1, jointly develo",-,-,-
google/gemma-3n-e4b-it:free,gemma-3n-e4b-it:free,Google: Gemma 3n 4B (free),C,-,-,-,8192,2048,J,verified,openrouter,2025-05-20,"Gemma 3n E4B-it is optimized for efficient execution on mobile and low-resource devices, such as pho",-,-,-
google/gemma-3n-e4b-it,gemma-3n-e4b-it,Google: Gemma 3n 4B,C,0.00000002,0.00000004,-,32768,8192,-,verified,openrouter,2025-05-20,"Gemma 3n E4B-it is optimized for efficient execution on mobile and low-resource devices, such as pho",-,-,-
openai/codex-mini,codex-mini,OpenAI: Codex Mini,C,0.00000150,0.00000600,-,200000,100000,JKSTV,verified,openrouter,2025-05-16,codex-mini-latest is a fine-tuned version of o4-mini specifically for use in Codex CLI. For direct u,-,-,-
nousresearch/deephermes-3-mistral-24b-preview,deephermes-3-mistral,Nous: DeepHermes 3 Mistral 24B Preview,C,0.00000002,0.00000010,-,32768,32768,JKST,verified,openrouter,2025-05-09,DeepHermes 3 (Mistral 24B Preview) is an instruction-tuned language model by Nous Research based on ,-,-,-
mistralai/mistral-medium-3,mistral-medium-3,Mistral: Mistral Medium 3,C,0.00000040,0.00000200,-,131072,32768,JSTV,verified,openrouter,2025-05-07,Mistral Medium 3 is a high-performance enterprise-grade language model designed to deliver frontier-,-,-,-
google/gemini-2.5-pro-preview-05-06,gemini-2.5-pro-previ,Google: Gemini 2.5 Pro Preview 05-06,C,0.00000125,0.00001000,-,1048576,65535,JKSTV,verified,openrouter,2025-05-06,"Gemini 2.5 Pro is Google’s state-of-the-art AI model designed for advanced reasoning, coding, mathem",-,-,-
arcee-ai/spotlight,spotlight,Arcee AI: Spotlight,C,0.00000018,0.00000018,-,131072,65537,V,verified,openrouter,2025-05-05,Spotlight is a 7‑billion‑parameter vision‑language model derived from Qwen 2.5‑VL and fine‑tuned by ,-,-,-
arcee-ai/maestro-reasoning,maestro-reasoning,Arcee AI: Maestro Reasoning,C,0.00000090,0.00000330,-,131072,32000,-,verified,openrouter,2025-05-05,Maestro Reasoning is Arcee's flagship analysis model: a 32 B‑parameter derivative of Qwen 2.5‑32 B t,-,-,-
arcee-ai/virtuoso-large,virtuoso-large,Arcee AI: Virtuoso Large,C,0.00000075,0.00000120,-,131072,64000,T,verified,openrouter,2025-05-05,"Virtuoso‑Large is Arcee's top‑tier general‑purpose LLM at 72 B parameters, tuned to tackle cross‑dom",-,-,-
arcee-ai/coder-large,coder-large,Arcee AI: Coder Large,C,0.00000050,0.00000080,-,32768,8192,-,verified,openrouter,2025-05-05,Coder‑Large is a 32 B‑parameter offspring of Qwen 2.5‑Instruct that has been further trained on perm,-,-,-
microsoft/phi-4-reasoning-plus,phi-4-reasoning-plus,Microsoft: Phi 4 Reasoning Plus,C,0.00000007,0.00000035,-,32768,8192,JK,verified,openrouter,2025-05-01,"Phi-4-reasoning-plus is an enhanced 14B parameter model from Microsoft, fine-tuned from Phi-4 with a",-,-,-
inception/mercury-coder,mercury-coder,Inception: Mercury Coder,C,0.00000025,0.00000100,-,128000,16384,JST,verified,openrouter,2025-04-30,Mercury Coder is the first diffusion large language model (dLLM). Applying a breakthrough discrete d,-,-,-
qwen/qwen3-4b:free,qwen3-4b:free,Qwen: Qwen3 4B (free),C,-,-,-,40960,10240,JKST,verified,openrouter,2025-04-30,"Qwen3-4B is a 4 billion parameter dense language model from the Qwen3 series, designed to support bo",-,-,-
deepseek/deepseek-prover-v2,deepseek-prover-v2,DeepSeek: DeepSeek Prover V2,C,0.00000050,0.00000218,-,163840,40960,J,verified,openrouter,2025-04-30,"DeepSeek Prover V2 is a 671B parameter model, speculated to be geared towards logic and mathematics.",-,-,-
meta-llama/llama-guard-4-12b,llama-guard-4-12b,Meta: Llama Guard 4 12B,C,0.00000018,0.00000018,-,163840,40960,JV,verified,openrouter,2025-04-29,"Llama Guard 4 is a Llama 4 Scout-derived multimodal pretrained model, fine-tuned for content safety ",-,-,-
qwen/qwen3-30b-a3b,qwen3-30b-a3b,Qwen: Qwen3 30B A3B,C,0.00000006,0.00000022,-,40960,40960,JKST,verified,openrouter,2025-04-28,"Qwen3, the latest generation in the Qwen large language model series, features both dense and mixtur",-,-,-
qwen/qwen3-8b,qwen3-8b,Qwen: Qwen3 8B,C,0.00000004,0.00000014,-,128000,20000,JKST,verified,openrouter,2025-04-28,"Qwen3-8B is a dense 8.2B parameter causal language model from the Qwen3 series, designed for both re",-,-,-
qwen/qwen3-14b,qwen3-14b,Qwen: Qwen3 14B,C,0.00000005,0.00000022,-,40960,40960,JKST,verified,openrouter,2025-04-28,"Qwen3-14B is a dense 14.8B parameter causal language model from the Qwen3 series, designed for both ",-,-,-
qwen/qwen3-32b,qwen3-32b,Qwen: Qwen3 32B,C,0.00000008,0.00000024,-,40960,40960,JKST,verified,openrouter,2025-04-28,"Qwen3-32B is a dense 32.8B parameter causal language model from the Qwen3 series, optimized for both",-,-,-
qwen/qwen3-235b-a22b,qwen3-235b-a22b,Qwen: Qwen3 235B A22B,C,0.00000018,0.00000054,-,40960,40960,JKST,verified,openrouter,2025-04-28,"Qwen3-235B-A22B is a 235B parameter mixture-of-experts (MoE) model developed by Qwen, activating 22B",-,-,-
tngtech/deepseek-r1t-chimera:free,deepseek-r1t-chimera,TNG: DeepSeek R1T Chimera (free),C,-,-,-,163840,40960,K,verified,openrouter,2025-04-27,"DeepSeek-R1T-Chimera is created by merging DeepSeek-R1 and DeepSeek-V3 (0324), combining the reasoni",-,-,-
tngtech/deepseek-r1t-chimera,deepseek-r1t-chimera,TNG: DeepSeek R1T Chimera,C,0.00000030,0.00000120,-,163840,163840,JKS,verified,openrouter,2025-04-27,"DeepSeek-R1T-Chimera is created by merging DeepSeek-R1 and DeepSeek-V3 (0324), combining the reasoni",-,-,-
openai/o4-mini-high,o4-mini-high,OpenAI: o4 Mini High,C,0.00000110,0.00000440,-,200000,100000,JKSTV,verified,openrouter,2025-04-16,OpenAI o4-mini-high is the same model as [o4-mini](/openai/o4-mini) with reasoning_effort set to hig,-,-,-
openai/o3,o3,OpenAI: o3,C,0.00000200,0.00000800,-,200000,100000,JKSTV,verified,openrouter,2025-04-16,"o3 is a well-rounded and powerful model across domains. It sets a new standard for math, science, co",-,-,-
openai/o4-mini,o4-mini,OpenAI: o4 Mini,C,0.00000110,0.00000440,-,200000,100000,JKSTV,verified,openrouter,2025-04-16,"OpenAI o4-mini is a compact reasoning model in the o-series, optimized for fast, cost-efficient perf",-,-,-
qwen/qwen2.5-coder-7b-instruct,qwen2.5-coder-7b-ins,Qwen: Qwen2.5 Coder 7B Instruct,C,0.00000003,0.00000009,-,32768,8192,JS,verified,openrouter,2025-04-15,Qwen2.5-Coder-7B-Instruct is a 7B parameter instruction-tuned language model optimized for code-rela,-,-,-
openai/gpt-4.1,gpt-4.1,OpenAI: GPT-4.1,C,0.00000200,0.00000800,-,1047576,32768,JSTV,verified,openrouter,2025-04-14,"GPT-4.1 is a flagship large language model optimized for advanced instruction following, real-world ",-,-,-
openai/gpt-4.1-mini,gpt-4.1-mini,OpenAI: GPT-4.1 Mini,C,0.00000040,0.00000160,-,1047576,32768,JSTV,verified,openrouter,2025-04-14,GPT-4.1 Mini is a mid-sized model delivering performance competitive with GPT-4o at substantially lo,-,-,-
openai/gpt-4.1-nano,gpt-4.1-nano,OpenAI: GPT-4.1 Nano,C,0.00000010,0.00000040,-,1047576,32768,JSTV,verified,openrouter,2025-04-14,"For tasks that demand low latency, GPT‑4.1 nano is the fastest and cheapest model in the GPT-4.1 ser",-,-,-
eleutherai/llemma_7b,llemma_7b,EleutherAI: Llemma 7b,C,0.00000080,0.00000120,-,4096,4096,-,verified,openrouter,2025-04-14,"Llemma 7B is a language model for mathematics. It was initialized with Code Llama 7B weights, and tr",-,-,-
alfredpros/codellama-7b-instruct-solidity,codellama-7b-instruc,AlfredPros: CodeLLaMa 7B Instruct Solidity,C,0.00000080,0.00000120,-,4096,4096,-,verified,openrouter,2025-04-14,A finetuned 7 billion parameters Code LLaMA - Instruct model to generate Solidity smart contract usi,-,-,-
arliai/qwq-32b-arliai-rpr-v1,qwq-32b-arliai-rpr-v,ArliAI: QwQ 32B RpR v1,C,0.00000003,0.00000011,-,32768,32768,JKS,verified,openrouter,2025-04-13,QwQ-32B-ArliAI-RpR-v1 is a 32B parameter model fine-tuned from Qwen/QwQ-32B using a curated creative,-,-,-
x-ai/grok-3-mini-beta,grok-3-mini-beta,xAI: Grok 3 Mini Beta,C,0.00000030,0.00000050,-,131072,32768,JKT,verified,openrouter,2025-04-09,"Grok 3 Mini is a lightweight, smaller thinking model. Unlike traditional models that generate answer",-,-,-
x-ai/grok-3-beta,grok-3-beta,xAI: Grok 3 Beta,C,0.00000300,0.00001500,-,131072,32768,JT,verified,openrouter,2025-04-09,Grok 3 is the latest model from xAI. It's their flagship model that excels at enterprise use cases l,-,-,-
nvidia/llama-3.1-nemotron-ultra-253b-v1,llama-3.1-nemotron-u,NVIDIA: Llama 3.1 Nemotron Ultra 253B v1,C,0.00000060,0.00000180,-,131072,32768,JKS,verified,openrouter,2025-04-08,"Llama-3.1-Nemotron-Ultra-253B-v1 is a large language model (LLM) optimized for advanced reasoning, h",-,-,-
meta-llama/llama-4-maverick,llama-4-maverick,Meta: Llama 4 Maverick,C,0.00000015,0.00000060,-,1048576,16384,JSTV,verified,openrouter,2025-04-05,"Llama 4 Maverick 17B Instruct (128E) is a high-capacity multimodal language model from Meta, built o",-,-,-
meta-llama/llama-4-scout,llama-4-scout,Meta: Llama 4 Scout,C,0.00000008,0.00000030,-,327680,16384,JSTV,verified,openrouter,2025-04-05,"Llama 4 Scout 17B Instruct (16E) is a mixture-of-experts (MoE) language model developed by Meta, act",-,-,-
qwen/qwen2.5-vl-32b-instruct,qwen2.5-vl-32b-instr,Qwen: Qwen2.5 VL 32B Instruct,C,0.00000005,0.00000022,-,16384,16384,JSV,verified,openrouter,2025-03-24,Qwen2.5-VL-32B is a multimodal vision-language model fine-tuned through reinforcement learning for e,-,-,-
deepseek/deepseek-chat-v3-0324,deepseek-chat-v3-032,DeepSeek: DeepSeek V3 0324,C,0.00000020,0.00000088,-,163840,40960,JKST,verified,openrouter,2025-03-24,"DeepSeek V3, a 685B-parameter, mixture-of-experts model, is the latest iteration of the flagship cha",-,-,-
openai/o1-pro,o1-pro,OpenAI: o1-pro,C,0.00015000,0.00060000,-,200000,100000,JKSV,verified,openrouter,2025-03-19,The o1 series of models are trained with reinforcement learning to think before they answer and perf,-,-,-
mistralai/mistral-small-3.1-24b-instruct:free,mistral-small-3.1-24,Mistral: Mistral Small 3.1 24B (free),C,-,-,-,128000,32000,JSTV,verified,openrouter,2025-03-17,"Mistral Small 3.1 24B Instruct is an upgraded variant of Mistral Small 3 (2501), featuring 24 billio",-,-,-
mistralai/mistral-small-3.1-24b-instruct,mistral-small-3.1-24,Mistral: Mistral Small 3.1 24B,C,0.00000003,0.00000011,-,131072,131072,JSTV,verified,openrouter,2025-03-17,"Mistral Small 3.1 24B Instruct is an upgraded variant of Mistral Small 3 (2501), featuring 24 billio",-,-,-
allenai/olmo-2-0325-32b-instruct,olmo-2-0325-32b-inst,AllenAI: Olmo 2 32B Instruct,C,0.00000005,0.00000020,-,128000,32000,-,verified,openrouter,2025-03-14,OLMo-2 32B Instruct is a supervised instruction-finetuned variant of the OLMo-2 32B March 2025 base ,-,-,-
google/gemma-3-4b-it:free,gemma-3-4b-it:free,Google: Gemma 3 4B (free),C,-,-,-,32768,8192,JSV,verified,openrouter,2025-03-13,"Gemma 3 introduces multimodality, supporting vision-language input and text outputs. It handles cont",-,-,-
google/gemma-3-4b-it,gemma-3-4b-it,Google: Gemma 3 4B,C,0.00000002,0.00000007,-,96000,24000,JV,verified,openrouter,2025-03-13,"Gemma 3 introduces multimodality, supporting vision-language input and text outputs. It handles cont",-,-,-
google/gemma-3-12b-it:free,gemma-3-12b-it:free,Google: Gemma 3 12B (free),C,-,-,-,32768,8192,V,verified,openrouter,2025-03-13,"Gemma 3 introduces multimodality, supporting vision-language input and text outputs. It handles cont",-,-,-
google/gemma-3-12b-it,gemma-3-12b-it,Google: Gemma 3 12B,C,0.00000003,0.00000010,-,131072,131072,JSV,verified,openrouter,2025-03-13,"Gemma 3 introduces multimodality, supporting vision-language input and text outputs. It handles cont",-,-,-
cohere/command-a,command-a,Cohere: Command A,C,0.00000250,0.00001000,-,256000,8192,JS,verified,openrouter,2025-03-13,Command A is an open-weights 111B parameter model with a 256k context window focused on delivering g,-,-,-
openai/gpt-4o-mini-search-preview,gpt-4o-mini-search-p,OpenAI: GPT-4o-mini Search Preview,C,0.00000015,0.00000060,-,128000,16384,JS,verified,openrouter,2025-03-12,GPT-4o mini Search Preview is a specialized model for web search in Chat Completions. It is trained ,-,-,-
openai/gpt-4o-search-preview,gpt-4o-search-previe,OpenAI: GPT-4o Search Preview,C,0.00000250,0.00001000,-,128000,16384,JS,verified,openrouter,2025-03-12,GPT-4o Search Previewis a specialized model for web search in Chat Completions. It is trained to und,-,-,-
google/gemma-3-27b-it:free,gemma-3-27b-it:free,Google: Gemma 3 27B (free),C,-,-,-,131072,32768,JSTV,verified,openrouter,2025-03-11,"Gemma 3 introduces multimodality, supporting vision-language input and text outputs. It handles cont",-,-,-
google/gemma-3-27b-it,gemma-3-27b-it,Google: Gemma 3 27B,C,0.00000004,0.00000006,-,131072,32768,JSTV,verified,openrouter,2025-03-11,"Gemma 3 introduces multimodality, supporting vision-language input and text outputs. It handles cont",-,-,-
thedrummer/skyfall-36b-v2,skyfall-36b-v2,TheDrummer: Skyfall 36B V2,C,0.00000055,0.00000080,-,32768,32768,-,verified,openrouter,2025-03-10,"Skyfall 36B v2 is an enhanced iteration of Mistral Small 2501, specifically fine-tuned for improved ",-,-,-
microsoft/phi-4-multimodal-instruct,phi-4-multimodal-ins,Microsoft: Phi 4 Multimodal Instruct,C,0.00000005,0.00000010,-,131072,32768,JV,verified,openrouter,2025-03-07,Phi-4 Multimodal Instruct is a versatile 5.6B parameter foundation model that combines advanced reas,-,-,-
perplexity/sonar-reasoning-pro,sonar-reasoning-pro,Perplexity: Sonar Reasoning Pro,C,0.00000200,0.00000800,-,128000,32000,KV,verified,openrouter,2025-03-06,Note: Sonar Pro pricing includes Perplexity search pricing. See [details here](https://docs.perplexi,-,-,-
perplexity/sonar-pro,sonar-pro,Perplexity: Sonar Pro,C,0.00000300,0.00001500,-,200000,8000,V,verified,openrouter,2025-03-06,Note: Sonar Pro pricing includes Perplexity search pricing. See [details here](https://docs.perplexi,-,-,-
perplexity/sonar-deep-research,sonar-deep-research,Perplexity: Sonar Deep Research,C,0.00000200,0.00000800,-,128000,32000,K,verified,openrouter,2025-03-06,"Sonar Deep Research is a research-focused model designed for multi-step retrieval, synthesis, and re",-,-,-
qwen/qwq-32b,qwq-32b,Qwen: QwQ 32B,C,0.00000015,0.00000040,-,32768,8192,JKST,verified,openrouter,2025-03-05,"QwQ is the reasoning model of the Qwen series. Compared with conventional instruction-tuned models, ",-,-,-
google/gemini-2.0-flash-lite-001,gemini-2.0-flash-lit,Google: Gemini 2.0 Flash Lite,C,0.00000007,0.00000030,-,1048576,8192,JSTV,verified,openrouter,2025-02-25,Gemini 2.0 Flash Lite offers a significantly faster time to first token (TTFT) compared to [Gemini F,-,-,-
anthropic/claude-3.7-sonnet:thinking,claude-3.7-sonnet:th,Anthropic: Claude 3.7 Sonnet (thinking),C,0.00000300,0.00001500,-,200000,64000,KTV,verified,openrouter,2025-02-24,"Claude 3.7 Sonnet is an advanced large language model with improved reasoning, coding, and problem-s",-,-,-
anthropic/claude-3.7-sonnet,claude-3.7-sonnet,Anthropic: Claude 3.7 Sonnet,C,0.00000300,0.00001500,-,200000,64000,KTV,verified,openrouter,2025-02-24,"Claude 3.7 Sonnet is an advanced large language model with improved reasoning, coding, and problem-s",-,-,-
mistralai/mistral-saba,mistral-saba,Mistral: Saba,C,0.00000020,0.00000060,-,32768,8192,JST,verified,openrouter,2025-02-17,Mistral Saba is a 24B-parameter language model specifically designed for the Middle East and South A,-,-,-
meta-llama/llama-guard-3-8b,llama-guard-3-8b,Llama Guard 3 8B,C,0.00000002,0.00000006,-,131072,32768,J,verified,openrouter,2025-02-12,"Llama Guard 3 is a Llama-3.1-8B pretrained model, fine-tuned for content safety classification. Simi",-,-,-
openai/o3-mini-high,o3-mini-high,OpenAI: o3 Mini High,C,0.00000110,0.00000440,-,200000,100000,JST,verified,openrouter,2025-02-12,OpenAI o3-mini-high is the same model as [o3-mini](/openai/o3-mini) with reasoning_effort set to hig,-,-,-
google/gemini-2.0-flash-001,gemini-2.0-flash-001,Google: Gemini 2.0 Flash,C,0.00000010,0.00000040,-,1048576,8192,JSTV,verified,openrouter,2025-02-05,Gemini Flash 2.0 offers a significantly faster time to first token (TTFT) compared to [Gemini Flash ,-,-,-
qwen/qwen-vl-plus,qwen-vl-plus,Qwen: Qwen VL Plus,C,0.00000021,0.00000063,-,7500,1500,JV,verified,openrouter,2025-02-04,Qwen's Enhanced Large Visual Language Model. Significantly upgraded for detailed recognition capabil,-,-,-
aion-labs/aion-1.0,aion-1.0,AionLabs: Aion-1.0,C,0.00000400,0.00000800,-,131072,32768,K,verified,openrouter,2025-02-04,"Aion-1.0 is a multi-model system designed for high performance across various tasks, including reaso",-,-,-
aion-labs/aion-1.0-mini,aion-1.0-mini,AionLabs: Aion-1.0-Mini,C,0.00000070,0.00000140,-,131072,32768,K,verified,openrouter,2025-02-04,"Aion-1.0-Mini 32B parameter model is a distilled version of the DeepSeek-R1 model, designed for stro",-,-,-
aion-labs/aion-rp-llama-3.1-8b,aion-rp-llama-3.1-8b,AionLabs: Aion-RP 1.0 (8B),C,0.00000080,0.00000160,-,32768,32768,-,verified,openrouter,2025-02-04,Aion-RP-Llama-3.1-8B ranks the highest in the character evaluation portion of the RPBench-Auto bench,-,-,-
qwen/qwen-vl-max,qwen-vl-max,Qwen: Qwen VL Max,C,0.00000080,0.00000320,-,131072,8192,JTV,verified,openrouter,2025-02-01,Qwen VL Max is a visual understanding model with 7500 tokens context length. It excels in delivering,-,-,-
qwen/qwen-turbo,qwen-turbo,Qwen: Qwen-Turbo,C,0.00000005,0.00000020,-,1000000,8192,JT,verified,openrouter,2025-02-01,"Qwen-Turbo, based on Qwen2.5, is a 1M context model that provides fast speed and low cost, suitable ",-,-,-
qwen/qwen2.5-vl-72b-instruct,qwen2.5-vl-72b-instr,Qwen: Qwen2.5 VL 72B Instruct,C,0.00000007,0.00000026,-,32768,32768,JSV,verified,openrouter,2025-02-01,"Qwen2.5-VL is proficient in recognizing common objects such as flowers, birds, fish, and insects. It",-,-,-
qwen/qwen-plus,qwen-plus,Qwen: Qwen-Plus,C,0.00000040,0.00000120,-,131072,8192,JT,verified,openrouter,2025-02-01,"Qwen-Plus, based on the Qwen2.5 foundation model, is a 131K context model with a balanced performanc",-,-,-
qwen/qwen-max,qwen-max,Qwen: Qwen-Max ,C,0.00000160,0.00000640,-,32768,8192,JT,verified,openrouter,2025-02-01,"Qwen-Max, based on Qwen2.5, provides the best inference performance among [Qwen models](/qwen), espe",-,-,-
openai/o3-mini,o3-mini,OpenAI: o3 Mini,C,0.00000110,0.00000440,-,200000,100000,JST,verified,openrouter,2025-01-31,"OpenAI o3-mini is a cost-efficient language model optimized for STEM reasoning tasks, particularly e",-,-,-
mistralai/mistral-small-24b-instruct-2501,mistral-small-24b-in,Mistral: Mistral Small 3,C,0.00000003,0.00000011,-,32768,32768,JST,verified,openrouter,2025-01-30,Mistral Small 3 is a 24B-parameter language model optimized for low-latency performance across commo,-,-,-
deepseek/deepseek-r1-distill-qwen-32b,deepseek-r1-distill-,DeepSeek: R1 Distill Qwen 32B,C,0.00000027,0.00000027,-,131072,32768,JKS,verified,openrouter,2025-01-29,DeepSeek R1 Distill Qwen 32B is a distilled large language model based on [Qwen 2.5 32B](https://hug,-,-,-
deepseek/deepseek-r1-distill-qwen-14b,deepseek-r1-distill-,DeepSeek: R1 Distill Qwen 14B,C,0.00000015,0.00000015,-,32768,16384,JKS,verified,openrouter,2025-01-29,DeepSeek R1 Distill Qwen 14B is a distilled large language model based on [Qwen 2.5 14B](https://hug,-,-,-
perplexity/sonar-reasoning,sonar-reasoning,Perplexity: Sonar Reasoning,C,0.00000100,0.00000500,-,127000,31750,K,verified,openrouter,2025-01-28,Sonar Reasoning is a reasoning model provided by Perplexity based on [DeepSeek R1](/deepseek/deepsee,-,-,-
perplexity/sonar,sonar,Perplexity: Sonar,C,0.00000100,0.00000100,-,127072,31768,V,verified,openrouter,2025-01-27,"Sonar is lightweight, affordable, fast, and simple to use — now featuring citations and the ability ",-,-,-
deepseek/deepseek-r1-distill-llama-70b,deepseek-r1-distill-,DeepSeek: R1 Distill Llama 70B,C,0.00000003,0.00000011,-,131072,131072,JKST,verified,openrouter,2025-01-23,DeepSeek R1 Distill Llama 70B is a distilled large language model based on [Llama-3.3-70B-Instruct](,-,-,-
deepseek/deepseek-r1,deepseek-r1,DeepSeek: R1,C,0.00000030,0.00000120,-,163840,40960,JKST,verified,openrouter,2025-01-20,"DeepSeek R1 is here: Performance on par with [OpenAI o1](/openai/o1), but open-sourced and with full",-,-,-
minimax/minimax-01,minimax-01,MiniMax: MiniMax-01,C,0.00000020,0.00000110,-,1000192,1000192,V,verified,openrouter,2025-01-14,MiniMax-01 is a combines MiniMax-Text-01 for text generation and MiniMax-VL-01 for image understandi,-,-,-
microsoft/phi-4,phi-4,Microsoft: Phi 4,C,0.00000006,0.00000014,-,16384,4096,JS,verified,openrouter,2025-01-09,[Microsoft Research](/microsoft) Phi-4 is designed to perform well in complex reasoning tasks and ca,-,-,-
sao10k/l3.1-70b-hanami-x1,l3.1-70b-hanami-x1,Sao10K: Llama 3.1 70B Hanami x1,C,0.00000300,0.00000300,-,16000,4000,-,verified,openrouter,2025-01-07,This is [Sao10K](/sao10k)'s experiment over [Euryale v2.2](/sao10k/l3.1-euryale-70b).,-,-,-
deepseek/deepseek-chat,deepseek-chat,DeepSeek: DeepSeek V3,C,0.00000030,0.00000120,-,163840,163840,JST,verified,openrouter,2024-12-26,"DeepSeek-V3 is the latest model from the DeepSeek team, building upon the instruction following and ",-,-,-
sao10k/l3.3-euryale-70b,l3.3-euryale-70b,Sao10K: Llama 3.3 Euryale 70B,C,0.00000065,0.00000075,-,131072,16384,JS,verified,openrouter,2024-12-18,Euryale L3.3 70B is a model focused on creative roleplay from [Sao10k](https://ko-fi.com/sao10k). It,-,-,-
openai/o1,o1,OpenAI: o1,C,0.00001500,0.00006000,-,200000,100000,JSTV,verified,openrouter,2024-12-17,"The latest and strongest model family from OpenAI, o1 is designed to spend more time thinking before",-,-,-
cohere/command-r7b-12-2024,command-r7b-12-2024,Cohere: Command R7B (12-2024),C,0.00000004,0.00000015,-,128000,4000,JS,verified,openrouter,2024-12-13,"Command R7B (12-2024) is a small, fast update of the Command R+ model, delivered in December 2024. I",-,-,-
google/gemini-2.0-flash-exp:free,gemini-2.0-flash-exp,Google: Gemini 2.0 Flash Experimental (free),C,-,-,-,1048576,8192,JTV,verified,openrouter,2024-12-11,Gemini Flash 2.0 offers a significantly faster time to first token (TTFT) compared to [Gemini Flash ,-,-,-
meta-llama/llama-3.3-70b-instruct:free,llama-3.3-70b-instru,Meta: Llama 3.3 70B Instruct (free),C,-,-,-,131072,32768,T,verified,openrouter,2024-12-06,The Meta Llama 3.3 multilingual large language model (LLM) is a pretrained and instruction tuned gen,-,-,-
meta-llama/llama-3.3-70b-instruct,llama-3.3-70b-instru,Meta: Llama 3.3 70B Instruct,C,0.00000010,0.00000032,-,131072,16384,JST,verified,openrouter,2024-12-06,The Meta Llama 3.3 multilingual large language model (LLM) is a pretrained and instruction tuned gen,-,-,-
amazon/nova-lite-v1,nova-lite-v1,Amazon: Nova Lite 1.0,C,0.00000006,0.00000024,-,300000,5120,TV,verified,openrouter,2024-12-05,Amazon Nova Lite 1.0 is a very low-cost multimodal model from Amazon that focused on fast processing,-,-,-
amazon/nova-micro-v1,nova-micro-v1,Amazon: Nova Micro 1.0,C,0.00000004,0.00000014,-,128000,5120,T,verified,openrouter,2024-12-05,Amazon Nova Micro 1.0 is a text-only model that delivers the lowest latency responses in the Amazon ,-,-,-
amazon/nova-pro-v1,nova-pro-v1,Amazon: Nova Pro 1.0,C,0.00000080,0.00000320,-,300000,5120,TV,verified,openrouter,2024-12-05,Amazon Nova Pro 1.0 is a capable multimodal model from Amazon focused on providing a combination of ,-,-,-
openai/gpt-4o-2024-11-20,gpt-4o-2024-11-20,OpenAI: GPT-4o (2024-11-20),C,0.00000250,0.00001000,-,128000,16384,JSTV,verified,openrouter,2024-11-20,"The 2024-11-20 version of GPT-4o offers a leveled-up creative writing ability with more natural, eng",-,-,-
mistralai/mistral-large-2411,mistral-large-2411,Mistral Large 2411,C,0.00000200,0.00000600,-,131072,32768,JST,verified,openrouter,2024-11-18,Mistral Large 2 2411 is an update of [Mistral Large 2](/mistralai/mistral-large) released together w,-,-,-
mistralai/mistral-large-2407,mistral-large-2407,Mistral Large 2407,C,0.00000200,0.00000600,-,131072,32768,JST,verified,openrouter,2024-11-18,"This is Mistral AI's flagship model, Mistral Large 2 (version mistral-large-2407). It's a proprietar",-,-,-
mistralai/pixtral-large-2411,pixtral-large-2411,Mistral: Pixtral Large 2411,C,0.00000200,0.00000600,-,131072,32768,JSTV,verified,openrouter,2024-11-18,"Pixtral Large is a 124B parameter, open-weight, multimodal model built on top of [Mistral Large 2](/",-,-,-
qwen/qwen-2.5-coder-32b-instruct,qwen-2.5-coder-32b-i,Qwen2.5 Coder 32B Instruct,C,0.00000003,0.00000011,-,32768,32768,JS,verified,openrouter,2024-11-11,Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as Co,-,-,-
raifle/sorcererlm-8x22b,sorcererlm-8x22b,SorcererLM 8x22B,C,0.00000450,0.00000450,-,16000,4000,-,verified,openrouter,2024-11-08,"SorcererLM is an advanced RP and storytelling model, built as a Low-rank 16-bit LoRA fine-tuned on [",-,-,-
thedrummer/unslopnemo-12b,unslopnemo-12b,TheDrummer: UnslopNemo 12B,C,0.00000040,0.00000040,-,32768,8192,JST,verified,openrouter,2024-11-08,"UnslopNemo v4.1 is the latest addition from the creator of Rocinante, designed for adventure writing",-,-,-
anthropic/claude-3.5-haiku-20241022,claude-3.5-haiku-202,Anthropic: Claude 3.5 Haiku (2024-10-22),C,0.00000080,0.00000400,-,200000,8192,TV,verified,openrouter,2024-11-03,"Claude 3.5 Haiku features enhancements across all skill sets including coding, tool use, and reasoni",-,-,-
anthropic/claude-3.5-haiku,claude-3.5-haiku,Anthropic: Claude 3.5 Haiku,C,0.00000080,0.00000400,-,200000,8192,TV,verified,openrouter,2024-11-03,"Claude 3.5 Haiku features offers enhanced capabilities in speed, coding accuracy, and tool use. Engi",-,-,-
anthracite-org/magnum-v4-72b,magnum-v4-72b,Magnum v4 72B,C,0.00000300,0.00000500,-,16384,2048,J,verified,openrouter,2024-10-21,"This is a series of models designed to replicate the prose quality of the Claude 3 models, specifica",-,-,-
anthropic/claude-3.5-sonnet,claude-3.5-sonnet,Anthropic: Claude 3.5 Sonnet,C,0.00000600,0.00003000,-,200000,8192,TV,verified,openrouter,2024-10-21,"New Claude 3.5 Sonnet delivers better-than-Opus capabilities, faster-than-Sonnet speeds, at the same",-,-,-
mistralai/ministral-8b,ministral-8b,Mistral: Ministral 8B,C,0.00000010,0.00000010,-,131072,32768,JST,verified,openrouter,2024-10-16,Ministral 8B is an 8B parameter model featuring a unique interleaved sliding-window attention patter,-,-,-
mistralai/ministral-3b,ministral-3b,Mistral: Ministral 3B,C,0.00000004,0.00000004,-,131072,32768,JST,verified,openrouter,2024-10-16,Ministral 3B is a 3B parameter model optimized for on-device and edge computing. It excels in knowle,-,-,-
qwen/qwen-2.5-7b-instruct,qwen-2.5-7b-instruct,Qwen: Qwen2.5 7B Instruct,C,0.00000004,0.00000010,-,32768,8192,-,verified,openrouter,2024-10-15,Qwen2.5 7B is the latest series of Qwen large language models. Qwen2.5 brings the following improvem,-,-,-
nvidia/llama-3.1-nemotron-70b-instruct,llama-3.1-nemotron-7,NVIDIA: Llama 3.1 Nemotron 70B Instruct,C,0.00000120,0.00000120,-,131072,16384,JT,verified,openrouter,2024-10-14,NVIDIA's Llama 3.1 Nemotron 70B is a language model designed for generating precise and useful respo,-,-,-
inflection/inflection-3-pi,inflection-3-pi,Inflection: Inflection 3 Pi,C,0.00000250,0.00001000,-,8000,1024,-,verified,openrouter,2024-10-10,"Inflection 3 Pi powers Inflection's [Pi](https://pi.ai) chatbot, including backstory, emotional inte",-,-,-
inflection/inflection-3-productivity,inflection-3-product,Inflection: Inflection 3 Productivity,C,0.00000250,0.00001000,-,8000,1024,-,verified,openrouter,2024-10-10,Inflection 3 Productivity is optimized for following instructions. It is better for tasks requiring ,-,-,-
thedrummer/rocinante-12b,rocinante-12b,TheDrummer: Rocinante 12B,C,0.00000017,0.00000043,-,32768,8192,JST,verified,openrouter,2024-09-29,"Rocinante 12B is designed for engaging storytelling and rich prose.

Early testers have reported:
- ",-,-,-
meta-llama/llama-3.2-3b-instruct:free,llama-3.2-3b-instruc,Meta: Llama 3.2 3B Instruct (free),C,-,-,-,131072,32768,-,verified,openrouter,2024-09-24,"Llama 3.2 3B is a 3-billion-parameter multilingual large language model, optimized for advanced natu",-,-,-
meta-llama/llama-3.2-3b-instruct,llama-3.2-3b-instruc,Meta: Llama 3.2 3B Instruct,C,0.00000002,0.00000002,-,131072,16384,JT,verified,openrouter,2024-09-24,"Llama 3.2 3B is a 3-billion-parameter multilingual large language model, optimized for advanced natu",-,-,-
meta-llama/llama-3.2-1b-instruct,llama-3.2-1b-instruc,Meta: Llama 3.2 1B Instruct,C,0.00000003,0.00000020,-,60000,15000,-,verified,openrouter,2024-09-24,Llama 3.2 1B is a 1-billion-parameter language model focused on efficiently performing natural langu,-,-,-
meta-llama/llama-3.2-90b-vision-instruct,llama-3.2-90b-vision,Meta: Llama 3.2 90B Vision Instruct,C,0.00000035,0.00000040,-,32768,16384,JV,verified,openrouter,2024-09-24,"The Llama 90B Vision model is a top-tier, 90-billion-parameter multimodal model designed for the mos",-,-,-
meta-llama/llama-3.2-11b-vision-instruct,llama-3.2-11b-vision,Meta: Llama 3.2 11B Vision Instruct,C,0.00000005,0.00000005,-,131072,16384,JV,verified,openrouter,2024-09-24,"Llama 3.2 11B Vision is a multimodal model with 11 billion parameters, designed to handle tasks comb",-,-,-
qwen/qwen-2.5-72b-instruct,qwen-2.5-72b-instruc,Qwen2.5 72B Instruct,C,0.00000012,0.00000039,-,32768,16384,JST,verified,openrouter,2024-09-18,Qwen2.5 72B is the latest series of Qwen large language models. Qwen2.5 brings the following improve,-,-,-
neversleep/llama-3.1-lumimaid-8b,llama-3.1-lumimaid-8,NeverSleep: Lumimaid v0.2 8B,C,0.00000009,0.00000060,-,32768,8192,JS,verified,openrouter,2024-09-14,"Lumimaid v0.2 8B is a finetune of [Llama 3.1 8B](/models/meta-llama/llama-3.1-8b-instruct) with a ""H",-,-,-
mistralai/pixtral-12b,pixtral-12b,Mistral: Pixtral 12B,C,0.00000010,0.00000010,-,32768,8192,JSTV,verified,openrouter,2024-09-09,"The first multi-modal, text+image-to-text model from Mistral AI. Its weights were launched via torre",-,-,-
cohere/command-r-08-2024,command-r-08-2024,Cohere: Command R (08-2024),C,0.00000015,0.00000060,-,128000,4000,JST,verified,openrouter,2024-08-29,command-r-08-2024 is an update of the [Command R](/models/cohere/command-r) with improved performanc,-,-,-
cohere/command-r-plus-08-2024,command-r-plus-08-20,Cohere: Command R+ (08-2024),C,0.00000250,0.00001000,-,128000,4000,JST,verified,openrouter,2024-08-29,command-r-plus-08-2024 is an update of the [Command R+](/models/cohere/command-r-plus) with roughly ,-,-,-
sao10k/l3.1-euryale-70b,l3.1-euryale-70b,Sao10K: Llama 3.1 Euryale 70B v2.2,C,0.00000065,0.00000075,-,32768,8192,JST,verified,openrouter,2024-08-27,Euryale L3.1 70B v2.2 is a model focused on creative roleplay from [Sao10k](https://ko-fi.com/sao10k,-,-,-
qwen/qwen-2.5-vl-7b-instruct:free,qwen-2.5-vl-7b-instr,Qwen: Qwen2.5-VL 7B Instruct (free),C,-,-,-,32768,8192,V,verified,openrouter,2024-08-27,"Qwen2.5 VL 7B is a multimodal LLM from the Qwen Team with the following key enhancements:

- SoTA un",-,-,-
qwen/qwen-2.5-vl-7b-instruct,qwen-2.5-vl-7b-instr,Qwen: Qwen2.5-VL 7B Instruct,C,0.00000020,0.00000020,-,32768,8192,V,verified,openrouter,2024-08-27,"Qwen2.5 VL 7B is a multimodal LLM from the Qwen Team with the following key enhancements:

- SoTA un",-,-,-
microsoft/phi-3.5-mini-128k-instruct,phi-3.5-mini-128k-in,Microsoft: Phi-3.5 Mini 128K Instruct,C,0.00000010,0.00000010,-,128000,32000,T,verified,openrouter,2024-08-20,"Phi-3.5 models are lightweight, state-of-the-art open models. These models were trained with Phi-3 d",-,-,-
nousresearch/hermes-3-llama-3.1-70b,hermes-3-llama-3.1-7,Nous: Hermes 3 70B Instruct,C,0.00000030,0.00000030,-,65536,16384,JS,verified,openrouter,2024-08-17,Hermes 3 is a generalist language model with many improvements over [Hermes 2](/models/nousresearch/,-,-,-
nousresearch/hermes-3-llama-3.1-405b:free,hermes-3-llama-3.1-4,Nous: Hermes 3 405B Instruct (free),C,-,-,-,131072,32768,-,verified,openrouter,2024-08-15,"Hermes 3 is a generalist language model with many improvements over Hermes 2, including advanced age",-,-,-
nousresearch/hermes-3-llama-3.1-405b,hermes-3-llama-3.1-4,Nous: Hermes 3 405B Instruct,C,0.00000100,0.00000100,-,131072,16384,J,verified,openrouter,2024-08-15,"Hermes 3 is a generalist language model with many improvements over Hermes 2, including advanced age",-,-,-
openai/chatgpt-4o-latest,chatgpt-4o-latest,OpenAI: ChatGPT-4o,C,0.00000500,0.00001500,-,128000,16384,JSV,verified,openrouter,2024-08-13,OpenAI ChatGPT 4o is continually updated by OpenAI to point to the current version of GPT-4o used by,-,-,-
sao10k/l3-lunaris-8b,l3-lunaris-8b,Sao10K: Llama 3 8B Lunaris,C,0.00000004,0.00000005,-,8192,2048,JS,verified,openrouter,2024-08-12,Lunaris 8B is a versatile generalist and roleplaying model based on Llama 3. It's a strategic merge ,-,-,-
openai/gpt-4o-2024-08-06,gpt-4o-2024-08-06,OpenAI: GPT-4o (2024-08-06),C,0.00000250,0.00001000,-,128000,16384,JSTV,verified,openrouter,2024-08-05,"The 2024-08-06 version of GPT-4o offers improved performance in structured outputs, with the ability",-,-,-
meta-llama/llama-3.1-405b,llama-3.1-405b,Meta: Llama 3.1 405B (base),C,0.00000400,0.00000400,-,32768,32768,-,verified,openrouter,2024-08-01,Meta's latest class of model (Llama 3.1) launched with a variety of sizes & flavors. This is the bas,-,-,-
meta-llama/llama-3.1-8b-instruct,llama-3.1-8b-instruc,Meta: Llama 3.1 8B Instruct,C,0.00000002,0.00000003,-,131072,16384,JST,verified,openrouter,2024-07-22,Meta's latest class of model (Llama 3.1) launched with a variety of sizes & flavors. This 8B instruc,-,-,-
meta-llama/llama-3.1-405b-instruct:free,llama-3.1-405b-instr,Meta: Llama 3.1 405B Instruct (free),C,-,-,-,131072,32768,-,verified,openrouter,2024-07-22,The highly anticipated 400B class of Llama3 is here! Clocking in at 128k context with impressive eva,-,-,-
meta-llama/llama-3.1-405b-instruct,llama-3.1-405b-instr,Meta: Llama 3.1 405B Instruct,C,0.00000350,0.00000350,-,10000,2500,JST,verified,openrouter,2024-07-22,The highly anticipated 400B class of Llama3 is here! Clocking in at 128k context with impressive eva,-,-,-
meta-llama/llama-3.1-70b-instruct,llama-3.1-70b-instru,Meta: Llama 3.1 70B Instruct,C,0.00000040,0.00000040,-,131072,32768,JT,verified,openrouter,2024-07-22,Meta's latest class of model (Llama 3.1) launched with a variety of sizes & flavors. This 70B instru,-,-,-
mistralai/mistral-nemo,mistral-nemo,Mistral: Mistral Nemo,C,0.00000002,0.00000004,-,131072,16384,JST,verified,openrouter,2024-07-18,A 12B parameter model with a 128k token context length built by Mistral in collaboration with NVIDIA,-,-,-
openai/gpt-4o-mini-2024-07-18,gpt-4o-mini-2024-07-,OpenAI: GPT-4o-mini (2024-07-18),C,0.00000015,0.00000060,-,128000,16384,JSTV,verified,openrouter,2024-07-17,"GPT-4o mini is OpenAI's newest model after [GPT-4 Omni](/models/openai/gpt-4o), supporting both text",-,-,-
openai/gpt-4o-mini,gpt-4o-mini,OpenAI: GPT-4o-mini,C,0.00000015,0.00000060,-,128000,16384,JSTV,verified,openrouter,2024-07-17,"GPT-4o mini is OpenAI's newest model after [GPT-4 Omni](/models/openai/gpt-4o), supporting both text",-,-,-
google/gemma-2-27b-it,gemma-2-27b-it,Google: Gemma 2 27B,C,0.00000065,0.00000065,-,8192,2048,JS,verified,openrouter,2024-07-12,Gemma 2 27B by Google is an open model built from the same research and technology used to create th,-,-,-
google/gemma-2-9b-it,gemma-2-9b-it,Google: Gemma 2 9B,C,0.00000003,0.00000009,-,8192,2048,-,verified,openrouter,2024-06-27,"Gemma 2 9B by Google is an advanced, open-source language model that sets a new standard for efficie",-,-,-
sao10k/l3-euryale-70b,l3-euryale-70b,Sao10k: Llama 3 Euryale 70B v2.1,C,0.00000148,0.00000148,-,8192,8192,T,verified,openrouter,2024-06-17,"Euryale 70B v2.1 is a model focused on creative roleplay from [Sao10k](https://ko-fi.com/sao10k).

-",-,-,-
nousresearch/hermes-2-pro-llama-3-8b,hermes-2-pro-llama-3,NousResearch: Hermes 2 Pro - Llama-3 8B,C,0.00000002,0.00000008,-,8192,2048,JS,verified,openrouter,2024-05-26,"Hermes 2 Pro is an upgraded, retrained version of Nous Hermes 2, consisting of an updated and cleane",-,-,-
mistralai/mistral-7b-instruct:free,mistral-7b-instruct:,Mistral: Mistral 7B Instruct (free),C,-,-,-,32768,16384,JT,verified,openrouter,2024-05-26,"A high-performing, industry-standard 7.3B parameter model, with optimizations for speed and context ",-,-,-
mistralai/mistral-7b-instruct,mistral-7b-instruct,Mistral: Mistral 7B Instruct,C,0.00000003,0.00000005,-,32768,16384,JT,verified,openrouter,2024-05-26,"A high-performing, industry-standard 7.3B parameter model, with optimizations for speed and context ",-,-,-
mistralai/mistral-7b-instruct-v0.3,mistral-7b-instruct-,Mistral: Mistral 7B Instruct v0.3,C,0.00000020,0.00000020,-,32768,4096,-,verified,openrouter,2024-05-26,"A high-performing, industry-standard 7.3B parameter model, with optimizations for speed and context ",-,-,-
microsoft/phi-3-mini-128k-instruct,phi-3-mini-128k-inst,Microsoft: Phi-3 Mini 128K Instruct,C,0.00000010,0.00000010,-,128000,32000,T,verified,openrouter,2024-05-25,"Phi-3 Mini is a powerful 3.8B parameter model designed for advanced language understanding, reasonin",-,-,-
microsoft/phi-3-medium-128k-instruct,phi-3-medium-128k-in,Microsoft: Phi-3 Medium 128K Instruct,C,0.00000100,0.00000100,-,128000,32000,T,verified,openrouter,2024-05-23,Phi-3 128K Medium is a powerful 14-billion parameter model designed for advanced language understand,-,-,-
meta-llama/llama-guard-2-8b,llama-guard-2-8b,Meta: LlamaGuard 2 8B,C,0.00000020,0.00000020,-,8192,2048,-,verified,openrouter,2024-05-12,"This safeguard model has 8B parameters and is based on the Llama 3 family. Just like is predecessor,",-,-,-
openai/gpt-4o-2024-05-13,gpt-4o-2024-05-13,OpenAI: GPT-4o (2024-05-13),C,0.00000500,0.00001500,-,128000,4096,JSTV,verified,openrouter,2024-05-12,"GPT-4o (""o"" for ""omni"") is OpenAI's latest AI model, supporting both text and image inputs with text",-,-,-
openai/gpt-4o,gpt-4o,OpenAI: GPT-4o,C,0.00000250,0.00001000,-,128000,16384,JSTV,verified,openrouter,2024-05-12,"GPT-4o (""o"" for ""omni"") is OpenAI's latest AI model, supporting both text and image inputs with text",-,-,-
openai/gpt-4o:extended,gpt-4o:extended,OpenAI: GPT-4o (extended),C,0.00000600,0.00001800,-,128000,64000,JSTV,verified,openrouter,2024-05-12,"GPT-4o (""o"" for ""omni"") is OpenAI's latest AI model, supporting both text and image inputs with text",-,-,-
meta-llama/llama-3-70b-instruct,llama-3-70b-instruct,Meta: Llama 3 70B Instruct,C,0.00000030,0.00000040,-,8192,16384,JST,verified,openrouter,2024-04-17,Meta's latest class of model (Llama 3) launched with a variety of sizes & flavors. This 70B instruct,-,-,-
meta-llama/llama-3-8b-instruct,llama-3-8b-instruct,Meta: Llama 3 8B Instruct,C,0.00000003,0.00000006,-,8192,16384,JT,verified,openrouter,2024-04-17,Meta's latest class of model (Llama 3) launched with a variety of sizes & flavors. This 8B instruct-,-,-,-
mistralai/mixtral-8x22b-instruct,mixtral-8x22b-instru,Mistral: Mixtral 8x22B Instruct,C,0.00000200,0.00000600,-,65536,16384,JST,verified,openrouter,2024-04-16,Mistral's official instruct fine-tuned version of [Mixtral 8x22B](/models/mistralai/mixtral-8x22b). ,-,-,-
microsoft/wizardlm-2-8x22b,wizardlm-2-8x22b,WizardLM-2 8x22B,C,0.00000048,0.00000048,-,65536,16384,J,verified,openrouter,2024-04-15,WizardLM-2 8x22B is Microsoft AI's most advanced Wizard model. It demonstrates highly competitive pe,-,-,-
openai/gpt-4-turbo,gpt-4-turbo,OpenAI: GPT-4 Turbo,C,0.00001000,0.00003000,-,128000,4096,JSTV,verified,openrouter,2024-04-08,The latest GPT-4 Turbo model with vision capabilities. Vision requests can now use JSON mode and fun,-,-,-
anthropic/claude-3-haiku,claude-3-haiku,Anthropic: Claude 3 Haiku,C,0.00000025,0.00000125,-,200000,4096,TV,verified,openrouter,2024-03-12,"Claude 3 Haiku is Anthropic's fastest and most compact model for
near-instant responsiveness. Quick ",-,-,-
anthropic/claude-3-opus,claude-3-opus,Anthropic: Claude 3 Opus,C,0.00001500,0.00007500,-,200000,4096,TV,verified,openrouter,2024-03-04,Claude 3 Opus is Anthropic's most powerful model for highly complex tasks. It boasts top-level perfo,-,-,-
mistralai/mistral-large,mistral-large,Mistral Large,C,0.00000200,0.00000600,-,128000,32000,JST,verified,openrouter,2024-02-25,"This is Mistral AI's flagship model, Mistral Large 2 (version `mistral-large-2407`). It's a propriet",-,-,-
openai/gpt-3.5-turbo-0613,gpt-3.5-turbo-0613,OpenAI: GPT-3.5 Turbo (older v0613),C,0.00000100,0.00000200,-,4095,4096,JST,verified,openrouter,2024-01-24,"GPT-3.5 Turbo is OpenAI's fastest model. It can understand and generate natural language or code, an",-,-,-
openai/gpt-4-turbo-preview,gpt-4-turbo-preview,OpenAI: GPT-4 Turbo Preview,C,0.00001000,0.00003000,-,128000,4096,JST,verified,openrouter,2024-01-24,"The preview GPT-4 model with improved instruction following, JSON mode, reproducible outputs, parall",-,-,-
mistralai/mistral-tiny,mistral-tiny,Mistral Tiny,C,0.00000025,0.00000025,-,32768,8192,JST,verified,openrouter,2024-01-09,Note: This model is being deprecated. Recommended replacement is the newer [Ministral 8B](/mistral/m,-,-,-
mistralai/mistral-7b-instruct-v0.2,mistral-7b-instruct-,Mistral: Mistral 7B Instruct v0.2,C,0.00000020,0.00000020,-,32768,8192,-,verified,openrouter,2023-12-27,"A high-performing, industry-standard 7.3B parameter model, with optimizations for speed and context ",-,-,-
mistralai/mixtral-8x7b-instruct,mixtral-8x7b-instruc,Mistral: Mixtral 8x7B Instruct,C,0.00000054,0.00000054,-,32768,16384,JT,verified,openrouter,2023-12-09,"Mixtral 8x7B Instruct is a pretrained generative Sparse Mixture of Experts, by Mistral AI, for chat ",-,-,-
neversleep/noromaid-20b,noromaid-20b,Noromaid 20B,C,0.00000100,0.00000175,-,4096,1024,JS,verified,openrouter,2023-11-25,"A collab between IkariDev and Undi. This merge is suitable for RP, ERP, and general knowledge.

#mer",-,-,-
alpindale/goliath-120b,goliath-120b,Goliath 120B,C,0.00000600,0.00000800,-,6144,1024,J,verified,openrouter,2023-11-09,A large LLM created by combining two fine-tuned Llama 70B models into one 120B model. Combines Xwin ,-,-,-
openrouter/auto,auto,Auto Router,C,-,-,-,2000000,500000,-,verified,openrouter,2023-11-07,"Your prompt will be processed by a meta-model and routed to one of dozens of models (see below), opt",-,-,-
openai/gpt-4-1106-preview,gpt-4-1106-preview,OpenAI: GPT-4 Turbo (older v1106),C,0.00001000,0.00003000,-,128000,4096,JST,verified,openrouter,2023-11-05,The latest GPT-4 Turbo model with vision capabilities. Vision requests can now use JSON mode and fun,-,-,-
openai/gpt-3.5-turbo-instruct,gpt-3.5-turbo-instru,OpenAI: GPT-3.5 Turbo Instruct,C,0.00000150,0.00000200,-,4095,4096,JS,verified,openrouter,2023-09-27,This model is a variant of GPT-3.5 Turbo tuned for instructional prompts and omitting chat-related o,-,-,-
mistralai/mistral-7b-instruct-v0.1,mistral-7b-instruct-,Mistral: Mistral 7B Instruct v0.1,C,0.00000011,0.00000019,-,2824,1024,-,verified,openrouter,2023-09-27,"A 7.3B parameter model that outperforms Llama 2 13B on all benchmarks, with optimizations for speed ",-,-,-
openai/gpt-3.5-turbo-16k,gpt-3.5-turbo-16k,OpenAI: GPT-3.5 Turbo 16k,C,0.00000300,0.00000400,-,16385,4096,JST,verified,openrouter,2023-08-27,"This model offers four times the context length of gpt-3.5-turbo, allowing it to support approximate",-,-,-
mancer/weaver,weaver,Mancer: Weaver (alpha),C,0.00000075,0.00000100,-,8000,2000,J,verified,openrouter,2023-08-01,"An attempt to recreate Claude-style verbosity, but don't expect the same level of coherence or memor",-,-,-
undi95/remm-slerp-l2-13b,remm-slerp-l2-13b,ReMM SLERP 13B,C,0.00000045,0.00000065,-,6144,1536,JS,verified,openrouter,2023-07-21,A recreation trial of the original MythoMax-L2-B13 but with updated models. #merge,-,-,-
gryphe/mythomax-l2-13b,mythomax-l2-13b,MythoMax 13B,C,0.00000006,0.00000006,-,4096,1024,JS,verified,openrouter,2023-07-01,"One of the highest performing and most popular fine-tunes of Llama 2 13B, with rich descriptions and",-,-,-
openai/gpt-4-0314,gpt-4-0314,OpenAI: GPT-4 (older v0314),C,0.00003000,0.00006000,-,8191,4096,JST,verified,openrouter,2023-05-27,"GPT-4-0314 is the first version of GPT-4 released, with a context length of 8,192 tokens, and was su",-,-,-
openai/gpt-4,gpt-4,OpenAI: GPT-4,C,0.00003000,0.00006000,-,8191,4096,JST,verified,openrouter,2023-05-27,"OpenAI's flagship model, GPT-4 is a large-scale multimodal language model capable of solving difficu",-,-,-
openai/gpt-3.5-turbo,gpt-3.5-turbo,OpenAI: GPT-3.5 Turbo,C,0.00000050,0.00000150,-,16385,4096,JST,verified,openrouter,2023-05-27,"GPT-3.5 Turbo is OpenAI's fastest model. It can understand and generate natural language or code, an",-,-,-
