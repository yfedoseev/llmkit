/* tslint:disable */
/* eslint-disable */

/* auto-generated by NAPI-RS */

/** Configuration for a single provider. */
export interface ProviderConfig {
  /** API key for the provider */
  apiKey?: string
  /** Custom base URL (optional) */
  baseUrl?: string
  /** Azure OpenAI endpoint */
  endpoint?: string
  /** Azure OpenAI deployment name */
  deployment?: string
  /** AWS region for Bedrock, or location for Vertex */
  region?: string
  /** Google Cloud project ID for Vertex */
  project?: string
  /** Location for Vertex AI */
  location?: string
  /** Cloudflare account ID */
  accountId?: string
  /** Cloudflare API token */
  apiToken?: string
  /** Vertex AI access token */
  accessToken?: string
  /** Databricks host URL */
  host?: string
  /** Databricks token */
  token?: string
  /** RunPod endpoint ID */
  endpointId?: string
  /** Model ID (for openai_compatible) */
  modelId?: string
}
/** Options for creating an LLMKitClient. */
export interface LlmKitClientOptions {
  /**
   * Provider configurations (key is provider name, value is config)
   * Supported providers: anthropic, openai, azure, bedrock, vertex, google,
   * groq, mistral, cohere, ai21, deepseek, together, fireworks, perplexity,
   * cerebras, sambanova, openrouter, ollama, huggingface, replicate,
   * cloudflare, watsonx, databricks, baseten, runpod, anyscale, deepinfra,
   * novita, hyperbolic, lm_studio, vllm, tgi, llamafile
   */
  providers?: Record<string, ProviderConfig>
  /** Default provider name */
  defaultProvider?: string
}
/** LLM Provider identifier. */
export const enum JsProvider {
  Anthropic = 'Anthropic',
  OpenAI = 'OpenAI',
  Google = 'Google',
  Mistral = 'Mistral',
  Groq = 'Groq',
  DeepSeek = 'DeepSeek',
  Cohere = 'Cohere',
  Bedrock = 'Bedrock',
  AzureOpenAI = 'AzureOpenAI',
  VertexAI = 'VertexAI',
  TogetherAI = 'TogetherAI',
  OpenRouter = 'OpenRouter',
  Cerebras = 'Cerebras',
  SambaNova = 'SambaNova',
  Fireworks = 'Fireworks',
  AI21 = 'AI21',
  HuggingFace = 'HuggingFace',
  Replicate = 'Replicate',
  Cloudflare = 'Cloudflare',
  Databricks = 'Databricks',
  Writer = 'Writer',
  Maritaca = 'Maritaca',
  Clova = 'Clova',
  Yandex = 'Yandex',
  GigaChat = 'GigaChat',
  Upstage = 'Upstage',
  SeaLion = 'SeaLion',
  Local = 'Local',
  Custom = 'Custom'
}
/** Model availability status. */
export const enum JsModelStatus {
  /** Currently recommended model. */
  Current = 'Current',
  /** Still available but superseded by newer version. */
  Legacy = 'Legacy',
  /** Scheduled for removal, not recommended for new projects. */
  Deprecated = 'Deprecated'
}
/** Model pricing (per 1M tokens in USD). */
export interface JsModelPricing {
  /** Input token price per 1M tokens. */
  inputPer1M: number
  /** Output token price per 1M tokens. */
  outputPer1M: number
  /** Cached input token price per 1M tokens (if supported). */
  cachedInputPer1M?: number
}
/** Model capabilities. */
export interface JsModelCapabilities {
  /** Maximum input context size in tokens. */
  maxContext: number
  /** Maximum output tokens. */
  maxOutput: number
  /** Supports vision/image input. */
  vision: boolean
  /** Supports tool/function calling. */
  tools: boolean
  /** Supports streaming responses. */
  streaming: boolean
  /** Supports JSON mode. */
  jsonMode: boolean
  /** Supports structured output with JSON schema enforcement. */
  structuredOutput: boolean
  /** Supports extended thinking/reasoning. */
  thinking: boolean
  /** Supports prompt caching. */
  caching: boolean
}
/** Benchmark scores (0-100 scale, higher is better). */
export interface JsModelBenchmarks {
  /** MMLU - General knowledge. */
  mmlu?: number
  /** HumanEval - Code generation. */
  humaneval?: number
  /** MATH - Mathematical reasoning. */
  math?: number
  /** GPQA Diamond - Graduate-level science. */
  gpqa?: number
  /** SWE-bench - Software engineering. */
  sweBench?: number
  /** IFEval - Instruction following. */
  ifeval?: number
  /** MMMU - Multimodal understanding. */
  mmmu?: number
  /** MGSM - Multilingual math. */
  mgsm?: number
  /** Time to first token (ms). */
  ttftMs?: number
  /** Tokens per second. */
  tokensPerSec?: number
}
/** Registry statistics. */
export interface JsRegistryStats {
  /** Total number of models in the registry. */
  totalModels: number
  /** Number of current (non-deprecated) models. */
  currentModels: number
  /** Number of providers. */
  providers: number
  /** Number of models available (API key configured). */
  availableModels: number
}
/**
 * Get model info by ID, alias, or raw ID.
 *
 * @example
 * ```typescript
 * import { getModelInfo } from 'modelsuite';
 *
 * const info = getModelInfo('claude-sonnet-4-5');
 * if (info) {
 *   console.log(`${info.name}: $${info.pricing.inputPer1m}/1M input tokens`);
 *   console.log(`Context: ${info.capabilities.maxContext} tokens`);
 * }
 * ```
 */
export declare function getModelInfo(modelId: string): JsModelInfo | null
/**
 * Get all models in the registry.
 *
 * @example
 * ```typescript
 * import { getAllModels } from 'modelsuite';
 *
 * const models = getAllModels();
 * console.log(`Registry contains ${models.length} models`);
 * ```
 */
export declare function getAllModels(): Array<JsModelInfo>
/**
 * Get all models for a specific provider.
 *
 * @example
 * ```typescript
 * import { getModelsByProvider, Provider } from 'modelsuite';
 *
 * const anthropicModels = getModelsByProvider(Provider.Anthropic);
 * for (const model of anthropicModels) {
 *   console.log(`${model.name}: ${model.description}`);
 * }
 * ```
 */
export declare function getModelsByProvider(provider: JsProvider): Array<JsModelInfo>
/**
 * Get all current (non-deprecated) models.
 *
 * @example
 * ```typescript
 * import { getCurrentModels } from 'modelsuite';
 *
 * const current = getCurrentModels();
 * console.log(`${current.length} current models available`);
 * ```
 */
export declare function getCurrentModels(): Array<JsModelInfo>
/**
 * Get models that can be used as classifiers (fast, cheap, good instruction following).
 *
 * @example
 * ```typescript
 * import { getClassifierModels } from 'modelsuite';
 *
 * const classifiers = getClassifierModels();
 * for (const model of classifiers) {
 *   console.log(`${model.name}: $${model.pricing.inputPer1m}/1M tokens`);
 * }
 * ```
 */
export declare function getClassifierModels(): Array<JsModelInfo>
/**
 * Get available models (provider API key is configured).
 *
 * @example
 * ```typescript
 * import { getAvailableModels } from 'modelsuite';
 *
 * const available = getAvailableModels();
 * console.log(`${available.length} models available with current credentials`);
 * ```
 */
export declare function getAvailableModels(): Array<JsModelInfo>
/**
 * Get models with specific capabilities.
 *
 * @param vision - Filter by vision support (null to ignore).
 * @param tools - Filter by tool calling support (null to ignore).
 * @param thinking - Filter by extended thinking support (null to ignore).
 *
 * @example
 * ```typescript
 * import { getModelsWithCapability } from 'modelsuite';
 *
 * // Get all vision models
 * const visionModels = getModelsWithCapability(true, null, null);
 *
 * // Get models with extended thinking
 * const thinkingModels = getModelsWithCapability(null, null, true);
 * ```
 */
export declare function getModelsWithCapability(vision?: boolean | undefined | null, tools?: boolean | undefined | null, thinking?: boolean | undefined | null): Array<JsModelInfo>
/**
 * Get the cheapest model that meets requirements.
 *
 * @param minContext - Minimum context window size (null for any).
 * @param needsVision - Whether vision support is required.
 * @param needsTools - Whether tool calling support is required.
 *
 * @example
 * ```typescript
 * import { getCheapestModel } from 'modelsuite';
 *
 * // Get cheapest model with at least 100k context
 * const cheapest = getCheapestModel(100000, false, true);
 * if (cheapest) {
 *   console.log(`Cheapest: ${cheapest.name} at $${cheapest.pricing.inputPer1m}/1M`);
 * }
 * ```
 */
export declare function getCheapestModel(minContext: number | undefined | null, needsVision: boolean, needsTools: boolean): JsModelInfo | null
/**
 * Check if a model supports structured output (JSON schema enforcement).
 *
 * @example
 * ```typescript
 * import { supportsStructuredOutput } from 'modelsuite';
 *
 * if (supportsStructuredOutput('gpt-4o')) {
 *   // Use structured output with JSON schema
 * } else {
 *   // Fall back to text-based parsing
 * }
 * ```
 */
export declare function supportsStructuredOutput(modelId: string): boolean
/**
 * Get registry statistics.
 *
 * @example
 * ```typescript
 * import { getRegistryStats } from 'modelsuite';
 *
 * const stats = getRegistryStats();
 * console.log(`Registry: ${stats.totalModels} models from ${stats.providers} providers`);
 * ```
 */
export declare function getRegistryStats(): JsRegistryStats
/**
 * List all providers with at least one model.
 *
 * @example
 * ```typescript
 * import { listProviders } from 'modelsuite';
 *
 * const providers = listProviders();
 * console.log(`Supported providers: ${providers.join(', ')}`);
 * ```
 */
export declare function listProviders(): Array<JsProvider>
/** Output encoding format for embeddings. */
export const enum JsEncodingFormat {
  /** Float32 array (default). */
  Float = 0,
  /** Base64-encoded binary. */
  Base64 = 1
}
/** Input type hint for embedding optimization. */
export const enum JsEmbeddingInputType {
  /** The input is a search query. */
  Query = 0,
  /** The input is a document to be indexed. */
  Document = 1
}
/** Message role in a conversation. */
export const enum JsRole {
  /** System message providing context or instructions */
  System = 0,
  /** User message */
  User = 1,
  /** Assistant (LLM) message */
  Assistant = 2
}
/** Reason the model stopped generating. */
export const enum JsStopReason {
  /** Natural end of response */
  EndTurn = 0,
  /** Hit max tokens limit */
  MaxTokens = 1,
  /** Model wants to use a tool */
  ToolUse = 2,
  /** Hit a stop sequence */
  StopSequence = 3,
  /** Response was filtered by content moderation */
  ContentFilter = 4
}
/** Streaming event type. */
export const enum JsStreamEventType {
  /** Message started */
  MessageStart = 0,
  /** Content block started */
  ContentBlockStart = 1,
  /** Content block delta (partial content) */
  ContentBlockDelta = 2,
  /** Content block stopped */
  ContentBlockStop = 3,
  /** Message delta */
  MessageDelta = 4,
  /** Message stopped */
  MessageStop = 5,
  /** Ping event */
  Ping = 6,
  /** Error event */
  Error = 7
}
/** Cache control type for prompt caching. */
export const enum JsCacheControl {
  /** 5-minute TTL cache */
  Ephemeral = 0,
  /** 1-hour TTL cache (Anthropic beta) */
  Extended = 1
}
/** Thinking mode type. */
export const enum JsThinkingType {
  /** Extended thinking is enabled */
  Enabled = 0,
  /** Extended thinking is disabled */
  Disabled = 1
}
/** Batch job status. */
export const enum JsBatchStatus {
  /** Batch is being validated */
  Validating = 0,
  /** Batch is in progress */
  InProgress = 1,
  /** Batch is finalizing */
  Finalizing = 2,
  /** Batch completed successfully */
  Completed = 3,
  /** Batch failed */
  Failed = 4,
  /** Batch expired */
  Expired = 5,
  /** Batch was cancelled */
  Cancelled = 6
}
/** Tool use information returned from asToolUse(). */
export interface JsToolUseInfo {
  id: string
  name: string
  input: any
}
/** Tool result information returned from asToolResult(). */
export interface JsToolResultInfo {
  toolUseId: string
  content: string
  isError: boolean
}
/** Tool use delta information. */
export interface JsToolUseDelta {
  id?: string
  name?: string
  inputJsonDelta?: string
}
export type JsLLMKitClient = JsLlmKitClient
/**
 * LLMKit client for JavaScript/TypeScript.
 *
 * @example
 * ```typescript
 * import { LLMKitClient, Message, CompletionRequest } from 'modelsuite'
 *
 * // Create client from environment variables
 * const client = LLMKitClient.fromEnv()
 *
 * // Create client with explicit provider config
 * const client = new LLMKitClient({
 *   providers: {
 *     anthropic: { apiKey: "sk-..." },
 *     openai: { apiKey: "sk-..." },
 *     azure: { apiKey: "...", endpoint: "https://...", deployment: "gpt-4" },
 *     bedrock: { region: "us-east-1" },
 *   }
 * })
 *
 * // Make a completion request
 * const response = await client.complete(
 *   CompletionRequest.create("claude-sonnet-4-20250514", [Message.user("Hello!")])
 * )
 * console.log(response.textContent())
 *
 * // Streaming with callback
 * client.completeStream(request.withStreaming(), (chunk, error) => {
 *   if (error) throw new Error(error)
 *   if (!chunk) return // done
 *   if (chunk.text) process.stdout.write(chunk.text)
 * })
 * ```
 */
export declare class JsLlmKitClient {
  /**
   * Create a new LLMKit client with provider configurations.
   *
   * @param options - Configuration options including providers dict
   *
   * @example
   * ```typescript
   * const client = new LLMKitClient({
   *   providers: {
   *     anthropic: { apiKey: "sk-..." },
   *     azure: { apiKey: "...", endpoint: "https://...", deployment: "gpt-4" },
   *   }
   * })
   * ```
   */
  constructor(options?: LlmKitClientOptions | undefined | null)
  /**
   * Create client from environment variables.
   *
   * Automatically detects and configures all available providers from environment variables.
   *
   * Supported environment variables:
   * - ANTHROPIC_API_KEY: Anthropic (Claude)
   * - OPENAI_API_KEY: OpenAI (GPT)
   * - AZURE_OPENAI_API_KEY, AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_DEPLOYMENT: Azure OpenAI
   * - AWS_REGION or AWS_DEFAULT_REGION: AWS Bedrock (uses default credential chain)
   * - GOOGLE_API_KEY: Google AI (Gemini)
   * - GOOGLE_CLOUD_PROJECT, VERTEX_LOCATION, VERTEX_ACCESS_TOKEN: Google Vertex AI
   * - GROQ_API_KEY: Groq
   * - MISTRAL_API_KEY: Mistral
   * - COHERE_API_KEY or CO_API_KEY: Cohere
   * - AI21_API_KEY: AI21 Labs
   * - DEEPSEEK_API_KEY: DeepSeek
   * - TOGETHER_API_KEY: Together AI
   * - FIREWORKS_API_KEY: Fireworks AI
   * - PERPLEXITY_API_KEY: Perplexity
   * - CEREBRAS_API_KEY: Cerebras
   * - SAMBANOVA_API_KEY: SambaNova
   * - OPENROUTER_API_KEY: OpenRouter
   * - HUGGINGFACE_API_KEY or HF_TOKEN: HuggingFace
   * - REPLICATE_API_TOKEN: Replicate
   * - CLOUDFLARE_API_TOKEN, CLOUDFLARE_ACCOUNT_ID: Cloudflare Workers AI
   * - WATSONX_API_KEY, WATSONX_PROJECT_ID: IBM watsonx.ai
   * - DATABRICKS_TOKEN, DATABRICKS_HOST: Databricks
   * - BASETEN_API_KEY: Baseten
   * - RUNPOD_API_KEY, RUNPOD_ENDPOINT_ID: RunPod
   * - ANYSCALE_API_KEY: Anyscale
   * - DEEPINFRA_API_KEY: DeepInfra
   * - NOVITA_API_KEY: Novita AI
   * - HYPERBOLIC_API_KEY: Hyperbolic
   * - OLLAMA_BASE_URL: Ollama (local, defaults to http://localhost:11434)
   */
  static fromEnv(): JsLlmKitClient
  /**
   * Make a completion request.
   *
   * Returns a Promise that resolves to CompletionResponse.
   */
  complete(request: JsCompletionRequest): Promise<JsCompletionResponse>
  /**
   * Make a streaming completion request with callback.
   *
   * The callback receives the stream chunk directly, or null on error/done.
   * Check chunk.isDone to determine when streaming is complete.
   */
  completeStream(request: JsCompletionRequest, callback: (chunk: StreamChunk | null, error: string | null) => void): void
  /**
   * Make a streaming completion request with async iterator.
   *
   * Returns an async iterator that you can call `next()` on to get stream chunks:
   *
   * @example
   * ```typescript
   * const stream = await client.stream(request);
   * let chunk;
   * while ((chunk = await stream.next()) !== null) {
   *   if (chunk.text) process.stdout.write(chunk.text);
   *   if (chunk.isDone) break;
   * }
   * ```
   */
  stream(request: JsCompletionRequest): Promise<JsAsyncStreamIterator>
  /** Make a completion request with a specific provider. */
  completeWithProvider(providerName: string, request: JsCompletionRequest): Promise<JsCompletionResponse>
  /** List all registered providers. */
  providers(): Array<string>
  /** Get the default provider name. */
  get defaultProvider(): string | null
  /**
   * Count tokens for a request.
   *
   * This allows estimation of token counts before making a completion request,
   * useful for cost estimation and context window management.
   *
   * Note: Not all providers support token counting. Currently only Anthropic
   * provides native token counting support.
   */
  countTokens(request: JsTokenCountRequest): Promise<JsTokenCountResult>
  /**
   * Create a batch processing job.
   *
   * Submits multiple completion requests to be processed asynchronously.
   * Returns a BatchJob that can be used to track progress and retrieve results.
   *
   * @example
   * ```typescript
   * const requests = [
   *   BatchRequest.create("req-1", CompletionRequest.create("claude-sonnet-4-20250514", [Message.user("Hello")])),
   *   BatchRequest.create("req-2", CompletionRequest.create("claude-sonnet-4-20250514", [Message.user("World")])),
   * ]
   * const batchJob = await client.createBatch(requests)
   * console.log(`Batch created: ${batchJob.id}`)
   * ```
   */
  createBatch(requests: Array<JsBatchRequest>): Promise<JsBatchJob>
  /**
   * Get the status of a batch job.
   *
   * @param providerName - The provider that created the batch
   * @param batchId - The batch ID
   *
   * @example
   * ```typescript
   * const job = await client.getBatch("anthropic", batchJob.id)
   * console.log(`Status: ${job.status}`)
   * ```
   */
  getBatch(providerName: string, batchId: string): Promise<JsBatchJob>
  /**
   * Get the results of a completed batch.
   *
   * @param providerName - The provider that created the batch
   * @param batchId - The batch ID
   *
   * @example
   * ```typescript
   * const results = await client.getBatchResults("anthropic", batchJob.id)
   * for (const result of results) {
   *   if (result.isSuccess()) {
   *     console.log(`${result.customId}: ${result.response?.textContent()}`)
   *   } else {
   *     console.error(`${result.customId}: ${result.error?.message}`)
   *   }
   * }
   * ```
   */
  getBatchResults(providerName: string, batchId: string): Promise<Array<JsBatchResult>>
  /**
   * Cancel a batch job.
   *
   * @param providerName - The provider that created the batch
   * @param batchId - The batch ID
   *
   * @example
   * ```typescript
   * const job = await client.cancelBatch("anthropic", batchJob.id)
   * console.log(`Batch cancelled: ${job.status}`)
   * ```
   */
  cancelBatch(providerName: string, batchId: string): Promise<JsBatchJob>
  /**
   * List batch jobs for a provider.
   *
   * @param providerName - The provider to list batches for
   * @param limit - Maximum number of batches to return (optional)
   *
   * @example
   * ```typescript
   * const batches = await client.listBatches("anthropic", 10)
   * for (const batch of batches) {
   *   console.log(`${batch.id}: ${batch.status}`)
   * }
   * ```
   */
  listBatches(providerName: string, limit?: number | undefined | null): Promise<Array<JsBatchJob>>
  /**
   * Generate embeddings for text.
   *
   * Creates vector representations of text that can be used for semantic search,
   * clustering, classification, and other NLP tasks.
   *
   * Note: Not all providers support embeddings. Currently OpenAI and Cohere
   * support this feature.
   *
   * @example
   * ```typescript
   * const response = await client.embed(
   *   new EmbeddingRequest("text-embedding-3-small", "Hello, world!")
   * )
   * console.log(`Dimensions: ${response.dimensionCount}`)
   * console.log(`Values: ${response.values()?.slice(0, 5)}...`)
   * ```
   */
  embed(request: JsEmbeddingRequest): Promise<JsEmbeddingResponse>
  /**
   * Generate embeddings with a specific provider.
   *
   * @param providerName - Name of the embedding provider (e.g., "openai", "cohere")
   * @param request - EmbeddingRequest with model and text(s) to embed
   *
   * @example
   * ```typescript
   * const response = await client.embedWithProvider(
   *   "openai",
   *   new EmbeddingRequest("text-embedding-3-small", "Hello")
   * )
   * ```
   */
  embedWithProvider(providerName: string, request: JsEmbeddingRequest): Promise<JsEmbeddingResponse>
  /**
   * List all registered embedding providers.
   *
   * @returns Names of providers that support embeddings
   */
  embeddingProviders(): Array<string>
  /**
   * Check if a provider supports embeddings.
   *
   * @param providerName - Name of the provider to check
   * @returns True if the provider supports embeddings
   */
  supportsEmbeddings(providerName: string): boolean
}
/** Complete model specification. */
export declare class JsModelInfo {
  /** LiteLLM-compatible model ID (e.g., "anthropic/claude-3-5-sonnet"). */
  get id(): string
  /** Short alias (e.g., "claude-3-5-sonnet"). */
  get alias(): string | null
  /** Human-readable name. */
  get name(): string
  /** Provider. */
  get provider(): JsProvider
  /** Model status. */
  get status(): JsModelStatus
  /** Pricing information. */
  get pricing(): JsModelPricing
  /** Model capabilities. */
  get capabilities(): JsModelCapabilities
  /** Benchmark scores. */
  get benchmarks(): JsModelBenchmarks
  /** Model description. */
  get description(): string
  /** Whether the model can be used as a classifier. */
  get canClassify(): boolean
  /** Get the raw model ID without provider prefix. */
  rawId(): string
  /** Calculate quality per dollar (higher is better value). */
  qualityPerDollar(): number
  /** Estimate cost for a request. */
  estimateCost(inputTokens: number, outputTokens: number): number
}
/** Definition of a tool that can be used by the model. */
export declare class JsToolDefinition {
  /** Create a new tool definition. */
  constructor(name: string, description: string, inputSchema: any)
  /** The tool name. */
  get name(): string
  /** The tool description. */
  get description(): string
  /** The input schema. */
  get inputSchema(): any
}
/**
 * Builder for creating tool definitions with a fluent API.
 *
 * @example
 * ```typescript
 * const tool = new ToolBuilder("get_weather")
 *   .description("Get current weather")
 *   .stringParam("city", "City name", true)
 *   .enumParam("unit", "Temperature unit", ["celsius", "fahrenheit"])
 *   .build()
 * ```
 */
export declare class JsToolBuilder {
  /** Create a new tool builder. */
  constructor(name: string)
  /** Set the tool description. */
  description(description: string): JsToolBuilder
  /** Add a string parameter. */
  stringParam(name: string, description: string, required?: boolean | undefined | null): JsToolBuilder
  /** Add an integer parameter. */
  integerParam(name: string, description: string, required?: boolean | undefined | null): JsToolBuilder
  /** Add a number (float) parameter. */
  numberParam(name: string, description: string, required?: boolean | undefined | null): JsToolBuilder
  /** Add a boolean parameter. */
  booleanParam(name: string, description: string, required?: boolean | undefined | null): JsToolBuilder
  /** Add an array parameter. */
  arrayParam(name: string, description: string, itemType: string, required?: boolean | undefined | null): JsToolBuilder
  /** Add an enum parameter (string with allowed values). */
  enumParam(name: string, description: string, values: Array<string>, required?: boolean | undefined | null): JsToolBuilder
  /** Add a custom parameter with a JSON schema. */
  customParam(name: string, schema: any, required?: boolean | undefined | null): JsToolBuilder
  /** Build the tool definition. */
  build(): JsToolDefinition
}
/**
 * Request for generating embeddings.
 *
 * @example
 * ```typescript
 * // Single text
 * const request = new EmbeddingRequest("text-embedding-3-small", "Hello, world!");
 *
 * // Batch
 * const request = EmbeddingRequest.batch("text-embedding-3-small", ["Hello", "World"]);
 * ```
 */
export declare class JsEmbeddingRequest {
  /**
   * Create a new embedding request for a single text.
   *
   * @param model - The embedding model to use (e.g., "text-embedding-3-small")
   * @param text - The text to embed
   */
  constructor(model: string, text: string)
  /**
   * Create a new embedding request for multiple texts (batch).
   *
   * @param model - The embedding model to use
   * @param texts - List of texts to embed
   */
  static batch(model: string, texts: Array<string>): JsEmbeddingRequest
  /**
   * Set the output dimensions (for models that support dimension reduction).
   *
   * @param dimensions - The number of dimensions for the output embedding
   * @returns Self for method chaining
   */
  withDimensions(dimensions: number): JsEmbeddingRequest
  /**
   * Set the encoding format.
   *
   * @param format - The encoding format (Float or Base64)
   * @returns Self for method chaining
   */
  withEncodingFormat(format: JsEncodingFormat): JsEmbeddingRequest
  /**
   * Set the input type hint for optimized embeddings.
   *
   * @param inputType - The input type (Query or Document)
   * @returns Self for method chaining
   */
  withInputType(inputType: JsEmbeddingInputType): JsEmbeddingRequest
  /** Get the model name. */
  get model(): string
  /** Get the number of texts to embed. */
  get textCount(): number
  /** Get all input texts as an array. */
  texts(): Array<string>
  /** Get the dimensions setting. */
  get dimensions(): number | null
}
/** A single embedding vector. */
export declare class JsEmbedding {
  /** The index of this embedding in the batch. */
  get index(): number
  /** The embedding vector values. */
  get values(): Array<number>
  /** Get the number of dimensions. */
  get dimensionCount(): number
  /**
   * Compute cosine similarity with another embedding.
   *
   * @param other - Another embedding to compare with
   * @returns Cosine similarity score (-1 to 1)
   */
  cosineSimilarity(other: JsEmbedding): number
  /**
   * Compute dot product with another embedding.
   *
   * @param other - Another embedding to compute dot product with
   * @returns Dot product value
   */
  dotProduct(other: JsEmbedding): number
  /**
   * Compute Euclidean distance to another embedding.
   *
   * @param other - Another embedding to compute distance to
   * @returns Euclidean distance
   */
  euclideanDistance(other: JsEmbedding): number
}
/** Token usage for embedding requests. */
export declare class JsEmbeddingUsage {
  /** Number of tokens in the input. */
  get promptTokens(): number
  /** Total tokens processed. */
  get totalTokens(): number
}
/** Response from an embedding request. */
export declare class JsEmbeddingResponse {
  /** The model used for embedding. */
  get model(): string
  /** The generated embeddings. */
  get embeddings(): Array<JsEmbedding>
  /** Token usage information. */
  get usage(): JsEmbeddingUsage
  /** Get the first embedding (convenience for single-text requests). */
  first(): JsEmbedding | null
  /** Get embedding values as a flat array (for single-text requests). */
  values(): Array<number> | null
  /** Get the embedding dimensions. */
  get dimensionCount(): number
}
/**
 * A block of content within a message.
 *
 * Use the static factory methods to create instances:
 * - `ContentBlock.text("Hello")`
 * - `ContentBlock.image("image/png", base64Data)`
 * - `ContentBlock.toolUse(id, name, inputObj)`
 */
export declare class JsContentBlock {
  /** Create a text content block. */
  static text(text: string): JsContentBlock
  /** Create an image content block from base64 data. */
  static image(mediaType: string, data: string): JsContentBlock
  /** Create an image content block from URL. */
  static imageUrl(url: string): JsContentBlock
  /** Create a tool use content block. */
  static toolUse(id: string, name: string, input: any): JsContentBlock
  /** Create a tool result content block. */
  static toolResult(toolUseId: string, content: string, isError?: boolean | undefined | null): JsContentBlock
  /** Create a thinking content block. */
  static thinking(thinking: string): JsContentBlock
  /** Create a PDF document content block. */
  static pdf(data: string): JsContentBlock
  /** Create a text content block with ephemeral caching. */
  static textCached(text: string): JsContentBlock
  /** True if this is a text block. */
  get isText(): boolean
  /** True if this is a tool use block. */
  get isToolUse(): boolean
  /** True if this is a tool result block. */
  get isToolResult(): boolean
  /** True if this is a document block. */
  get isDocument(): boolean
  /** True if this is a thinking block. */
  get isThinking(): boolean
  /** True if this is an image block. */
  get isImage(): boolean
  /** Get text content if this is a text block. */
  get textValue(): string | null
  /** Get thinking content if this is a thinking block. */
  get thinkingContent(): string | null
  /**
   * Get tool use details if this is a tool use block.
   * Returns an object with id, name, and input properties.
   */
  asToolUse(): JsToolUseInfo | null
  /**
   * Get tool result details if this is a tool result block.
   * Returns an object with toolUseId, content, and isError properties.
   */
  asToolResult(): JsToolResultInfo | null
}
/**
 * A message in a conversation.
 *
 * Use the static factory methods to create instances:
 * - `Message.system("You are helpful")`
 * - `Message.user("Hello")`
 * - `Message.assistant("Hi there")`
 */
export declare class JsMessage {
  /** Create a system message with text content. */
  static system(text: string): JsMessage
  /** Create a user message with text content. */
  static user(text: string): JsMessage
  /** Create an assistant message with text content. */
  static assistant(text: string): JsMessage
  /** Create a user message with multiple content blocks. */
  static userWithContent(content: Array<JsContentBlock>): JsMessage
  /** Create an assistant message with multiple content blocks. */
  static assistantWithContent(content: Array<JsContentBlock>): JsMessage
  /** Create a user message with tool results. */
  static toolResults(results: Array<JsContentBlock>): JsMessage
  /** The role of the message sender. */
  get role(): JsRole
  /** The content blocks in this message. */
  get content(): Array<JsContentBlock>
  /** Get all text content from the message concatenated. */
  textContent(): string
  /** Check if the message contains any tool use blocks. */
  hasToolUse(): boolean
  /** Extract all tool use blocks from the message. */
  toolUses(): Array<JsContentBlock>
}
/** Cache breakpoint configuration for prompt caching. */
export declare class JsCacheBreakpoint {
  /** Create an ephemeral cache breakpoint (5-minute TTL). */
  static ephemeral(): JsCacheBreakpoint
  /** Create an extended cache breakpoint (1-hour TTL, Anthropic beta). */
  static extended(): JsCacheBreakpoint
  /** The cache control type. */
  get cacheControl(): JsCacheControl
}
/** Configuration for extended thinking mode. */
export declare class JsThinkingConfig {
  /** Enable extended thinking with a token budget. */
  static enabled(budgetTokens: number): JsThinkingConfig
  /** Disable extended thinking. */
  static disabled(): JsThinkingConfig
  /** The thinking type (enabled or disabled). */
  get thinkingType(): JsThinkingType
  /** The token budget for thinking (if enabled). */
  get budgetTokens(): number | null
  /** Check if thinking is enabled. */
  get isEnabled(): boolean
}
/** Configuration for structured output format. */
export declare class JsStructuredOutput {
  /** Create a JSON schema structured output. */
  static jsonSchema(name: string, schema: any): JsStructuredOutput
  /** Create a JSON object structured output (no specific schema). */
  static jsonObject(): JsStructuredOutput
  /** Create a text structured output. */
  static text(): JsStructuredOutput
  /** Check if this is a JSON schema output. */
  get isJsonSchema(): boolean
  /** Check if this is a JSON object output. */
  get isJsonObject(): boolean
  /** Check if this is a text output. */
  get isTextOutput(): boolean
}
/**
 * Request to complete a conversation.
 *
 * Use the static factory method `create()` or the builder pattern.
 *
 * @example
 * ```typescript
 * // Factory with model and messages
 * const request = CompletionRequest.create("claude-sonnet-4-20250514", [Message.user("Hello")])
 *   .withSystem("You are helpful")
 *   .withMaxTokens(1024)
 *
 * // Or use builder pattern
 * const request = CompletionRequest.create("gpt-4o", [Message.user("Hi")])
 *   .withTemperature(0.7)
 *   .withStreaming()
 * ```
 */
export declare class JsCompletionRequest {
  /** Create a new completion request with model and messages. */
  static create(model: string, messages: Array<JsMessage>): JsCompletionRequest
  /** Builder method: Set the system prompt. */
  withSystem(system: string): JsCompletionRequest
  /** Builder method: Set max tokens. */
  withMaxTokens(maxTokens: number): JsCompletionRequest
  /** Builder method: Set temperature. */
  withTemperature(temperature: number): JsCompletionRequest
  /** Builder method: Set top_p (nucleus sampling). */
  withTopP(topP: number): JsCompletionRequest
  /** Builder method: Set tools. */
  withTools(tools: Array<JsToolDefinition>): JsCompletionRequest
  /** Builder method: Set stop sequences. */
  withStopSequences(stopSequences: Array<string>): JsCompletionRequest
  /** Builder method: Enable streaming. */
  withStreaming(): JsCompletionRequest
  /** Builder method: Enable extended thinking with token budget. */
  withThinking(budgetTokens: number): JsCompletionRequest
  /** Builder method: Set thinking configuration. */
  withThinkingConfig(config: JsThinkingConfig): JsCompletionRequest
  /** Builder method: Set JSON schema for structured output. */
  withJsonSchema(name: string, schema: any): JsCompletionRequest
  /** Builder method: Set response format. */
  withResponseFormat(format: JsStructuredOutput): JsCompletionRequest
  /** Builder method: Enable JSON object output. */
  withJsonOutput(): JsCompletionRequest
  /** Builder method: Set predicted output for speculative decoding. */
  withPrediction(predictedContent: string): JsCompletionRequest
  /** Builder method: Enable system prompt caching (ephemeral). */
  withSystemCaching(): JsCompletionRequest
  /** Builder method: Enable system prompt caching (extended, 1-hour TTL). */
  withSystemCachingExtended(): JsCompletionRequest
  /** Builder method: Enable extended output (128k tokens, Anthropic beta). */
  withExtendedOutput(): JsCompletionRequest
  /** Builder method: Enable interleaved thinking (Anthropic beta). */
  withInterleavedThinking(): JsCompletionRequest
  /** Builder method: Set extra provider-specific options. */
  withExtra(extra: any): JsCompletionRequest
  /** The model identifier. */
  get model(): string
  /** The conversation messages. */
  get messages(): Array<JsMessage>
  /** The system prompt (if set). */
  get system(): string | null
  /** Maximum tokens to generate. */
  get maxTokens(): number | null
  /** Sampling temperature. */
  get temperature(): number | null
  /** Whether streaming is enabled. */
  get stream(): boolean
  /** Check if the request has caching enabled. */
  hasCaching(): boolean
  /** Check if the request has thinking enabled. */
  hasThinking(): boolean
  /** Check if the request has structured output. */
  hasStructuredOutput(): boolean
}
/**
 * Request to count tokens for a model.
 *
 * This allows estimation of token counts before making a completion request,
 * useful for cost estimation and context window management.
 *
 * @example
 * ```typescript
 * // Create from model and messages
 * const request = TokenCountRequest.create(
 *   "claude-sonnet-4-20250514",
 *   [Message.user("Hello, how are you?")]
 * ).withSystem("You are a helpful assistant")
 *
 * // Count tokens
 * const result = await client.countTokens(request)
 * console.log(`Input tokens: ${result.inputTokens}`)
 *
 * // Or create from existing completion request
 * const request = TokenCountRequest.fromCompletionRequest(completionRequest)
 * ```
 */
export declare class JsTokenCountRequest {
  /** Create a new token count request with model and messages. */
  static create(model: string, messages: Array<JsMessage>): JsTokenCountRequest
  /** Create a token count request from an existing completion request. */
  static fromCompletionRequest(request: JsCompletionRequest): JsTokenCountRequest
  /** Builder method: Set the system prompt. */
  withSystem(system: string): JsTokenCountRequest
  /** Builder method: Set the tools. */
  withTools(tools: Array<JsToolDefinition>): JsTokenCountRequest
  /** The model identifier. */
  get model(): string
  /** The conversation messages. */
  get messages(): Array<JsMessage>
  /** The system prompt (if set). */
  get system(): string | null
}
/**
 * A request within a batch.
 *
 * @example
 * ```typescript
 * const batchRequests = [
 *   BatchRequest.create("request-1", completionRequest1),
 *   BatchRequest.create("request-2", completionRequest2),
 * ]
 * const batchJob = await client.createBatch(batchRequests)
 * ```
 */
export declare class JsBatchRequest {
  /** Create a new batch request with a custom ID and completion request. */
  static create(customId: string, request: JsCompletionRequest): JsBatchRequest
  /** The custom ID for this request. */
  get customId(): string
}
/** Request counts for a batch job. */
export declare class JsBatchRequestCounts {
  /** Total number of requests in the batch. */
  get total(): number
  /** Number of successfully completed requests. */
  get succeeded(): number
  /** Number of failed requests. */
  get failed(): number
  /** Number of pending requests. */
  get pending(): number
}
/**
 * A batch processing job.
 *
 * Contains information about the status and progress of a batch.
 */
export declare class JsBatchJob {
  /** The batch ID. */
  get id(): string
  /** The current status of the batch. */
  get status(): JsBatchStatus
  /** Request counts. */
  get requestCounts(): JsBatchRequestCounts
  /** When the batch was created (ISO 8601 timestamp). */
  get createdAt(): string | null
  /** When the batch started processing (ISO 8601 timestamp). */
  get startedAt(): string | null
  /** When the batch finished processing (ISO 8601 timestamp). */
  get endedAt(): string | null
  /** When the batch will expire (ISO 8601 timestamp). */
  get expiresAt(): string | null
  /** Error message if the batch failed. */
  get error(): string | null
  /** Check if the batch is complete (completed, failed, expired, or cancelled). */
  isComplete(): boolean
  /** Check if the batch is still in progress. */
  isInProgress(): boolean
}
/** Error information for a failed batch request. */
export declare class JsBatchError {
  /** Error type. */
  get errorType(): string
  /** Error message. */
  get message(): string
}
/** Result of a single request within a batch. */
export declare class JsBatchResult {
  /** The custom ID of the original request. */
  get customId(): string
  /** The completion response (if successful). */
  get response(): JsCompletionResponse | null
  /** The error (if failed). */
  get error(): JsBatchError | null
  /** Check if this result is successful. */
  isSuccess(): boolean
  /** Check if this result is an error. */
  isError(): boolean
}
/**
 * Result of a token counting request.
 *
 * @example
 * ```typescript
 * const result = await client.countTokens(request)
 * console.log(`Input tokens: ${result.inputTokens}`)
 * ```
 */
export declare class JsTokenCountResult {
  /** Total number of input tokens. */
  get inputTokens(): number
}
/** Token usage information. */
export declare class JsUsage {
  /** Number of tokens in the prompt. */
  get inputTokens(): number
  /** Number of tokens in the completion. */
  get outputTokens(): number
  /** Cache creation tokens (if applicable). */
  get cacheCreationInputTokens(): number
  /** Cache read tokens (if applicable). */
  get cacheReadInputTokens(): number
  /** Total tokens used (input + output). */
  totalTokens(): number
}
/** Response from a completion request. */
export declare class JsCompletionResponse {
  /** Unique response ID. */
  get id(): string
  /** Model that generated the response. */
  get model(): string
  /** Content blocks in the response. */
  get content(): Array<JsContentBlock>
  /** Reason the model stopped. */
  get stopReason(): JsStopReason
  /** Token usage information. */
  get usage(): JsUsage
  /** Get all text content from the response concatenated. */
  textContent(): string
  /** Extract all tool use blocks from the response. */
  toolUses(): Array<JsContentBlock>
  /** Check if the response contains tool use. */
  hasToolUse(): boolean
  /** Get thinking content from the response if present. */
  thinkingContent(): string | null
}
/** Delta content for streaming responses. */
export declare class JsContentDelta {
  /** Get text if this is a text delta. */
  get text(): string | null
  /** Get thinking content if this is a thinking delta. */
  get thinking(): string | null
  /** True if this is a text delta. */
  get isText(): boolean
  /** True if this is a tool use delta. */
  get isToolUse(): boolean
  /** True if this is a thinking delta. */
  get isThinking(): boolean
  /**
   * Get tool use delta details.
   * Returns an object with optional id, name, and inputJsonDelta properties.
   */
  asToolUseDelta(): JsToolUseDelta | null
}
/** A chunk from a streaming response. */
export declare class JsStreamChunk {
  /** The type of stream event. */
  get eventType(): JsStreamEventType
  /** Index of the content block being updated. */
  get index(): number | null
  /** The delta content (if applicable). */
  get delta(): JsContentDelta | null
  /** Convenience: Get text from delta if present. */
  get text(): string | null
  /** Stop reason (only on message_stop). */
  get stopReason(): JsStopReason | null
  /** Usage information (may be partial or final). */
  get usage(): JsUsage | null
  /** True if this is a message stop event. */
  get isDone(): boolean
}
/**
 * Async iterator for streaming completion responses.
 *
 * Use with manual iteration by calling `next()`:
 *
 * @example
 * ```typescript
 * const stream = await client.stream(request);
 * let chunk;
 * while ((chunk = await stream.next()) !== null) {
 *   if (chunk.text) {
 *     process.stdout.write(chunk.text);
 *   }
 *   if (chunk.isDone) break;
 * }
 * ```
 *
 * Or use the callback-based `completeStream` for simpler consumption.
 */
export declare class JsAsyncStreamIterator {
  /**
   * Get the next chunk from the stream.
   *
   * Returns the next StreamChunk, or null when the stream is complete.
   * Check `chunk.isDone` to determine when streaming is complete.
   */
  next(): Promise<JsStreamChunk | null>
  /** Check if the stream is done. */
  get isFinished(): Promise<boolean>
}
